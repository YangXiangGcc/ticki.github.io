<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ticki&#39;s blog</title>
    <link>http://ticki.github.io/post/</link>
    <description>Recent content in Posts on Ticki&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Nov 2016 16:28:44 +0200</lastBuildDate>
    <atom:link href="http://ticki.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Designing a good non-cryptographic hash function</title>
      <link>http://ticki.github.io/blog/designing-a-good-non-cryptographic-hash-function/</link>
      <pubDate>Fri, 04 Nov 2016 16:28:44 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/designing-a-good-non-cryptographic-hash-function/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;So, I&#39;ve been needing a hash function for various purposes, lately. None of the existing hash functions I could find were sufficient for my needs, so I went and designed my own. These are my notes on the design of hash functions.&lt;/p&gt;

&lt;h1 id=&#34;what-is-a-hash-function-really&#34;&gt;What is a hash function &lt;em&gt;really&lt;/em&gt;?&lt;/h1&gt;

&lt;p&gt;Hash functions are functions which maps a infinite domain to a finite codomain. Two elements in the domain, &lt;span  class=&#34;math&#34;&gt;\(a, b\)&lt;/span&gt; are said to collide if &lt;span  class=&#34;math&#34;&gt;\(h(a) = h(b)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The ideal hash functions has the property that the distribution of image of a a subset of the domain is statistically independent of the probability of said subset occuring. That is, collisions are not likely to occur even within non-uniform distributed sets.&lt;/p&gt;

&lt;p&gt;Consider you have an english dictionary. Clearly, &lt;code&gt;hello&lt;/code&gt; is more likely to be a word than &lt;code&gt;ctyhbnkmaasrt&lt;/code&gt;, but the hash function must not be affected by this statistical redundancy.&lt;/p&gt;

&lt;p&gt;In a sense, you can think of the ideal hash function as being a function where the output is uniformly distributed (e.g., chosen by a sequence of coinflips) over the codomain no matter what the distribution of the input is.&lt;/p&gt;

&lt;p&gt;With a good hash function, it should be hard to distinguish between a truely random sequence and the hashes of some permutation of the domain.&lt;/p&gt;

&lt;p&gt;Hash function ought to be as chaotic as possible. A small change in the input should appear in the output as if it was a big change. This is called the hash function butterfly effect.&lt;/p&gt;

&lt;h2 id=&#34;noncryptographic-and-cryptographic&#34;&gt;Non-cryptographic and cryptographic&lt;/h2&gt;

&lt;p&gt;One must make the distinction between cryptographic and non-cryptographic hash functions. In a cryptographic hash function, it must be infeasible to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Generate the input from its hash output.&lt;/li&gt;
&lt;li&gt;Generate two inputs with the same output.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Non-cryptographic hash functions can be thought of as approximations of these invariants. The reason for the use of non-cryptographic hash function is that they&#39;re significantly faster than cryptographic hash functions.&lt;/p&gt;

&lt;h1 id=&#34;diffusions-and-bijection&#34;&gt;Diffusions and bijection&lt;/h1&gt;

&lt;p&gt;The basic building block of good hash functions are difussions. Difussions can be thought of as bijective (i.e. every input has one and only one output, and vice versa) hash functions, namely that input and output are uncorrelated:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/bijective_diffusion_diagram.svg&#34; alt=&#34;A diagram of a diffusion.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;This diffusion function has a relatively small domain, for illustrational purpose.&lt;/p&gt;

&lt;h2 id=&#34;building-a-good-diffusion&#34;&gt;Building a good diffusion&lt;/h2&gt;

&lt;p&gt;Diffusions are often build by smaller, bijective components, which we will call &amp;quot;subdiffusions&amp;quot;.&lt;/p&gt;

&lt;h3 id=&#34;types-of-subdiffusions&#34;&gt;Types of subdiffusions&lt;/h3&gt;

&lt;p&gt;One must distinguish between the different kinds of subdiffusions.&lt;/p&gt;

&lt;p&gt;The first class to consider is the &lt;strong&gt;bitwise subdiffusions&lt;/strong&gt;. These are quite weak when they stand alone, and thus must be combined with other types of subdiffusions. Bitwise subdiffusions might flip certain bits and/or reorganize them:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = \sigma(x) \oplus m\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(we use &lt;span  class=&#34;math&#34;&gt;\(\sigma\)&lt;/span&gt; to denote permutation of bits)&lt;/p&gt;

&lt;p&gt;The second class is &lt;strong&gt;dependent bitwise subdiffusions&lt;/strong&gt;. These are diffusions which permutes the bits and XOR them with the original value:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = \sigma(x) \oplus x\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(exercise to reader: prove that the above subdivision is revertible)&lt;/p&gt;

&lt;p&gt;Another similar often used subdiffusion in the same class is the XOR-shift:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = (x \ll m) \oplus x\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(note that &lt;span  class=&#34;math&#34;&gt;\(m\)&lt;/span&gt; can be negative, in which case the bitshift becomes a right bitshift)&lt;/p&gt;

&lt;p&gt;The next subdiffusion are of massive importance. It&#39;s the class of &lt;strong&gt;linear subdiffusions&lt;/strong&gt; similar to the LCG random number generator:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) \equiv ax + c \pmod m, \quad \gcd(x, m) = 1\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(&lt;span  class=&#34;math&#34;&gt;\(\gcd\)&lt;/span&gt; means &amp;quot;greatest common divisor&amp;quot;, this constraint is necessary in order to have &lt;span  class=&#34;math&#34;&gt;\(a\)&lt;/span&gt; have an inverse in the ring)&lt;/p&gt;

&lt;p&gt;The next are particularly interesting, it&#39;s the &lt;strong&gt;arithmetic subdiffusions&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = x \oplus (x + c)\]&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&#34;combining-subdiffusions&#34;&gt;Combining subdiffusions&lt;/h3&gt;

&lt;p&gt;Subdiffusions themself are quite poor quality. Combining them is what creates a good diffusion function.&lt;/p&gt;

&lt;p&gt;Indeed if you combining enough different subdiffusions, you get a good diffusion function, but there is a catch: The more subdiffusions you combine the slower it is to compute.&lt;/p&gt;

&lt;p&gt;As such, it is important to find a small, diverse set of subdiffusions which has a good quality.&lt;/p&gt;

&lt;h3 id=&#34;zerosensitivity&#34;&gt;Zero-sensitivity&lt;/h3&gt;

&lt;p&gt;If your diffusion isn&#39;t zero-sensitive (i.e., &lt;span  class=&#34;math&#34;&gt;\(f(x) = \{0, 1\}\)&lt;/span&gt;), you should &lt;del&gt;panic&lt;/del&gt; come up with something better. In particular, make sure your diffusion contains at least one zero-sensitive subdiffusion as component.&lt;/p&gt;

&lt;h3 id=&#34;avalanche-diagrams&#34;&gt;Avalanche diagrams&lt;/h3&gt;

&lt;p&gt;Avalanche diagrams are the best and quickist way to find out if your diffusion function has a good quality.&lt;/p&gt;

&lt;p&gt;Essentially, you draw a grid such that the &lt;span  class=&#34;math&#34;&gt;\((x, y)\)&lt;/span&gt; cell&#39;s color represents the probability that flipping &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;&#39;th bit of the input will result of &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt;&#39;th bit being flipped in the output. If &lt;span  class=&#34;math&#34;&gt;\((x, y)\)&lt;/span&gt; is very red, the probability that &lt;span  class=&#34;math&#34;&gt;\(d(a&#39;)\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(a&#39;\)&lt;/span&gt; is &lt;span  class=&#34;math&#34;&gt;\(a\)&lt;/span&gt; with the &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;&#39;th bit flipped,&#39; has the &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt;&#39;th bit flipped is very high.&lt;/p&gt;

&lt;p&gt;Here&#39;s an example of the identity function, &lt;span  class=&#34;math&#34;&gt;\(f(x) = x\)&lt;/span&gt;:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/identity_function_avalanche_diagram.svg&#34; alt=&#34;The identity function.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;So why is it a straight line?&lt;/p&gt;

&lt;p&gt;Well, if you flip the &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt;&#39;th bit in the input, the only bit flipped in the output is the &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt;&#39;th bit. That&#39;s kind of boring, let&#39;s try adding a number:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/addition_avalanche_diagram.svg&#34; alt=&#34;Adding a big number.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Meh, this is kind of obvious. Let&#39;s try multiplying by a prime:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/prime_multiplication_avalanche_diagram.svg&#34; alt=&#34;Multiplying by a non-even prime is a bijection.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Now, this is quite interesting actually. We call all the black area &amp;quot;blind spots&amp;quot;, and you can see here that anything with &lt;span  class=&#34;math&#34;&gt;\(x &gt; y\)&lt;/span&gt; is a blind spot. Why is that? Well, if I flip a high bit, it won&#39;t affect the lower bits because you can see multiplication as a form of overlay:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;100011101000101010101010111
      :
    111
↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕
100000001000101010101010111
      :
    111
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flipping a single bit will only change the integer forward, never backwards, hence it forms this blind spot. So how can we fix this (we don&#39;t want this bias)?&lt;/p&gt;

&lt;h4 id=&#34;designing-a-diffusion-function--by-example&#34;&gt;Designing a diffusion function -- by example&lt;/h4&gt;

&lt;p&gt;If we throw in (after prime multiplication) a dependent bitwise-shift subdiffusions, we have&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;\gets x + 1 \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \ll z) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(note that we have the &lt;span  class=&#34;math&#34;&gt;\(+1\)&lt;/span&gt; in order to make it zero-sensitive)&lt;/p&gt;

&lt;p&gt;This generates following avalanche diagram&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/shift_xor_multiply_avalanche_diagram.svg&#34; alt=&#34;Shift-XOR then multiply.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;What can cause these? Clearly there is some form of bias. Turns out that this bias mostly originates in the lack of hybrid arithmetic/bitwise sub.
Without such hybrid, the behavior tends to be relatively local and not interfering well with each other.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[x \gets x + \text{ROL}_k(x)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;At this point, it looks something like&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/shift_xor_multiply_rotate_avalanche_diagram.svg&#34; alt=&#34;Shift-XOR then multiply.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;That&#39;s good, but we&#39;re not quite there yet...&lt;/p&gt;

&lt;p&gt;Let&#39;s throw in the following bijection:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[x \gets px \oplus (px \gg z)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And voilà, we now have a perfect bit independence:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/perfect_avalanche_diagram.svg&#34; alt=&#34;Everything is red!&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;So our finalized version of an example diffusion is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;\gets x + 1 \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \ll z) \\
x &amp;\gets x + \text{ROL}_k(x) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \gg z) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;That seems like a pretty lengthy chunk of operations. We will try to boil it down to few operations while preserving the quality of this diffusion.&lt;/p&gt;

&lt;p&gt;The most obvious think to remove is the rotation line. But it hurts quality:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/multiply_up_avalanche_diagram.svg&#34; alt=&#34;Here&#39;s the avalanche diagram of said line removed.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Where do these blind spot comes from? The answer is pretty simple: shifting left moves the entropy upwards, hence the multiplication will never really flip the lower bits. For example, if we flip the sixth bit, and trace it down the operations, you will how it never flips in the other end.&lt;/p&gt;

&lt;p&gt;So what do we do? Instead of shifting left, we need to shift right, since multiplication only affects upwards:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;\gets x + 1 \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \gg z) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And we&#39;re back again. This time with two less instructions.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_1_avalanche_diagram.svg&#34; alt=&#34;Stage 1&#34;&gt;&lt;/figure&gt;&lt;/th&gt;
&lt;th&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_2_avalanche_diagram.svg&#34; alt=&#34;Stage 2&#34;&gt;&lt;/figure&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_3_avalanche_diagram.svg&#34; alt=&#34;Stage 3&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_4_avalanche_diagram.svg&#34; alt=&#34;Stage 4&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_5_avalanche_diagram.svg&#34; alt=&#34;Stage 5&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/perfect_avalanche_diagram.svg&#34; alt=&#34;Stage 6&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;combining-diffusions&#34;&gt;Combining diffusions&lt;/h1&gt;

&lt;p&gt;Diffusions maps a finite state space to a finite state space, as such they&#39;re not alone sufficient as arbitrary-length hash function, so we need a way to combine diffusions.&lt;/p&gt;

&lt;p&gt;In particular, we can eat &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; bytes of the input at once and modify the state based on that:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[s&#39; = d(f(s&#39;, x))\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Or in graphic form,&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/hash_round_flowchart.svg&#34; alt=&#34;A flowchart.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(f(s&#39;, x)\)&lt;/span&gt; is what we call our combinator function. It serves for combining the old state and the new input block (&lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;). &lt;span  class=&#34;math&#34;&gt;\(d(a)\)&lt;/span&gt; is just our diffusion function.&lt;/p&gt;

&lt;p&gt;It doesn&#39;t matter if the combinator function is commutative or not, but it is crucial that it is not biased, i.e. if &lt;span  class=&#34;math&#34;&gt;\(a, b\)&lt;/span&gt; are uniformly distributed variables, &lt;span  class=&#34;math&#34;&gt;\(f(a, b)\)&lt;/span&gt; is too. Ideally, there should exist a bijection, &lt;span  class=&#34;math&#34;&gt;\(g(f(a, b), b) = a\)&lt;/span&gt;, which implies that it is not biased.&lt;/p&gt;

&lt;p&gt;An example of such combination function is simple addition.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(a, b) = a + b\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Another is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(a, b) = a \oplus b\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I&#39;m partial towards saying that these are the only sane choices for combinator functions, and you must pick between them based on the characteristics of your diffusion function:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If your diffusion function is primarily based on arithmetics, you should use the XOR combinator function.&lt;/li&gt;
&lt;li&gt;If your diffusion function is primarily based on bitwise operations, you should use the additive combinator function.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The reason for this is that you want to have the operations to be as diverse as possible, to create complex, seemingly random behavior.&lt;/p&gt;

&lt;h1 id=&#34;simd-simd-simd&#34;&gt;SIMD, SIMD, SIMD&lt;/h1&gt;

&lt;p&gt;If you want good performance, you shouldn&#39;t read only one byte at a time. By reading multiple bytes at a time, your algorithm becomes several times faster.&lt;/p&gt;

&lt;p&gt;This however introduces the need for some finalization, if the total number of written bytes doesn&#39;t divide the number of bytes read in a round. One possibility is to pad it with zeros and write the total length in the end, however this turns out to be somewhat slow for small inputs.&lt;/p&gt;

&lt;p&gt;A better option is to write in the number of padding bytes into the last byte.&lt;/p&gt;

&lt;h1 id=&#34;instruction-level-parallelism&#34;&gt;Instruction level parallelism&lt;/h1&gt;

&lt;p&gt;Fetching multiple blocks and sequentially (without dependency until last) running a round is something I&#39;ve found to work well. This has to do with the so-called instruction pipeline in which modern processors run instructions in parallel when they can.&lt;/p&gt;

&lt;h1 id=&#34;testing-the-hash-function&#34;&gt;Testing the hash function&lt;/h1&gt;

&lt;p&gt;Multiple test suits for testing the quality and performance of your hash function. &lt;a href=&#34;https://github.com/aappleby/smhasher&#34;&gt;Smhasher&lt;/a&gt; is one of these.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Many relatively simple components can be combined into a strong and robust non-cryptographic hash function for use in hash tables and in checksumming. Deriving such a function is really just coming up with the components to construct this hash function.&lt;/p&gt;

&lt;p&gt;Breaking the problem down into small subproblems significantly simplifies analysis and guarantees.&lt;/p&gt;

&lt;p&gt;The key to a good hash function is to try-and-miss. Testing and throwing out candidates is the only way you can really find out if you hash function works in practice.&lt;/p&gt;

&lt;p&gt;Have fun hacking!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How LZ4 works</title>
      <link>http://ticki.github.io/blog/how-lz4-works/</link>
      <pubDate>Tue, 25 Oct 2016 23:25:15 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/how-lz4-works/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;LZ4 is a really fast compression algorithm with a reasonable compression ratio, but unfortunately there is limited documentation on how it works. The only explanation (not spec, explanation) &lt;a href=&#34;https://fastcompression.blogspot.com/2011/05/lz4-explained.html&#34;&gt;can be found&lt;/a&gt; on the author&#39;s blog, but I think it is less of an explanation and more of an informal specification.&lt;/p&gt;

&lt;p&gt;This blog post tries to explain it such that anybody (even new beginners) can understand and implement it.&lt;/p&gt;

&lt;h1 id=&#34;linear-smallinteger-code-lsic&#34;&gt;Linear small-integer code (LSIC)&lt;/h1&gt;

&lt;p&gt;The first part of LZ4 we need to explain is a smart but simple integer encoder. It is very space efficient for 0-255, and then grows linearly, based on the assumption that the integers used with this encoding rarely exceeds this limit, as such it is only used for small integers in the standard.&lt;/p&gt;

&lt;p&gt;It is a form of addition code, in which we read a byte. If this byte is the maximal value (255), another byte is read and added to the sum. This process is repeated until a byte below 255 is reached, which will be added to the sum, and the sequence will then end.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_int_encoding_flowchart.svg&#34; alt=&#34;We try to fit it into the next cluster.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;In short, we just keep adding bytes and stop when we hit a non-0xFF byte.&lt;/p&gt;

&lt;p&gt;We&#39;ll use the name &amp;quot;LSIC&amp;quot; for convinience.&lt;/p&gt;

&lt;h1 id=&#34;block&#34;&gt;Block&lt;/h1&gt;

&lt;p&gt;An LZ4 stream is divided into segments called &amp;quot;blocks&amp;quot;. Blocks contains a literal which is to be copied directly to the output stream, and then a back reference, which tells us to copy some number of bytes from the already decompressed stream.&lt;/p&gt;

&lt;p&gt;This is really were the compression is going on. Copying from the old stream allows deduplication and runs-length encoding.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;A block looks like:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\overbrace{\underbrace{t_1}_\text{4 bits}\  \underbrace{t_2}_\text{4 bits}}^\text{Token} \quad \underbrace{\overbrace{e_1}^\texttt{LISC}}_\text{If $t_1 = 15$} \quad \underbrace{\overbrace{L}^\text{Literal}}_{t_1 + e\text{ bytes }} \quad \overbrace{\underbrace{O}_\text{2 bytes}}^\text{Little endian} \quad \underbrace{\overbrace{e_2}^\texttt{LISC}}_\text{If $t_2 = 15$}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And decodes to the &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt; segment, followed by a &lt;span  class=&#34;math&#34;&gt;\(t_2 + e_2 + 4\)&lt;/span&gt; bytes sequence copied from position &lt;span  class=&#34;math&#34;&gt;\(l - O\)&lt;/span&gt; from the output buffer (where &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt; is the length of the output buffer).&lt;/p&gt;

&lt;p&gt;We will explain all of these in the next sections.&lt;/p&gt;

&lt;h2 id=&#34;token&#34;&gt;Token&lt;/h2&gt;

&lt;p&gt;Any block starts with a 1 byte token, which is divided into two 4-bit fields.&lt;/p&gt;

&lt;h2 id=&#34;literals&#34;&gt;Literals&lt;/h2&gt;

&lt;p&gt;The first (highest) field in the token is used to define the literal. This obviously takes a value 0-15.&lt;/p&gt;

&lt;p&gt;Since we might want to encode higher integer, as such we make use of LSIC encoding: If the field is 15 (the meximal value), we read an integer with LSIC and add it to the original value (15) to obtain the literals length.&lt;/p&gt;

&lt;p&gt;Call the final value &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Then we forward the next &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt; bytes from the input stream to the output stream.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_literals_copy_diagram.svg&#34; alt=&#34;We copy from the buffer directly.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;deduplication&#34;&gt;Deduplication&lt;/h2&gt;

&lt;p&gt;The next few bytes are used to define some segment in the already decoded buffer, which is going to be appended to the output buffer.&lt;/p&gt;

&lt;p&gt;This allows us to transmit a position and a length to read from in the already decoded buffer instead of transmitting the literals themself.&lt;/p&gt;

&lt;p&gt;To start with, we read a 16-bit little endian integer. This defines the so called offset, &lt;span  class=&#34;math&#34;&gt;\(O\)&lt;/span&gt;. It is important to understand that the offset is not the starting position of the copied buffer. This starting point is calculated by &lt;span  class=&#34;math&#34;&gt;\(l - O\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt; being the number of bytes already decoded.&lt;/p&gt;

&lt;p&gt;Secondly, similarly to the literals length, if &lt;span  class=&#34;math&#34;&gt;\(t_2\)&lt;/span&gt; is 15 (the maximal value), we use LSIC to &amp;quot;extend&amp;quot; this value and we add the result. This plus 4 yields the number of bytes we will copy from the output buffer. The reason we add 4 is because copying less than 4 bytes would result in a negative expansion of the compressed buffer.&lt;/p&gt;

&lt;p&gt;Now that we know the start position and the length, we can append the segment to the buffer itself:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_deduplicating_diagram.svg&#34; alt=&#34;Copying in action.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;It is important to understand that the end of the segment might not be initializied before the rest of the segment is appended, because overlaps are allowed. This allows a neat trick, namely &amp;quot;runs-length encoding&amp;quot;, where you repeat some sequence a given number of times:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_runs_encoding_diagram.svg&#34; alt=&#34;We repeat the last byte.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Note that the duplicate section is not required if you&#39;re in the end of the stream, i.e. if there&#39;s no more compressed bytes to read.&lt;/p&gt;

&lt;h1 id=&#34;compression&#34;&gt;Compression&lt;/h1&gt;

&lt;p&gt;Until now, we have only considered decoding, not the reverse process.&lt;/p&gt;

&lt;p&gt;A dozen of approaches to compression exists. They have the aspects that they need to be able to find duplicates in the already input buffer.&lt;/p&gt;

&lt;p&gt;In general, there are two classes of such compression algorithms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;HC: High-compression ratio algorithms, these are often very complex, and might include steps like backtracking, removing repeatation, non-greediy.&lt;/li&gt;
&lt;li&gt;FC: Fast compression, these are simpler and faster, but provides a slightly worse compression ratio.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will focus on the FC-class algorithms.&lt;/p&gt;

&lt;p&gt;Binary Search Trees (often B-trees) are often used for searching for duplicates. In particular, every byte iterated over will add a pointer to the rest of the buffer to a B-tree, we call the &amp;quot;duplicate tree&amp;quot;. Now, B-trees allows us to retrieve the largest element smaller than or equal to some key. In lexiographic ordering, this is equivalent to asking the element sharing the largest number of bytes as prefix.&lt;/p&gt;

&lt;p&gt;For example, consider the table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;abcdddd =&amp;gt; 0
bcdddd  =&amp;gt; 1
cdddd   =&amp;gt; 2
dddd    =&amp;gt; 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we search for &lt;code&gt;cddda&lt;/code&gt;, we&#39;ll get a partial match, namely &lt;code&gt;cdddd =&amp;gt; 2&lt;/code&gt;. So we can quickly find out how many bytes they have in common as prefix. In this case, it is 4 bytes.&lt;/p&gt;

&lt;p&gt;What if we found no match or a bad match (a match that shares less than some threshold)? Well, then we write it as literal until a good match is found.&lt;/p&gt;

&lt;p&gt;As you may notice, the dictionary grows linearly. As such, it is important that you reduce memory once in a while, by trimming it. Note that just trimming the first (or last) &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; entries is inefficient, because some might be used often. Instead, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Cache_Replacement_Policies&#34;&gt;cache replacement policy&lt;/a&gt; should be used. If the dictionary is filled, the cache replacement policy should determine which match should be replaced. I&#39;ve found PLRU a good choice of CRP for LZ4 compression.&lt;/p&gt;

&lt;p&gt;Note that you should add additional rules like being addressible (within &lt;span  class=&#34;math&#34;&gt;\(2^{16} + 4\)&lt;/span&gt; bytes of the cursor, which is required because &lt;span  class=&#34;math&#34;&gt;\(O\)&lt;/span&gt; is 16-bit) and being above some length (smaller keys have worse block-level compression ratio).&lt;/p&gt;

&lt;p&gt;Another faster but worse (compression-wise) approach is hashing every four bytes and placing them in a table. This means that you can only look up the latest sequence given some 4-byte prefix. Looking up allows you to progress and see how long the duplicate sequence match. When you can&#39;t go any longer, you encode the literals section until another duplicate 4-byte is found.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;LZ4 is a reasonably simple algorithm with reasonably good compression ratio. It is the type of algorithm that you can implement on an afternoon without much complication.&lt;/p&gt;

&lt;p&gt;If you need a portable and efficient compression algorithm which can be implement in only a few hundreds of lines, LZ4 would be my go-to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On Random-Access Compression</title>
      <link>http://ticki.github.io/blog/on-random-access-compression/</link>
      <pubDate>Sun, 23 Oct 2016 23:25:15 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/on-random-access-compression/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;This post will contains an algorithm I came up with, doing efficient rolling compression. It&#39;s going to be used in &lt;a href=&#34;https://github.com/ticki/tfs&#34;&gt;TFS&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;what-is-rolling-compression&#34;&gt;What is rolling compression?&lt;/h1&gt;

&lt;p&gt;Consider that you have a large file and you want to compress it. That&#39;s easy enough and many algorithms exists for doing so. Now, consider that you want to read or write a small part of the file.&lt;/p&gt;

&lt;p&gt;Most algorithms would require you to decompress, write, and recompress the whole file. Clearly, this gets expensive when the file is big.&lt;/p&gt;

&lt;h1 id=&#34;clusterbased-compression&#34;&gt;Cluster-based compression&lt;/h1&gt;

&lt;p&gt;A cluster is some small fixed-size block (often 512, 1024, or 4096 bytes). We can have a basic cluster allocator by linking unused clusters together. Cluster-centric compression is interesting, because it can exploit the allocator.&lt;/p&gt;

&lt;p&gt;So, the outline is that we compress every &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt; adjacent clusters to some &lt;span  class=&#34;math&#34;&gt;\(n&#39; &lt; n%&gt;\)&lt;/span&gt;, then we can free the excessive clusters in this compressed line.&lt;/p&gt;

&lt;h1 id=&#34;copyonwrite&#34;&gt;Copy-on-write&lt;/h1&gt;

&lt;p&gt;Our algorithm is not writable, but it can be written by allocating, copying, and deallocating. This is called copy-on-write, or COW for short. It is a common technique used in many file systems.&lt;/p&gt;

&lt;p&gt;Essentially, we never write a cluster. Instead, we allocate a new cluster, and copy the data to it. Then we deallocate the old cluster.&lt;/p&gt;

&lt;p&gt;This allows us to approach everything much more functionally, and we thus don&#39;t have to worry about make compressible blocks uncompressible (consider that you overwrite a highly compressible cluster with random data, then you extend a physical cluster containing many virtual clusters, these wouldn&#39;t be possible to have in one cluster).&lt;/p&gt;

&lt;h1 id=&#34;physical-and-virtual-clusters&#34;&gt;Physical and virtual clusters&lt;/h1&gt;

&lt;p&gt;Our goal is really fit multiple clusters into one physical cluster. Therefore, it is essential to distinguish between physical (the stored) and virtual (the compressed) clusters.&lt;/p&gt;

&lt;p&gt;A physical cluster can contain up to 8 virtual clusters. A pointer to a virtual cluster starts with 3 bits defining the index into the physical cluster, which is defined by the rest of the pointer.&lt;/p&gt;

&lt;p&gt;The allocated physical cluster contains 8 bitflags, defining which of the 8 virtual clusters in the physical cluster are used. This allows us to know how many virtual clusters we need to go over before we get the target decompressed cluster.&lt;/p&gt;

&lt;p&gt;When the integer hits zero (i.e. all the virtual clusters are freed), the physical cluster is freed.&lt;/p&gt;

&lt;p&gt;Since an active cluster will never have the state zero, we use this blind state to represent an uncompressed physical cluster. This means we maximally have one byte in space overhead for uncompressible clusters.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/virtual_physical_random_access_compression_diagram.svg&#34; alt=&#34;A diagram&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-physical-cluster-allocator&#34;&gt;The physical cluster allocator&lt;/h1&gt;

&lt;p&gt;The cluster allocator is nothing but a linked list of clusters. Every free cluster links to another free cluster or NIL (no more free clusters).&lt;/p&gt;

&lt;p&gt;This method is called SLOB (Simple List Of Objects) and has the advantage of being complete zero-cost in that there is no wasted space.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/slob_allocation_diagram.svg&#34; alt=&#34;Physical allocation is simply linked list of free objects.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-virtual-cluster-allocator&#34;&gt;The virtual cluster allocator&lt;/h1&gt;

&lt;p&gt;Now we hit the meat of the matter.&lt;/p&gt;

&lt;p&gt;When virtual cluster is allocated, we read from the physical cluster list. The first thing we will check is if we can fit in our virtual cluster into the cluster next to the head of the list (we wrap if we reach the end).&lt;/p&gt;

&lt;p&gt;If we can fit it in &lt;em&gt;and&lt;/em&gt; we have less than 8 virtual clusters in this physical cluster, we will put it into the compressed physical cluster at the first free virtual slot (and then set the respective bitflag):&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/allocating_compressed_virtual_page_into_next_diagram.svg&#34; alt=&#34;We try to fit it into the next cluster.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;If we cannot, we pop the list and use the fully-free physical cluster to store etablish a new stack of virtual clusters. It starts as uncompressed:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/pop_and_create_new_uncompressed_cluster_diagram.svg&#34; alt=&#34;We pop the list and put the virtual cluster in the physical uncompressed slot.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;properties-of-this-approach&#34;&gt;Properties of this approach&lt;/h1&gt;

&lt;p&gt;This approach to writable random-access compression has some very nice properties.&lt;/p&gt;

&lt;h2 id=&#34;compression-miss&#34;&gt;Compression miss&lt;/h2&gt;

&lt;p&gt;We call it a compression miss when we need to pop from the freelist (i.e. we cannot fit it into the cluster next to the head). When you allocate you can maximally have one compression miss, and therefore allocation is constant-time.&lt;/p&gt;

&lt;h2 id=&#34;every-cluster-has-a-sister-cluster&#34;&gt;Every cluster has a sister cluster&lt;/h2&gt;

&lt;p&gt;Because the &amp;quot;next cluster or wrap&amp;quot; function is bijective, we&#39;re sure that we try to insert a virtual cluster to every cluster at least once. This wouldn&#39;t be true if we used a hash function or something else.&lt;/p&gt;

&lt;p&gt;This has the interesting consequence that filled clusters won&#39;t be tried to allocate in multiple times.&lt;/p&gt;

&lt;h1 id=&#34;limitations&#34;&gt;Limitations&lt;/h1&gt;

&lt;p&gt;A number of limitations are in this algorithms. The first and most obvious one is the limitation on the compression ratio. This is a minor one: it limits the ratio to maxmially slightly less than 1:8.&lt;/p&gt;

&lt;p&gt;A more important limitation is fragmentation. If I allocate many clusters and then deallocate some of them such that many adjacent physical clusters only contain one virtual cluster, this row will have a compression ratio of 1:1 until they&#39;re deallocated. Note that it is very rare that this happens, and will only marginally affect the global compression ratio.&lt;/p&gt;

&lt;h1 id=&#34;update-an-idea&#34;&gt;Update: An idea&lt;/h1&gt;

&lt;p&gt;A simple trick can improve performance in some cases. Instead of compressing all the virtual clusters in a physical cluster together, you should compress each virtual cluster seperately and place them sequentially (with some delimiter) in the physical cluster.&lt;/p&gt;

&lt;p&gt;If your compression algorithm is streaming, you can much faster iterate to the right delimiter, and then only decompress that virtual cluster.&lt;/p&gt;

&lt;p&gt;This has the downside of making the compression ratio worse. One solution is to have an initial dictionary (if using a dictionary-based compression algorithm).&lt;/p&gt;

&lt;p&gt;Another idea is to eliminate the cluster state and replace it by repeated delimiters. I need to investigate this some more with benchmarks and so on in order to tell if this is actually superior to having a centralized cluster state.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making Terminal Applications in Rust with Termion</title>
      <link>http://ticki.github.io/blog/making-terminal-applications-in-rust-with-termion/</link>
      <pubDate>Thu, 06 Oct 2016 10:12:22 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/making-terminal-applications-in-rust-with-termion/</guid>
      <description>

&lt;p&gt;This post will walk through the basics of implementing a terminal (TTY) application for both new beginners and experienced users of Rust.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Terminal applications play an important role in many programmers&amp;rsquo; toolchain, from text editors to minigames while your code is compiling. And it&amp;rsquo;s great to know and understand how to make these yourself, so you can create a customized TUI application for your needs.&lt;/p&gt;

&lt;p&gt;Escape codes and TTY I/O is messy, but fortunately there are libraries for this. We will use &lt;a href=&#34;https://github.com/ticki/termion&#34;&gt;Termion&lt;/a&gt;, which is the most feature-complete TUI library in pure Rust.&lt;/p&gt;

&lt;p&gt;Termion is pretty simple and straight-forward. This &amp;ldquo;tutorial&amp;rdquo; or guide is going to walk through these in a manner that even Rust new beginners can understand.&lt;/p&gt;

&lt;h1 id=&#34;understanding-the-tty&#34;&gt;Understanding the TTY&lt;/h1&gt;

&lt;p&gt;Ignoring historical facts, the TTY is the name of the virtual device that takes some stream of text and presents it to the user. As opposed to sophisticated UIs and graphics, it is incredibly simple to get started with.&lt;/p&gt;

&lt;p&gt;The terminal emulator keeps a grid of characters, and a cursor. When you write to the standard output the cell is overwritten with the new character and the cursor moves respectively.&lt;/p&gt;

&lt;p&gt;Take the code,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;println!(&amp;quot;Text here.&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All this does is writing some text to the standard output, and when you run this program, &amp;ldquo;Text here.&amp;rdquo; should appear before the TTY cursor.&lt;/p&gt;

&lt;p&gt;If this is all we can do, how can we create interactive TTY applications? Well, it turns out that there is a whole lot more, we can do.&lt;/p&gt;

&lt;p&gt;Certain sequences represents some operations to the TTY. These are called &amp;ldquo;escape sequences&amp;rdquo; and can do things like changing the color of the text, change the background, moving the cursor, clearing the screen, and so on. Writing these codes by hand quickly gets messy, so we let Termion do it for us:&lt;/p&gt;

&lt;h1 id=&#34;setting-up-termion&#34;&gt;Setting up Termion&lt;/h1&gt;

&lt;p&gt;Start by making sure &lt;code&gt;cargo&lt;/code&gt; is installed, then do&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Initialize a new cargo repository.
cargo new --bin my-tui-app
# Cd into it
cd my-tui-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then open the &lt;code&gt;Cargo.toml&lt;/code&gt; file with your favorite text editor, and add&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;termion = &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To the file under the section &lt;code&gt;[dependencies]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then open up &lt;code&gt;src/lib.rs&lt;/code&gt; and add&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now everything is ready to start!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For documentation, see &lt;a href=&#34;https://github.com/ticki/termion&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;the-structure-of-termion&#34;&gt;The structure of Termion&lt;/h1&gt;

&lt;p&gt;Termion is divided into 8 different modules each providing different functions and primitives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;clear&lt;/code&gt;: For clearing the screen or parts of the screen.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;color&lt;/code&gt;: For changing the foreground or background color of the text.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cursor&lt;/code&gt;: For moving the cursor around.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;event&lt;/code&gt;: For handling mouse cursor or modifiers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;input&lt;/code&gt;: For getting more advanced user input (like asynchronous user input).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raw&lt;/code&gt;: Switching to raw mode (we will get back to this later)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scroll&lt;/code&gt;: Scrolling up or down the text stream.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;style&lt;/code&gt;: Changing the text style or formatting.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;color&#34;&gt;Color&lt;/h2&gt;

&lt;p&gt;Since escapes really are nothing but just another text output, we use the &lt;code&gt;std::fmt::Display&lt;/code&gt; to generate the escape codes. This means that we can use it with macros like &lt;code&gt;write!&lt;/code&gt; or &lt;code&gt;println!&lt;/code&gt;. If we want red text for example, we can do simply:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

// Import the color module.
use termion::color;

fn main() {
    println!(&amp;quot;{red}more red than any comrade{reset}&amp;quot;,
             red   = color::Fg(color::Red),
             reset = color::Fg(color::Reset));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;color::Fg&lt;/code&gt; specifies that we want to change the &lt;em&gt;foreground color&lt;/em&gt; (i.e. the color of the text), &lt;code&gt;color::Fg(color::Reset)&lt;/code&gt; means that we &lt;em&gt;reset&lt;/em&gt; the foreground color.&lt;/p&gt;

&lt;h2 id=&#34;clear&#34;&gt;Clear&lt;/h2&gt;

&lt;p&gt;Clearing the screen allows you to remove text which is already written without overwriting it manually with spaces. For example, I can easily implement the &lt;code&gt;clear&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

// Import the `clear` module.
use termion::clear;

fn main() {
    println!(&amp;quot;{}&amp;quot;, clear::All);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It should be pretty obvious that &lt;code&gt;clear::All&lt;/code&gt; clears the whole grid, but what if we only want to clear the screen partially?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;clear::CurrentLine&lt;/code&gt; will leave the current line empty.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clear::AfterCursor&lt;/code&gt; clears from the cursor to the end of the grid.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clear::BeforeCursor&lt;/code&gt; clears from the cursor to the beginning of the grid.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/clear/index.html&#34;&gt;and so on&amp;hellip;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cursor&#34;&gt;Cursor&lt;/h2&gt;

&lt;p&gt;What if I want to jump back and overwrite what I just wrote? The easy way is to use &lt;code&gt;\r&lt;/code&gt;, which will jump back to the start of the line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::{color, clear};
use std::time::Duration;
use std::thread;

fn main() {
    println!(&amp;quot;{red}more red than any comrade{reset}&amp;quot;,
             red   = color::Fg(color::Red),
             reset = color::Fg(color::Reset));
    // Sleep for a short period of time.
    thread::sleep(Duration::from_millis(300));
    // Go back;
    println!(&amp;quot;\r&amp;quot;);
    // Clear the line and print some new stuff
    print!(&amp;quot;{clear}{red}g{blue}a{green}y{red} space communism{reset}&amp;quot;,
            clear = clear::CurrentLine,
            red   = color::Fg(color::Red),
            blue  = color::Fg(color::Blue),
            green = color::Fg(color::Green),
            reset = color::Fg(color::Reset));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But actually, &lt;code&gt;\r&lt;/code&gt; is pretty limited, because it only allows us to jump to the start of the line. What if we want to jump to an arbitrary cell in the text grid?&lt;/p&gt;

&lt;p&gt;Well, we can do that with &lt;code&gt;cursor::Goto&lt;/code&gt;, say we want to print the text at (4,2):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::{color, cursor, clear};

fn main() {
    println!(&amp;quot;{clear}{goto}{red}more red than any comrade{reset}&amp;quot;,
             // Full screen clear.
             clear = clear::All,
             // Goto the cell.
             goto  = cursor::Goto(4, 2),
             red   = color::Fg(color::Red),
             reset = color::Fg(color::Reset));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;style&#34;&gt;Style&lt;/h2&gt;

&lt;p&gt;What if I want my gay space communism to have style?&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;style&lt;/code&gt; module provides escape codes for that. For example, let&amp;rsquo;s print it in bold (&lt;code&gt;style::Bold&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::{color, clear, style};

fn main() {
    println!(&amp;quot;{bold}{red}g{blue}a{green}y{red} space communism{reset}&amp;quot;,
            bold  = style::Bold,
            red   = color::Fg(color::Red),
            blue  = color::Fg(color::Blue),
            green = color::Fg(color::Green),
            reset = style::Reset);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Neat. Now we can control the cursor, clear stuff, set color, and set style. That should be good enough to get us started.&lt;/p&gt;

&lt;h1 id=&#34;entering-raw-mode&#34;&gt;Entering raw mode&lt;/h1&gt;

&lt;p&gt;Without raw mode, you cannot write a proper interactive TTY application. Raw mode gives you complete control over the TTY:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It disables the line buffering: As you might notice, your command-line application tends to behave like the command-line. The programs will first get the input when the user types &lt;code&gt;\n&lt;/code&gt;. Raw mode makes the program get the input after every key stroke.&lt;/li&gt;
&lt;li&gt;It disables displaying the input: Without raw mode, the things you type appear on the screen, making it insufficient for most interactive TTY applications, where keys can represent controls and not textual input.&lt;/li&gt;
&lt;li&gt;It disables canonicalization of the output: For example, &lt;code&gt;\n&lt;/code&gt; represents &amp;ldquo;go one cell down&amp;rdquo; not &amp;ldquo;break the line&amp;rdquo;, for line breaks &lt;code&gt;\n\r&lt;/code&gt; is needed.&lt;/li&gt;
&lt;li&gt;It disables scrolling.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, how do we enter raw mode?&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not that hard:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use termion::raw::IntoRawMode;
use std::io::{Write, stdout};

fn main() {
    // Enter raw mode.
    let mut stdout = stdout().into_raw_mode().unwrap();

    // Write to stdout (note that we don&#39;t use `println!`)
    writeln!(stdout, &amp;quot;Hey there.&amp;quot;).unwrap();

    // Here the destructor is automatically called, and the terminal state is restored.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;inputs&#34;&gt;Inputs&lt;/h1&gt;

&lt;p&gt;Keys and modifiers are somewhat oddly encoded in the ANSI standards, and fortunately Termion parses those for you. If you take a look at the &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/input/trait.TermRead.html&#34;&gt;&lt;code&gt;TermRead&lt;/code&gt;&lt;/a&gt; trait, you&amp;rsquo;ll see the method called &lt;code&gt;keys&lt;/code&gt;. This returns an iterator over &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/event/enum.Key.html&#34;&gt;&lt;code&gt;Key&lt;/code&gt;&lt;/a&gt;, an enum which contains the parsed keys.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::event::Key;
use termion::input::TermRead;
use termion::raw::IntoRawMode;
use std::io::{Write, stdout, stdin};

fn main() {
    // Get the standard input stream.
    let stdin = stdin();
    // Get the standard output stream and go to raw mode.
    let mut stdout = stdout().into_raw_mode().unwrap();

    write!(stdout, &amp;quot;{}{}q to exit. Type stuff, use alt, and so on.{}&amp;quot;,
           // Clear the screen.
           termion::clear::All,
           // Goto (1,1).
           termion::cursor::Goto(1, 1),
           // Hide the cursor.
           termion::cursor::Hide).unwrap();
    // Flush stdout (i.e. make the output appear).
    stdout.flush().unwrap();

    for c in stdin.keys() {
        // Clear the current line.
        write!(stdout, &amp;quot;{}{}&amp;quot;, termion::cursor::Goto(1, 1), termion::clear::CurrentLine).unwrap();

        // Print the key we type...
        match c.unwrap() {
            // Exit.
            Key::Char(&#39;q&#39;) =&amp;gt; break,
            Key::Char(c)   =&amp;gt; println!(&amp;quot;{}&amp;quot;, c),
            Key::Alt(c)    =&amp;gt; println!(&amp;quot;Alt-{}&amp;quot;, c),
            Key::Ctrl(c)   =&amp;gt; println!(&amp;quot;Ctrl-{}&amp;quot;, c),
            Key::Left      =&amp;gt; println!(&amp;quot;&amp;lt;left&amp;gt;&amp;quot;),
            Key::Right     =&amp;gt; println!(&amp;quot;&amp;lt;right&amp;gt;&amp;quot;),
            Key::Up        =&amp;gt; println!(&amp;quot;&amp;lt;up&amp;gt;&amp;quot;),
            Key::Down      =&amp;gt; println!(&amp;quot;&amp;lt;down&amp;gt;&amp;quot;),
            _              =&amp;gt; println!(&amp;quot;Other&amp;quot;),
        }

        // Flush again.
        stdout.flush().unwrap();
    }

    // Show the cursor again before we exit.
    write!(stdout, &amp;quot;{}&amp;quot;, termion::cursor::Show).unwrap();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What the above snippet does is to open a blank screen, where it informs you what keys and modifiers you type as you press keys.&lt;/p&gt;

&lt;h1 id=&#34;asynchronized-stdin&#34;&gt;Asynchronized stdin&lt;/h1&gt;

&lt;p&gt;One interesting problem you will run into, while writing certain terminal application is that the stdin is blocking, and you need to wait to the user giving the input. This potentially could block your application from doing work while waiting for user input (e.g. you freeze the graphics).&lt;/p&gt;

&lt;p&gt;Fortunately, Termion has a solution to that &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/fn.async_stdin.html&#34;&gt;&lt;code&gt;termion::async_stdin()&lt;/code&gt;&lt;/a&gt;. In principle, it is really simple. It works around the limitation to TTYs by using another thread to read from the stdin, and when your main thread needs to read from the stream, it pops from a concurrent queue to read the bytes. It doesn&amp;rsquo;t scale to things like byte streams, but it works seamlessly with user input.&lt;/p&gt;

&lt;h1 id=&#34;mouse&#34;&gt;Mouse&lt;/h1&gt;

&lt;p&gt;You can read mouse clicks etc. by converting your stdin stream to &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/input/struct.MouseTerminal.html&#34;&gt;&lt;code&gt;termion::input::MouseTerminal&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::event::*;
use termion::cursor;
use termion::input::{TermRead, MouseTerminal};
use termion::raw::IntoRawMode;
use std::io::{self, Write};

fn main() {
    let stdin = io::stdin();
    let mut stdout = MouseTerminal::from(io::stdout().into_raw_mode().unwrap());
    // ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can clear the screen:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;    writeln!(stdout,
             &amp;quot;{}{}q to exit. Type stuff, use alt, click around...&amp;quot;,
             termion::clear::All,
             termion::cursor::Goto(1, 1))
        .unwrap();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you can read mouse inputs through the &lt;code&gt;events()&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;    for c in stdin.events() {
        let evt = c.unwrap();
        match evt {
            Event::Key(Key::Char(&#39;q&#39;)) =&amp;gt; break,
            Event::Mouse(me) =&amp;gt; {
                match me {
                    MouseEvent::Press(_, a, b) |
                    MouseEvent::Release(a, b) |
                    MouseEvent::Hold(a, b) =&amp;gt; {
                        write!(stdout, &amp;quot;{}&amp;quot;, cursor::Goto(a, b)).unwrap();
                    }
                }
            }
            _ =&amp;gt; {}
        }
        stdout.flush().unwrap();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, if you click around or hold your your mouse, the TTY cursor should follow.&lt;/p&gt;

&lt;h1 id=&#34;a-few-extra-tricks&#34;&gt;A few extra tricks&lt;/h1&gt;

&lt;h2 id=&#34;the-terminal-size&#34;&gt;The terminal size&lt;/h2&gt;

&lt;p&gt;Sometimes you might want to center or align things. This need the terminal size, which can be obtained by &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/fn.terminal_size.html&#34;&gt;&lt;code&gt;termion::terminal_size()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;bypassing-piped-input&#34;&gt;Bypassing piped input&lt;/h2&gt;

&lt;p&gt;Sometimes you might want to pipe some input to your program while controling the TTY. This is actually not that hard. With &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/fn.get_tty.html&#34;&gt;&lt;code&gt;termion::get_tty()&lt;/code&gt;&lt;/a&gt;, you can read and write from the TTY, while still being able to read or write to stdin/stdout via &lt;code&gt;std::io&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;truecolor&#34;&gt;Truecolor&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/color/struct.Rgb.html&#34;&gt;&lt;code&gt;termion::color::Rgb(r, g, b)&lt;/code&gt;&lt;/a&gt; allows you to use full 24-bit truecolor.&lt;/p&gt;

&lt;h1 id=&#34;trying-all-this-out-yourself&#34;&gt;Trying all this out yourself&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s a lot of things you can do as well:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Writing a simple nano clone.&lt;/li&gt;
&lt;li&gt;Writing a TUI music player.&lt;/li&gt;
&lt;li&gt;Writing a TODO list manager.&lt;/li&gt;
&lt;li&gt;Writing an interactive TUI file manager.&lt;/li&gt;
&lt;li&gt;Writing a game.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;reference-programs-and-examples&#34;&gt;Reference programs and examples&lt;/h1&gt;

&lt;p&gt;If you need a hands-on reference or examples on using termion, you can check out one of the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ticki/termion/tree/master/examples&#34;&gt;The termion examples&lt;/a&gt;* (easy/overview)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ticki/rem/blob/master/src/main.rs&#34;&gt;An utility to set countdowns/reminders in the terminal&lt;/a&gt;* (easy)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hoodie/battery-rs&#34;&gt;An utility to get the battery status from command line&lt;/a&gt;* (easy)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/redox-os/games-for-redox/blob/master/src/ice/main.rs&#34;&gt;Pokemon-style ice sliding puzzle for terminal&lt;/a&gt;* (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/redox-os/games-for-redox/blob/master/src/minesweeper/main.rs&#34;&gt;Minesweeper implementation&lt;/a&gt;* (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/redox-os/games-for-redox/blob/master/src/snake/main.rs&#34;&gt;Snake implementation&lt;/a&gt; (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ca1ek/ircim&#34;&gt;An IRC client&lt;/a&gt; (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MovingtoMars/liner&#34;&gt;A line-editing library&lt;/a&gt; (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/IGI-111/Smith&#34;&gt;A standalone editor&lt;/a&gt; (hard)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Munksgaard/inquirer-rs&#34;&gt;A more high-level TTY library built on top of Termion&lt;/a&gt; (hard)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/little-dude/xi-tui&#34;&gt;A Termion Xi-editor frontend&lt;/a&gt; (hard)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;= Recommended as reference or example.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want your program added, just contact me.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Hoare Logic for Rust</title>
      <link>http://ticki.github.io/blog/a-hoare-logic-for-rust/</link>
      <pubDate>Sat, 24 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/a-hoare-logic-for-rust/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;Lately, I&#39;ve been working on a Hoare-logic-based model of the Rust MIR, which I will introduce in the post. This is a minor step towards a memory model of Rust, and it allows formalization of programs and their behavior.&lt;/p&gt;

&lt;p&gt;This project was born out of the effort to formalize &lt;a href=&#34;https://github.com/redox-os/redox&#34;&gt;the Redox kernel&lt;/a&gt; and &lt;a href=&#34;https://github.com/redox-os/ralloc/tree/skiplist&#34;&gt;the ralloc memory allocator&lt;/a&gt; as well as coming up with a &lt;a href=&#34;https://github.com/rust-lang/rfcs/issues/1447&#34;&gt;Rust memory model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here I will walk through the techniques, axioms, and transformations in detail. I&#39;ve divided this post into three parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An introduction to Hoare logic: An gentle introduction for the beginners (can be skipped if you&#39;re already familiar with Hoare logic).&lt;/li&gt;
&lt;li&gt;Applying Hoare logic to the Rust MIR: Notably dropping structured programming in favour of a lower-level goto-based representation, and how it helps simplifying certain things.&lt;/li&gt;
&lt;li&gt;Reasoning about pointers: Pointers are notoriously hard to reason about. Here we try to formalize their behavior and give various insight on how they can be reasoned about. Priory to this part, we assume that pointers doesn&#39;t exist.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This blog post is not a formal specification or a paper, but rather a mere introduction to the subject and proposed axioms.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If the math doesn&#39;t show up properly, reload the page.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&#34;an-introduction-to-hoare-logic&#34;&gt;An introduction to Hoare logic&lt;/h1&gt;

&lt;p&gt;So, what is Hoare logic? Well, it&#39;s a set of axioms and inference rules allowing one to reason about &lt;em&gt;imperative programs&lt;/em&gt; in a rigorous manner.&lt;/p&gt;

&lt;p&gt;The program is divided into so called &lt;strong&gt;Hoare triples&lt;/strong&gt;, denoted &lt;span  class=&#34;math&#34;&gt;\(\{P\} \ S \ \{Q\}\)&lt;/span&gt;. &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; is called the &amp;quot;precondition&amp;quot;. Informally, if &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; is satisfied, then after &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt; (the statement or instruction) has been executed, &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; (the postcondition) should be true. In other words, &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; is true before &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt;, and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; should be true after.&lt;/p&gt;

&lt;p&gt;In fact, we can view &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt; as a function on the state space, going from &lt;span  class=&#34;math&#34;&gt;\(\sigma\)&lt;/span&gt; satisfying property &lt;span  class=&#34;math&#34;&gt;\(P(\sigma)\)&lt;/span&gt; to a state &lt;span  class=&#34;math&#34;&gt;\(S(\sigma) = \sigma&#39;\)&lt;/span&gt; satisfying the postcondition, &lt;span  class=&#34;math&#34;&gt;\(Q(\sigma&#39;)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Thus a Hoare triple can be seen as a 3-tuple&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[(P, f, Q)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;satisfying:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[P(\sigma) \to Q(f(\sigma))\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It turns out that this interpretation is a strong one, and we will use it throughout the post to derive the Hoare rules, some of which follows directly from this interpretation.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\forall \sigma.P(\sigma) \to Q(f(\sigma))}{\{P\}\ f\ \{Q\}} \qquad \frac{\{P\}\ f\ \{Q\}}{\forall \sigma.P(\sigma) \to Q(f(\sigma))}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(on a side note, this notation should be understood as: what is below the line is true if what is above is true)&lt;/p&gt;

&lt;h2 id=&#34;an-example&#34;&gt;An example&lt;/h2&gt;

&lt;p&gt;Suppose we have the program,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;// a = 4
a += 2;
// a = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is expressed by the Hoare triple&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\{a = 4\} \ a \gets a + 2 \ \{a = 6\}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So far, we have only introduced the notation, which in itself is worthless, what&#39;s really the core is the rules that allows us to reason about valid Hoare triples. We need a way to essentially construct new Hoare triples from old ones.&lt;/p&gt;

&lt;h2 id=&#34;rules-and-axioms&#34;&gt;Rules and axioms&lt;/h2&gt;

&lt;h3 id=&#34;empty-statement-rule&#34;&gt;Empty statement rule&lt;/h3&gt;

&lt;p&gt;The empty statement rule states that: Let &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt; be any statement which carries no side-effect, then &lt;span  class=&#34;math&#34;&gt;\(\{P\} \ S \ \{P\}\)&lt;/span&gt;, or in inference line notation:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{S \text{ is pure}}{\{P\} \ S \ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This rule is relatively simple: If the state is not changed, the invariants are neither. Note that this is only true for effect-less statements, since the statement could otherwise change variables or in other ways invalidate the postcondition.&lt;/p&gt;

&lt;p&gt;In fact, we can express it in terms of the identity function, &lt;span  class=&#34;math&#34;&gt;\(f(x)=x\)&lt;/span&gt;. Then,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[P(x) \to P(f(x)) = P(x)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Hence, the triple is valid.&lt;/p&gt;

&lt;h3 id=&#34;composition-rule&#34;&gt;Composition rule&lt;/h3&gt;

&lt;p&gt;The composition rule allows you to concatenate two statements (into a Hoare triple) if the first statement&#39;s postcondition is equal to the second statement&#39;s precondition:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P\}\ S\ \{Q\}, \quad \{Q\}\ T\ \{R\}}{\{P\}\ S;T\ \{R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It is left as an exercise for the reader to verify the correctness of the rule above.&lt;/p&gt;

&lt;h3 id=&#34;strengthening-and-weakening-conditions&#34;&gt;Strengthening and weakening conditions&lt;/h3&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{P_1 \to P_2,\quad \{P_2\}\ S\ \{Q_2\},\quad Q_2 \to Q_1}{\{P_1\}\ S\ \{Q_1\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So, what&#39;s going on here? Well, &lt;span  class=&#34;math&#34;&gt;\(P_1\)&lt;/span&gt; implies &lt;span  class=&#34;math&#34;&gt;\(P_2\)&lt;/span&gt;, so we can replace the precondition by a stronger version which implies the old one. The same cannot be applied to postcondition, because the strengthened precondition might not yield the strengthened postcondition after the statement. We can however replace it by a weaker postcondition (i.e. one which is implied by original postcondition).&lt;/p&gt;

&lt;p&gt;We can always weaken guarantees, but never assumptions, since the assumption is what the guarantee relies on. Assumptions can be made stronger, however.&lt;/p&gt;

&lt;p&gt;It is left as an exercise for the reader to verify the correctness of the rule above.&lt;/p&gt;

&lt;h3 id=&#34;the-assignment-axiom&#34;&gt;The assignment axiom&lt;/h3&gt;

&lt;p&gt;This axiom is the most important. It allows for reasoning about preconditions in the case of assignments. It is absolutely essential to Hoare logic.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P[x \gets E]\}\ x \gets E\ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(P[x \gets E]\)&lt;/span&gt; denotes replacing every free (unbound) &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Let&#39;s say &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; involves some assertion about &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;, then we can move it over the assignment (to the precondition) replacing &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; with the right-hand-side of the assignment, because every occurence of &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; represents said value anyway, so substituting the value &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; represents for &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; won&#39;t change the structure.&lt;/p&gt;

&lt;p&gt;Let&#39;s say we have the statement, &lt;span  class=&#34;math&#34;&gt;\(x \gets x + 2\)&lt;/span&gt;, with the postcondition &lt;span  class=&#34;math&#34;&gt;\(\{x = 6\}\)&lt;/span&gt;, we can then derive the Hoare triple:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\{x + 2 = 6\}\ x \gets x + 2\ \{x = 6\}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One thing that is surprising, but also incredibly important, is that you substitute it into the precondition and not the postcondition. To see why such a rule (&lt;span  class=&#34;math&#34;&gt;\(\{P\}\ x \gets E\ \{P[x \gets E]\}\)&lt;/span&gt;) would be wrong, observe how you could derive &lt;span  class=&#34;math&#34;&gt;\(\{x = 1\}\ x \gets 2\ \{2 = 1\}\)&lt;/span&gt;, which is clearly false.&lt;/p&gt;

&lt;p&gt;It is also worth noting that, in this context, expressions cannot carry side-effects. We&#39;ll cover this in detail in part two.&lt;/p&gt;

&lt;h3 id=&#34;conditional-rule&#34;&gt;Conditional rule&lt;/h3&gt;

&lt;p&gt;So far, we have only covered a simple language without loops, conditionals, and other forms of branches.&lt;/p&gt;

&lt;p&gt;The first (and simplest) form of branches is a conditional non-cyclic branch (&lt;code&gt;if&lt;/code&gt;). These behaves in a very simple way:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{C \land P\}\ B\ \{Q\},\quad \{\neg C \land P\}\ E\ \{Q\}}{\{P\}\ \textbf{if } C \textbf{ then } B \textbf{ else } E \textbf{ end}\ \{Q\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As complex this looks, it&#39;s actually relatively simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In your &lt;code&gt;if&lt;/code&gt; statement&#39;s body, you can safely assume the &lt;code&gt;if&lt;/code&gt; condition to be true.&lt;/li&gt;
&lt;li&gt;If both branches shares their postcondition (&lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt;), then the &lt;code&gt;if&lt;/code&gt; statement does as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As an example, consider the code,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;if x == 4 {
    // I can safely assume that x = 4 here.
    ...
    x = 2;
    // Now x = 2.
} else {
    // I can safely assume that x ≠ 4 here.
    ...
    x = 2;
    // Now x = 2.
}
// All branches share postcondition, so the whole if-statement does as well: x = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-loop-rule&#34;&gt;The loop rule&lt;/h3&gt;

&lt;p&gt;The loop rule reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{I \land C\}\ B\ \{I\}}{\{I\}\ \textbf{while } C \textbf{ do } B \textbf{ done}\ \{I \land \neg C\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(I\)&lt;/span&gt; is called the &lt;em&gt;loop invariant&lt;/em&gt;, i.e. the condition which is true before and after the loop. The loop will terminate when &lt;span  class=&#34;math&#34;&gt;\(\neg C\)&lt;/span&gt;, hence the postcondition of the loop.&lt;/p&gt;

&lt;p&gt;As a simple example, take the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;let mut x = 3;
let mut y = 4;
// Precondition: x == 3 (loop invariant)
while y &amp;lt; 100 {
    // Precondition: y &amp;lt; 100 &amp;amp;&amp;amp; x == 3
    y += 1;
    // Posttcondition: x == 3 (loop invariant)
}
// Postcondition: !(y &amp;lt; 100) ⇒ y &amp;gt;= 100
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;applying-hoare-logic-to-the-mir&#34;&gt;Applying Hoare logic to the MIR&lt;/h1&gt;

&lt;p&gt;The Rust MIR is in many ways an interesting language. It can be seen as an extremely stripped-down version of Rust. What we&#39;ll work with is the MIR from the last compiler pass.&lt;/p&gt;

&lt;h2 id=&#34;the-rust-mir&#34;&gt;The Rust MIR&lt;/h2&gt;

&lt;p&gt;The Rust MIR has no structural control flow. It directly resembles the CFG of the program.&lt;/p&gt;

&lt;p&gt;There are three concepts we must be familiar with to understand the Rust MIR:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;: A graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Basic blocks&lt;/strong&gt;: The nodes in the graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terminators&lt;/strong&gt;: The edges in the graph.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&#39;ll not get into the representation of scopes and type information the MIR contains.&lt;/p&gt;

&lt;h3 id=&#34;functions&#34;&gt;Functions&lt;/h3&gt;

&lt;p&gt;Taking aside the type information, functions have two components: A set of variables and a Control Flow Graph.&lt;/p&gt;

&lt;p&gt;The function starts with a bunch of variable declarations (arguments, temporaries, and variables). There&#39;s one implicit variable, the &lt;code&gt;return&lt;/code&gt; variable, which contains the return values.&lt;/p&gt;

&lt;p&gt;Secondly, there&#39;s a set of basic blocks, as well as a starting block.&lt;/p&gt;

&lt;h3 id=&#34;basic-blocks&#34;&gt;Basic blocks&lt;/h3&gt;

&lt;p&gt;Basic blocks are the nodes of the CFG. They each represent a series of statements. In our model, we can wlog. assume that a statement is simply an assignment, &lt;span  class=&#34;math&#34;&gt;\(x \gets y\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt; is an operand. In other words, a basic block is of the form &lt;span  class=&#34;math&#34;&gt;\((x_1 \gets y_1; x_2 \gets y_2; \ldots; x_n \gets y_n, t)\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(t\)&lt;/span&gt; being the terminator.&lt;/p&gt;

&lt;p&gt;In fact, we can go even further: A statement is a single assignment. This can be shown by simply constructing a map between the two graphs (by using the goto terminator to chain).&lt;/p&gt;

&lt;p&gt;Note that there are two kinds of assignments. Up until now, we have only considered the &lt;em&gt;simple assignment&lt;/em&gt; &lt;span  class=&#34;math&#34;&gt;\(x \gets y\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt; being a simple, effectless expression. There&#39;s actually a second form of assignment, the function call assignment, &lt;span  class=&#34;math&#34;&gt;\(x \gets f(y)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;In such an assignment, the function can change the state of the program, and thus care must be taken, since you cannot always use the assignment axiom. We&#39;ll get back to that later on.&lt;/p&gt;

&lt;h3 id=&#34;terminators&#34;&gt;Terminators&lt;/h3&gt;

&lt;p&gt;Terminators are what binds basic blocks together. Every basic block has an associated terminator, which takes one of the following forms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Return from the current function: &lt;span  class=&#34;math&#34;&gt;\(\textbf{return}\)&lt;/span&gt;. The return value is stored in the &lt;code&gt;return&lt;/code&gt; variable.&lt;/li&gt;
&lt;li&gt;Calling a diverging function (&amp;quot;transferring&amp;quot; to the function), &lt;span  class=&#34;math&#34;&gt;\(f(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Non-conditionally jumping to another block &lt;span  class=&#34;math&#34;&gt;\(\textbf{goto}(b)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Jumping to another block if a condition is true, &lt;span  class=&#34;math&#34;&gt;\(\textbf{if}_c(b_1, b_2)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(there is a few - in the implementation - we ignore in our model for simplification purposes)&lt;/p&gt;

&lt;p&gt;Notice how none of these are structural. All are based around gotos. Not only does this simplify our analysis, but it&#39;s also more near to the machine representation.&lt;/p&gt;

&lt;p&gt;As an example, let&#39;s write a program that finds the 10th Fibonacci number:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://i.imgur.com/gk6b2ZQ.png&#34; alt=&#34;Tenth Fibonacci number&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;First of all, the program starts by assigning starting values. Then it enters a loop with a conditional branch in the end (is 10 reached yet?). In this loop we do the classic, add the two numbers and shift one down. When the loops ends, we assign the return value, and then return from the function.&lt;/p&gt;

&lt;h2 id=&#34;reasoning-about-the-mir&#34;&gt;Reasoning about the MIR&lt;/h2&gt;

&lt;h3 id=&#34;unconditional-gotos&#34;&gt;Unconditional gotos&lt;/h3&gt;

&lt;p&gt;The first rule is the non-structural equivalent of the composition rule. All it says is that for a goto-statement to be valid, the precondition of the target basic block must be true:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P\}\ b_1\ \{Q\}, \quad \{Q\}\ b_2\ \{R\}}{\{P\}\ b_1; \textbf{goto}(b_2)\ \{R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&#34;conditional-gotos&#34;&gt;Conditional gotos&lt;/h3&gt;

&lt;p&gt;Conditional gotos are interesting in that it allows us to reason about both while-loops and if-statements in only run rule.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P \land C\}\ b_1\ \{Q\},\quad \{P \land \neg C\}\ b_2\ \{Q\}}{\{P\}\ \textbf{if}_C(b_1, b_2)\ \{Q\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It is the non-structural equivalent of the conditional rule, we described earlier.&lt;/p&gt;

&lt;h3 id=&#34;function-calls&#34;&gt;Function calls&lt;/h3&gt;

&lt;p&gt;Functions take the form &lt;span  class=&#34;math&#34;&gt;\(f(x) \stackrel{\text{def}}{=} \{P(x)\}\ b\ \{Q(x)\}\)&lt;/span&gt;, i.e. an initial starting block, &lt;span  class=&#34;math&#34;&gt;\(b\)&lt;/span&gt;, and a precondition and postcondition, respectively.&lt;/p&gt;

&lt;p&gt;The rule of correctness for function calls reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{f(x) = \{P(x)\}\ b\ \{Q(x, \textbf{return})\}}{\{P(y) \land R[x \gets f(y)]\}\ x \gets f(y)\ \{Q(y, x) \land R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This one is a big one. Let&#39;s break it up:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The assumption (above the inference line) states that &lt;span  class=&#34;math&#34;&gt;\(f(x)\)&lt;/span&gt; is a Hoare triple with the precondition and postcondition being terms depending on the argument.&lt;/li&gt;
&lt;li&gt;The postcondition depends on the return value of &lt;span  class=&#34;math&#34;&gt;\(f(x)\)&lt;/span&gt; as well.&lt;/li&gt;
&lt;li&gt;The conclusion (below the inference line) consists of a Hoare triple with an assignment to &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The postcondition of the assignment is &lt;span  class=&#34;math&#34;&gt;\(Q(y, x)\)&lt;/span&gt; which express that the return value of the function is assigned to &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;, and the argument is &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt;. This is logically joined with &lt;span  class=&#34;math&#34;&gt;\(R\)&lt;/span&gt;, which is carried over to the other side:&lt;/li&gt;
&lt;li&gt;The precondition consists of &lt;span  class=&#34;math&#34;&gt;\(R[x \gets f(y)]\)&lt;/span&gt;, in a similar manner to the assignment axiom, as well as &lt;span  class=&#34;math&#34;&gt;\(P(y)\)&lt;/span&gt;, the precondition of the function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that this rule will be modified later when we introduce pointers into our model.&lt;/p&gt;

&lt;p&gt;Take this imaginary program:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn subtract_without_overflow(a: u32, b: u32) -&amp;gt; u32 {
    // Precondition: b ≤ a
    a - b
    // Postcondition: return ≤ a
}

fn main() {
    let mut n = 0;
    let mut res;
    while n &amp;lt; 10 {
        res = subtract_without_overflow(10, n);
        // Postcondition: res &amp;lt; 10 (this is what we&#39;re going to prove!)
        n += 1;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We know here that the condition for the loop is &lt;span  class=&#34;math&#34;&gt;\(x &lt; 10\)&lt;/span&gt;, as such we set:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;= \mathtt{res}\\
y &amp;= (10, n)\\
R &amp;= [\mathtt{res} &lt; 10]\\
P((a, b)) &amp;= [b \leq a]\\
Q((a, b), r) &amp;= [r \leq a]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Plug it all in, and get:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{f(a, b) = \{n \leq a\}\ S\ \{f(a, b) \leq a\}}{\{n \leq 10 \land f(10, n) &lt; 10\}\ x \gets f(10, n)\ \{f(10, n) \leq 10 \land \mathtt{res} &lt; 10\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The desired result is obtained: the precondition implies that &lt;span  class=&#34;math&#34;&gt;\(n &lt; 10\)&lt;/span&gt;, which is also the loop condition.&lt;/p&gt;

&lt;!--  --&gt;

&lt;p&gt;Thus, we can conclude that there is no overflow in the program. Cool, no?&lt;/p&gt;

&lt;h3 id=&#34;dont-repeat-yourself&#34;&gt;Don&#39;t repeat yourself!&lt;/h3&gt;

&lt;p&gt;The rest of the rules are exactly matching the &amp;quot;classical&amp;quot; Hoare logic axioms. In other words, the assignment axiom, skip axiom, and consequence axiom remains unchanged.&lt;/p&gt;

&lt;h1 id=&#34;reasoning-about-pointers&#34;&gt;Reasoning about pointers&lt;/h1&gt;

&lt;p&gt;This is a tricky subject. Pointers are notorious for being hard to reason about. In fact, they are probably the single hardest subject in program verification.&lt;/p&gt;

&lt;h2 id=&#34;approach-1-global-reasoning&#34;&gt;Approach 1: Global reasoning&lt;/h2&gt;

&lt;p&gt;We could simply consider memory as one big array, in which pointers are indexes, but it turns out such a model is not only non-local, but also very messy, as such we need to derive a more expressive and convenient model to be able to reason about pointers without too much hassle.&lt;/p&gt;

&lt;h2 id=&#34;approach-2-relational-alias-analysis&#34;&gt;Approach 2: Relational alias analysis&lt;/h2&gt;

&lt;p&gt;To start with, I&#39;ll introduce a model I call &amp;quot;relational alias analysis&amp;quot;. We define an equivalence relation, &lt;span  class=&#34;math&#34;&gt;\(\sim\)&lt;/span&gt;, on the set of variables. This equivalence relation tells if two variables are &lt;em&gt;aliased&lt;/em&gt; (i.e. pointers to the same location).&lt;/p&gt;

&lt;h3 id=&#34;aliasing-variables&#34;&gt;Aliasing variables&lt;/h3&gt;

&lt;p&gt;The first axiom reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{x \sim y\}}{\{x = y\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;i.e. if two variables are aliased, they&#39;re equal.&lt;/p&gt;

&lt;p&gt;This is perhaps more of a definition than an axiom. None the less, it describes the semantics of our alias relation.&lt;/p&gt;

&lt;p&gt;Then we describe the behavior of alias asignments:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{A = \textbf{alias}(a)\}\ a \stackrel{\text{alias}}{\gets} b\ \{\textbf{alias}(a) = A \cup \{b\}\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(&lt;span  class=&#34;math&#34;&gt;\(\textbf{alias}(x)\)&lt;/span&gt; defines the equivalence class of &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; under &lt;span  class=&#34;math&#34;&gt;\(\sim\)&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;This allows for declaring a variable to be aliased with another variable.&lt;/p&gt;

&lt;h3 id=&#34;assignment-axiom-for-aliased-values&#34;&gt;Assignment axiom for aliased values&lt;/h3&gt;

&lt;p&gt;Preconditions and postconditions can contain statements on the value behind the pointer, which has the unfortunate consequence that the old assignment axiom schema is no longer valid.&lt;/p&gt;

&lt;p&gt;In fact, we simply need to observe that previously, we had &lt;span  class=&#34;math&#34;&gt;\(\textbf{alias}(x) = \{x\}\)&lt;/span&gt;. Now that we introduced aliased values, the situation changed, and the equivalence class can be arbitrarily large.&lt;/p&gt;

&lt;p&gt;We put,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P[\textbf{alias}(x) \gets E]\}\ x \gets E\ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Note that &lt;span  class=&#34;math&#34;&gt;\(P[A \gets E]\)&lt;/span&gt; means that we replace every element &lt;span  class=&#34;math&#34;&gt;\(a \in A\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;In other words, we do the same as before except that we assign the value to &lt;em&gt;all&lt;/em&gt; the aliased variables.&lt;/p&gt;

&lt;h3 id=&#34;insufficiency&#34;&gt;Insufficiency&lt;/h3&gt;

&lt;p&gt;This model allows reasoning about aliases, but &lt;em&gt;not&lt;/em&gt; pointers in general. In fact, it cannot reason about &lt;code&gt;noalias&lt;/code&gt; pointers, deallocation, and pointer arithmetics.&lt;/p&gt;

&lt;h2 id=&#34;approach-3-separation-logic&#34;&gt;Approach 3: Separation logic&lt;/h2&gt;

&lt;p&gt;Separation logic was originally introduced by JC Reynolds in one of the most cited program verification papers ever. It is more complex than the alternative model we just presented, but also more expressive in some cases.&lt;/p&gt;

&lt;h3 id=&#34;modeling-memory&#34;&gt;Modeling memory&lt;/h3&gt;

&lt;p&gt;Our model of memory consists of multiple new notations. First of all, the model becomes memory aware. We use &lt;span  class=&#34;math&#34;&gt;\(p \mapsto x\)&lt;/span&gt; to denote that some pointer, &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt;, maps to the value &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;We use the notation &lt;span  class=&#34;math&#34;&gt;\(\mathcal{H}(p)\)&lt;/span&gt; to denote pointer reads. The reason we keep the notation function-like is because it is, in fact, just a function! It simply maps pointers to values. We can define,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{p \mapsto x}{\mathcal{H}(p) = x}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We denote pointer writes by &lt;span  class=&#34;math&#34;&gt;\(p \stackrel{\text{ptr}}{\gets} x\)&lt;/span&gt;.&lt;/p&gt;

&lt;h4 id=&#34;disjointness&#34;&gt;Disjointness&lt;/h4&gt;

&lt;p&gt;The first feature of separation logic is the notion of &amp;quot;separate conjunction&amp;quot;, denotes &lt;span  class=&#34;math&#34;&gt;\(P * Q\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;This asserts that &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; are both true and independent, i.e. their &amp;quot;heaps&amp;quot; are disjointed and not affected by the statement of the Hoare triple. In particular, let &lt;span  class=&#34;math&#34;&gt;\(A\)&lt;/span&gt; be the domain of &lt;span  class=&#34;math&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;, then let &lt;span  class=&#34;math&#34;&gt;\(\{A_1, A_2\}\)&lt;/span&gt; be some semipartition of &lt;span  class=&#34;math&#34;&gt;\(A\)&lt;/span&gt; (&lt;span  class=&#34;math&#34;&gt;\(A_1 \cap A_2 = \emptyset\)&lt;/span&gt;), then put &lt;span  class=&#34;math&#34;&gt;\(A_1 = \textbf{ref}(P)\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(A_b = \textbf{ref}(Q)\)&lt;/span&gt; (&lt;span  class=&#34;math&#34;&gt;\(\textbf{ref}(P)\)&lt;/span&gt; denotes all the locations that are referenced in &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt;, e.g. &lt;span  class=&#34;math&#34;&gt;\(\textbf{ref}([\mathcal{H}(x) = 3]) = \{x\}\)&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;We can then put &lt;span  class=&#34;math&#34;&gt;\(P * Q\)&lt;/span&gt;. This might seem useless at first (how much different from &lt;span  class=&#34;math&#34;&gt;\(\land\)&lt;/span&gt; is it?), but it is incredibly important: If &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; are dependent, not by sharing a free variable, but instead share a variable through aliasing (say &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; has &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; free and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; has &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt; free, and &lt;span  class=&#34;math&#34;&gt;\(x \sim y\)&lt;/span&gt;).&lt;/p&gt;

&lt;p&gt;All this will be formally defined in the next subsection.&lt;/p&gt;

&lt;h4 id=&#34;the-frame-rule&#34;&gt;The frame rule&lt;/h4&gt;

&lt;p&gt;The frame rule is the most important component of separation logic. It reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\textbf{mut}(C) \cap \textbf{free}(R) = \emptyset,\quad \{P\}\ C\ \{Q\}}{\{P * R\}\ C\ \{Q * R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\textbf{mut}(C)\)&lt;/span&gt; means the set of variables &lt;span  class=&#34;math&#34;&gt;\(C\)&lt;/span&gt; &amp;quot;mutates&amp;quot; (changes) when executed. For example, &lt;span  class=&#34;math&#34;&gt;\(\textbf{mut}(a \gets b) = \{a\}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;What the rule says is that if &lt;span  class=&#34;math&#34;&gt;\(C\)&lt;/span&gt; never changes the &amp;quot;environment&amp;quot; from &lt;span  class=&#34;math&#34;&gt;\(R\)&lt;/span&gt;, then you can safely join the precondition and postcondition with &lt;span  class=&#34;math&#34;&gt;\(R\)&lt;/span&gt; of some Hoare triple with &lt;span  class=&#34;math&#34;&gt;\(C\)&lt;/span&gt;.&lt;/p&gt;

&lt;h3 id=&#34;the-behavior-of-byreference-assignments&#34;&gt;The behavior of by-reference assignments&lt;/h3&gt;

&lt;p&gt;The next thing we need is a way to reason about assignments behind pointers, or &amp;quot;pointer writes&amp;quot;. We use the term &amp;quot;by-reference assignments&amp;quot; to signify the similarities between normal assignments.&lt;/p&gt;

&lt;p&gt;Starting by defining by-reference assignment, we add a rule allowing us to write to valid pointers:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P * p \mapsto \bullet\}\ p \stackrel{\text{ptr}}{\gets} x\ \{P * p \mapsto x\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Next, we need to specify the semantics of &lt;em&gt;reading&lt;/em&gt; from a pointer:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P \land p \mapsto x\}\ k \gets \mathcal{H}(p)\ \{Q\}}{\{P \land p \mapsto x\}\ k \gets x\ \{Q\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In other words, writing the data read from a pointer to a variable is equivalent to writing the value it&#39;s pointing to. This is more of a definition than an actual rule, because it is obvious, ignoring the notation.&lt;/p&gt;

&lt;h3 id=&#34;allocation&#34;&gt;Allocation&lt;/h3&gt;

&lt;p&gt;Allocation is what introduces a new heap store/pointer into the heap. And its behavior is relatively straight-forward:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{p \notin \textbf{free}(P)}{\{P\}\ p \gets \textbf{alloc}(s)\ \{P * p \to \bullet\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Namely, if &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; is not contained in &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt;, allocation creates a new, separate pointer. &lt;span  class=&#34;math&#34;&gt;\(\bullet\)&lt;/span&gt; denotes that the pointer is uninitialized or the value is unknown.&lt;/p&gt;

&lt;h3 id=&#34;deallocation&#34;&gt;Deallocation&lt;/h3&gt;

&lt;p&gt;As an example, take the dealloc function. This function obviously requires that there is no usage of the pointer later on (i.e. no use-after-free). We can express this in a relatively simple way:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P * p \mapsto x\}\ \textbf{dealloc}(p)\ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The &lt;span  class=&#34;math&#34;&gt;\(*\)&lt;/span&gt; here express the independence of the content and validity of the pointer &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt;, which is really where separation logic shines: We can express pointer relation, and in this case, make sure that there is no usage of &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; after the free.&lt;/p&gt;

&lt;h3 id=&#34;pointers-on-the-stack&#34;&gt;Pointers on the stack&lt;/h3&gt;

&lt;p&gt;In a formal model, the stack and the heap are not semantically different. In fact, we can interpret function calls as allocating the arguments onto the heap and deallocating them again when returning.&lt;/p&gt;

&lt;h3 id=&#34;detecting-memory-leaks&#34;&gt;Detecting memory leaks&lt;/h3&gt;

&lt;p&gt;In this model, it is surprisingly easy to prove your program leak-free. You simply have to put that the heap is empty in the postcondition and propagate it forward.&lt;/p&gt;

&lt;h2 id=&#34;future-work-and-whats-next&#34;&gt;Future work and what&#39;s next&lt;/h2&gt;

&lt;p&gt;Currently, I am writing a theorem extractor, which will generate the statement of correctness for some arbitrary program. This can then be fed into SMT solver and shown to be true.&lt;/p&gt;

&lt;p&gt;Another aspect is the compilation itself, which must be a verified process, as such I am working on a compiler and formal proof of correctness of said compiler.&lt;/p&gt;

&lt;p&gt;Lastly, I can formally verify Ralloc and Redox.&lt;/p&gt;

&lt;h2 id=&#34;conclusion-and-final-words&#34;&gt;Conclusion and final words&lt;/h2&gt;

&lt;p&gt;We have seen how a modest set of rules can create an elegant way to reason about the complex behavior of programs. Rust already has a very interesting form of static analysis, but it is decidable and much simpler, as a result, there is a lot of things it can not reason about, like raw pointers. We need a more advanced model (like the one we proposed in this post) to reason about such things.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skip Lists: Done Right</title>
      <link>http://ticki.github.io/blog/skip-lists-done-right/</link>
      <pubDate>Sat, 17 Sep 2016 13:46:49 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/skip-lists-done-right/</guid>
      <description>

&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css&#34;&gt;&lt;/p&gt;

&lt;h1 id=&#34;what-is-a-skip-list&#34;&gt;What is a skip list?&lt;/h1&gt;

&lt;p&gt;In short, skip lists are a linked-list-like structure which allows for fast search. It consists of a base list holding the elements, together with a tower of lists maintaining a linked hierarchy of subsequences, each skipping over fewer elements.&lt;/p&gt;

&lt;p&gt;Skip list is a wonderful data structure, one of my personal favorites, but a trend in the past ten years has made them more and more uncommon as a single-threaded in-memory structure.&lt;/p&gt;

&lt;p&gt;My take is that this is because of how hard they are to get right. The simplicity can easily fool you into being too relaxed with respect to performance, and while they are simple, it is important to pay attention to the details.&lt;/p&gt;

&lt;p&gt;In the past five years, people have become increasingly sceptical of skip lists&amp;rsquo; performance, due to their poor cache behavior when compared to e.g. B-trees, but fear not, a good implementation of skip lists can easily outperform B-trees while being implementable in only a couple of hundred lines.&lt;/p&gt;

&lt;p&gt;How? We will walk through a variety of techniques that can be used to achieve this speed-up.&lt;/p&gt;

&lt;p&gt;These are my thoughts on how a bad and a good implementation of skip list looks like.&lt;/p&gt;

&lt;h2 id=&#34;advantages&#34;&gt;Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Skip lists perform very well on rapid insertions because there are no rotations or reallocations.&lt;/li&gt;
&lt;li&gt;They&amp;rsquo;re simpler to implement than both self-balancing binary search trees and hash tables.&lt;/li&gt;
&lt;li&gt;You can retrieve the next element in constant time (compare to logarithmic time for inorder traversal for BSTs and linear time in hash tables).&lt;/li&gt;
&lt;li&gt;The algorithms can easily be modified to a more specialized structure (like segment or range &amp;ldquo;trees&amp;rdquo;, indexable skip lists, or keyed priority queues).&lt;/li&gt;
&lt;li&gt;Making it lockless is simple.&lt;/li&gt;
&lt;li&gt;It does well in persistent (slow) storage (often even better than AVL and EH).&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;a-naïve-but-common-implementation&#34;&gt;A naïve (but common) implementation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/nNjOtfa.png&#34; alt=&#34;Each shortcut has its own node.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our skip list consists of (in this case, three) lists, stacked such that the &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n&lt;/b&gt;&amp;lsquo;th list visits a subset of the node the &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n - 1&lt;/b&gt;&amp;lsquo;th list does. This subset is defined by a probability distribution, which we will get back to later.&lt;/p&gt;

&lt;p&gt;If you rotate the skip list and remove duplicate edges, you can see how it resembles a binary search tree:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/DO031ek.png&#34; alt=&#34;A binary search tree.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Say I wanted to look up the node &amp;ldquo;30&amp;rdquo;, then I&amp;rsquo;d perform normal binary search from the root and down. Due to duplicate nodes, we use the rule of going right if both children are equal:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/H5KjvqC.png&#34; alt=&#34;Searching the tree.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Self-balancing Binary Search Trees often have complex algorithms to keep the tree balanced, but skip lists are easier: They aren&amp;rsquo;t trees, they&amp;rsquo;re similar to trees in some ways, but they are not trees.&lt;/p&gt;

&lt;p&gt;Every node in the skip list is given a &amp;ldquo;height&amp;rdquo;, defined by the highest level containing the node (similarly, the number of decendants of a leaf containing the same value). As an example, in the above diagram, &amp;ldquo;42&amp;rdquo; has height 2, &amp;ldquo;25&amp;rdquo; has height 3, and &amp;ldquo;11&amp;rdquo; has height 1.&lt;/p&gt;

&lt;p&gt;When we insert, we assign the node a height, following the probability distribution:&lt;/p&gt;

&lt;p&gt;&lt;center style=&#34;font: 400 1.21em KaTeX_Math;font-style: italic;&#34;&gt; p(n) = 2&lt;sup&gt;1-n&lt;/sup&gt; &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;To obtain this distribution, we flip a coin until it hits tails, and count the flips:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uint generate_level() {
    uint n = 0;
    while coin_flip() {
        n++;
    }

    return n;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By this distribution, statistically the parent layer would contain half as many nodes, so searching is amortized &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;O(log &lt;i&gt;n&lt;/i&gt;) &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Note that we only have pointers to the right and below node, so insertion must be done while searching, that is, instead of searching and then inserting, we insert whenever we go a level down (pseudocode):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-- Recursive skip list insertion function.
define insert(elem, root, height, level):
    if right of root &amp;lt; elem:
        -- If right isn&#39;t &amp;quot;overshot&amp;quot; (i.e. we are going to long), we go right.
        return insert(elem, right of root, height, level)
    else:
        if level = 0:
            -- We&#39;re at bottom level and the right node is overshot, hence
            -- we&#39;ve reached our goal, so we insert the node inbetween root
            -- and the node next to root.
            old ← right of root
            right of root ← elem
            right of elem ← old
        else:
            if level ≤ height:
                -- Our level is below the height, hence we need to insert a
                -- link before we go on.
                old ← right of root
                right of root ← elem
                right of elem ← old

            -- Go a level down.
            return insert(elem, below root, height, level - 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above algorithm is recursive, but we can with relative ease turn it into an iterative form (or let tail-call optimization do the job for us).&lt;/p&gt;

&lt;p&gt;As an example, here&amp;rsquo;s a diagram, the curved lines marks overshoots/edges where a new node is inserted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/jr9V8Ot.png&#34; alt=&#34;An example&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;waste-waste-everywhere&#34;&gt;Waste, waste everywhere&lt;/h1&gt;

&lt;p&gt;That seems fine doesn&amp;rsquo;t it? No, not at all. It&amp;rsquo;s absolute garbage.&lt;/p&gt;

&lt;p&gt;There is a total and complete waste of space going on. Let&amp;rsquo;s assume there are &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n&lt;/b&gt; elements, then the tallest node is approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;&lt;i&gt;h = &lt;/i&gt;log&lt;sub&gt;2&lt;/sub&gt; &lt;i&gt;n&lt;/i&gt;&lt;/b&gt;, that gives us approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;1 + Σ&lt;sub&gt;&lt;i&gt;k ←0..h&lt;/i&gt;&lt;/sub&gt; &lt;i&gt;&lt;/i&gt;2&lt;sup&gt;&lt;i&gt;-k&lt;/i&gt;&lt;/sup&gt; n ≈ 2&lt;i&gt;n&lt;/i&gt;&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;2&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; is certainly no small amount, especially if you consider what each node contains, a pointer to the inner data, the node right and down, giving 5 pointers in total, so a single structure of &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; nodes consists of approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;6&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; pointers.&lt;/p&gt;

&lt;p&gt;But memory isn&amp;rsquo;t even the main concern! When you need to follow a pointer on every decrease (apprx. 50% of all the links), possibly leading to cache misses. It turns out that there is a really simple fix for solving this:&lt;/p&gt;

&lt;p&gt;Instead of linking vertically, a good implementation should consist of a singly linked list, in which each node contains  an array (representing the nodes above) with pointers to later nodes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Fd6gDLv.png&#34; alt=&#34;A better skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you represent the links (&amp;ldquo;shortcuts&amp;rdquo;) through dynamic arrays, you will still often get cache miss. Particularly, you might get a cache miss on both the node itself (which is not data local) and/or the dynamic array. As such, I recommend using a fixed-size array (beware of the two negative downsides: 1. more space usage, 2. a hard limit on the highest level, and the implication of linear upperbound when &lt;i style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;h &amp;gt; c&lt;/i&gt;. Furthermore, you should keep small enough to fit a cache line.).&lt;/p&gt;

&lt;p&gt;Searching is done by following the top shortcuts as long as you don&amp;rsquo;t overshoot your target, then you decrement the level and repeat, until you reach the lowest level and overshoot. Here&amp;rsquo;s an example of searching for &amp;ldquo;22&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/cQsPnGa.png&#34; alt=&#34;Searching for &amp;quot;22&amp;quot;.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In pseudocode:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;define search(skip_list, needle):
    -- Initialize to the first node at the highest level.
    level ← max_level
    current_node ← root of skip_list

    loop:
        -- Go right until we overshoot.
        while level&#39;th shortcut of current_node &amp;lt; needle:
            current_node ← level&#39;th shortcut of current_node

        if level = 0:
            -- We hit our target.
            return current_node
        else:
            -- Decrement the level.
            level ← level - 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;b-style-font-400-1-21em-katex-math-o-1-b-level-generation&#34;&gt;&lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;O(1)&lt;/b&gt; level generation&lt;/h1&gt;

&lt;p&gt;Even William Pugh did this mistake in &lt;a href=&#34;http://epaperpress.com/sortsearch/download/skiplist.pdf&#34;&gt;his original paper&lt;/a&gt;. The problem lies in the way the level is generated: Repeating coin flips (calling the random number generator, and checking parity), can mean a couple of RNG state updates (approximately 2 on every insertion). If your RNG is a slow one (e.g. you need high security against DOS attacks), this is noticable.&lt;/p&gt;

&lt;p&gt;The output of the RNG is uniformly distributed, so you need to apply some function which can transform this into the desired distribution. My favorite is this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;define generate_level():
    -- First we apply some mask which makes sure that we don&#39;t get a level
    -- above our desired level. Then we find the first set bit.
    ffz(random() &amp;amp; ((1 &amp;lt;&amp;lt; max_level) - 1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This of course implies that you &lt;code&gt;max_level&lt;/code&gt; is no higher than the bit width of the &lt;code&gt;random()&lt;/code&gt; output. In practice, most RNGs return 32-bit or 64-bit integers, which means this shouldn&amp;rsquo;t be a problem, unless you have more elements than there can be in your address space.&lt;/p&gt;

&lt;h1 id=&#34;improving-cache-efficiency&#34;&gt;Improving cache efficiency&lt;/h1&gt;

&lt;p&gt;A couple of techniques can be used to improve the cache efficiency:&lt;/p&gt;

&lt;h2 id=&#34;memory-pools&#34;&gt;Memory pools&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Wa8IVBJ.png&#34; alt=&#34;A skip list in a memory pool.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our nodes are simply fixed-size blocks, so we can keep them data local, with high allocation/deallocation performance, through linked memory pools (SLOBs), which is basically just a list of free objects.&lt;/p&gt;

&lt;p&gt;The order doesn&amp;rsquo;t matter. Indeed, if we swap &amp;ldquo;9&amp;rdquo; and &amp;ldquo;7&amp;rdquo;, we can suddenly see that this is simply a skip list:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/O863RR1.png&#34; alt=&#34;It&#39;s true.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can keep these together in some arbitrary number of (not necessarily consecutive) pages, drastically reducing cache misses, when the nodes are of smaller size.&lt;/p&gt;

&lt;p&gt;Since these are pointers into memory, and not indexes in an array, we need not reallocate on growth. We can simply extend the free list.&lt;/p&gt;

&lt;h2 id=&#34;flat-arrays&#34;&gt;Flat arrays&lt;/h2&gt;

&lt;p&gt;If we are interested in compactness and have a insertion/removal ratio near to 1, a variant of linked memory pools can be used: We can store the skip list in a flat array, such that we have indexes into said array instead of pointers.&lt;/p&gt;

&lt;h2 id=&#34;unrolled-lists&#34;&gt;Unrolled lists&lt;/h2&gt;

&lt;p&gt;Unrolled lists means that instead of linking each element, you link some number of fixed-size chuncks contains two or more elements (often the chunk is around 64 bytes, i.e. the normal cache line size).&lt;/p&gt;

&lt;p&gt;Unrolling is essential for a good cache performance. Depending on the size of the objects you store, unrolling can reduce cache misses when following links while searching by 50-80%.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of an unrolled skip list:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/FYpPQPh.png&#34; alt=&#34;A simple 4 layer unrolled skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The gray box marks excessive space in the chunk, i.e. where new elements can be placed. Searching is done over the skip list, and when a candidate is found, the chunk is searched through &lt;strong&gt;linear&lt;/strong&gt; search. To insert, you push to the chunk (i.e. replace the first free space). If no excessive space is available, the insertion happens in the skip list itself.&lt;/p&gt;

&lt;p&gt;Note that these algorithms requires information about how we found the chunk. Hence we store a &amp;ldquo;back look&amp;rdquo;, an array of the last node visited, for each level. We can then backtrack if we couldn&amp;rsquo;t fit the element into the chunk.&lt;/p&gt;

&lt;p&gt;We effectively reduce cache misses by some factor depending on the size of the object you store. This is due to fewer links need to be followed before the goal is reached.&lt;/p&gt;

&lt;h1 id=&#34;self-balancing-skip-lists&#34;&gt;Self-balancing skip lists&lt;/h1&gt;

&lt;p&gt;Various techniques can be used to improve the height generation, to give a better distribution. In other words, we make the level generator aware of our nodes, instead of purely random, independent RNGs.&lt;/p&gt;

&lt;h2 id=&#34;self-correcting-skip-list&#34;&gt;Self-correcting skip list&lt;/h2&gt;

&lt;p&gt;The simplest way to achieve a content-aware level generator is to keep track of the number of node of each level in the skip list. If we assume there are &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; nodes, the expected number of nodes with level &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;l&lt;/i&gt;&lt;/b&gt; is &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;2&lt;sup&gt;&lt;i&gt;-l&lt;/i&gt;&lt;/sup&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt;. Subtracting this from actual number gives us a measure of how well-balanced each height is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/bBf7kcg.png&#34; alt=&#34;Balance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When we generate a new node&amp;rsquo;s level, you choose one of the heights with the biggest under-representation (see the black line in the diagram), either randomly or by some fixed rule (e.g. the highest or the lowest).&lt;/p&gt;

&lt;h2 id=&#34;perfectly-balanced-skip-lists&#34;&gt;Perfectly balanced skip lists&lt;/h2&gt;

&lt;p&gt;Perfect balancing often ends up hurting performance, due to backwards level changes, but it is possible. The basic idea is to reduce the most over-represented level when removing elements.&lt;/p&gt;

&lt;h1 id=&#34;an-extra-remark&#34;&gt;An extra remark&lt;/h1&gt;

&lt;p&gt;Skip lists are wonderful as an alternative to Distributed Hash Tables. Performance is mostly about the same, but skip lists are more DoS resistant if you make sure that all links are F2F.&lt;/p&gt;

&lt;p&gt;Each node represents a node in the network. Instead of having a head node and a nil node, we connect the ends, so any machine can search starting at it self:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/moD7oy9.png&#34; alt=&#34;A network organized as a skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you want a secure open system, the trick is that any node can invite a node, giving it a level equal to or lower than the level itself. If the node control the key space in the interval of A to B, we partition it into two and transfer all KV pairs in the second part to the new node. Obviously, this approach has no privilege escalation, so you can&amp;rsquo;t initialize a sybil attack easily.&lt;/p&gt;

&lt;h1 id=&#34;conclusion-and-final-words&#34;&gt;Conclusion and final words&lt;/h1&gt;

&lt;p&gt;By apply a lot of small, subtle tricks, we can drastically improve performance of skip lists, providing a simpler and faster alternative to Binary Search Trees. Many of these are really just minor tweaks, but give an absolutely enormous speed-up.&lt;/p&gt;

&lt;p&gt;The diagrams were made with &lt;a href=&#34;https://en.wikipedia.org/wiki/Dia_(software)&#34;&gt;Dia&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/PGF/TikZ&#34;&gt;TikZ&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Rust&#39;s `std::collections` is absolutely fantastic</title>
      <link>http://ticki.github.io/blog/fantastic/</link>
      <pubDate>Wed, 14 Sep 2016 16:07:34 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/fantastic/</guid>
      <description>

&lt;p&gt;My &lt;a href=&#34;http://ticki.github.io/blog/horrible/&#34;&gt;last blog post&lt;/a&gt; was about all the short-fallings and problems &lt;code&gt;std::collections&lt;/code&gt; has. This post will be about the opposite: all the good things about &lt;code&gt;std::collections&lt;/code&gt; and what other languages can learn from Rust.&lt;/p&gt;

&lt;p&gt;This post is a part of an on-going series of posts criticizing and praising various parts of Rust.&lt;/p&gt;

&lt;h1 id=&#34;the-philosophy-of-std-collections&#34;&gt;The philosophy of &lt;code&gt;std::collections&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;Rust has an intentionally small set of collections. This has both advantages and disadvantages.&lt;/p&gt;

&lt;p&gt;For one, the surface area becomes smaller, which allows libstd to focus on the support and performance of a few really good collections.&lt;/p&gt;

&lt;p&gt;Another advantage is the fact that the user can easier choose which collection. If you take Java&amp;rsquo;s standard library, which is arguably bloated, the programmer is often confused about which data structure to use. Rust&amp;rsquo;s small sets of primitives is easier to work with.&lt;/p&gt;

&lt;p&gt;If a specialized data structure really is needed, &lt;a href=&#34;https://crates.io&#34;&gt;crates.io&lt;/a&gt; contains many cool implementations of various data structures, including some &lt;em&gt;really&lt;/em&gt; specialized ones. That just proves that the Rust ecosystem is healthy.&lt;/p&gt;

&lt;h1 id=&#34;when-is-a-primitive-appropriate-for-the-standard-library&#34;&gt;When is a primitive appropriate for the standard library?&lt;/h1&gt;

&lt;p&gt;There are generally two factors to consider:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Standardization: The standard library serves for consituting some uniformity between libraries. This includes (but is not limited to) API interoperability, familiar API.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Convenience: Many primitives in the standard library are there simply for the sake of convenience.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Rust&amp;rsquo;s standard library is the smallest set of primitives which are sufficient per the factors above. The collection module is no different: The smallest viable set of collections for standardization and convenience is provided.&lt;/p&gt;

&lt;p&gt;I think the &lt;code&gt;std::collections&lt;/code&gt; accomplishes that pretty well. It contains a few, frequently used structures, such as hash tables, B-trees, and vectors.&lt;/p&gt;

&lt;h1 id=&#34;vec-t&#34;&gt;&lt;code&gt;Vec&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/UDeQOFK.png&#34; alt=&#34;Vector growth&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most programmers are familiar with vectors, the growable contiguous array with random-access property. Indeed, it is a very simple and well-defined structure. It is not a particularly elegant structure, but it is practical, simple, and fast.&lt;/p&gt;

&lt;p&gt;The only thing that is really implementation defined is the reallocation strategy. Vectors tries to avoid excessive allocations by allocating some extra capacity, making pop/push amortized O(1). I think the choice of strategy is a sane one. It&amp;rsquo;s relatively simple: The initial allocation is some constant number of elements, when the capacity is reached, multiply it by two.&lt;/p&gt;

&lt;p&gt;The API is very similar to C++&amp;rsquo;s &lt;code&gt;std::vector&lt;/code&gt;. Rust has no overloaded syntax with respect to initializing it, however it provides macro &lt;code&gt;vec!&lt;/code&gt;, which allows for doing exactly that.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Vec::new()&lt;/code&gt; does not do an initial allocation, this is based off the observation that many vectors are empty throughout their life. This mean that we can (without calling &lt;code&gt;malloc&lt;/code&gt;) initialize an empty vector with capacity zero. This will simply set the start point to 0x1 (the reason it isn&amp;rsquo;t null is because Rust has a language-supported optimization, which makes type marked as non-null nullable in certain tagged unions).&lt;/p&gt;

&lt;h2 id=&#34;side-note-rust-s-allocation-api&#34;&gt;Side note: Rust&amp;rsquo;s allocation API&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/cH5a8gu.png&#34; alt=&#34;A simple diagram comparing the two&#34; /&gt;&lt;/p&gt;

&lt;p&gt;One of my favorite aspects of Rust is that it provides a malloc API which has actually been thought about, in contrast to most other APIs (I&amp;rsquo;m looking at you libc &lt;code&gt;malloc&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;There are two main differences from &lt;code&gt;malloc&lt;/code&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The programmer provides the size. Instead of doing &lt;code&gt;free(pointer)&lt;/code&gt;, you do &lt;code&gt;free(pointer, size)&lt;/code&gt;. That might seem like a minor change (and maybe even annoying), but the motivation is pretty clear: Due to the limitations of the libc &lt;code&gt;malloc&lt;/code&gt; API, most allocators add a the size to the block metadata of the allocation. This ranges from 2-8 bytes¹, but if you think about it, that&amp;rsquo;s quite a lot for small allocations¹. The information is redundant, because the user almost always know the size of the buffer. For example, &lt;code&gt;Vec&lt;/code&gt; would only have to provide its capacity, etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;You can force inplace allocations (buffer extensions). In the libc API, there is no way to reallocate inplace, if possible. You have to go through &lt;code&gt;realloc&lt;/code&gt;, which might call &lt;code&gt;memcpy&lt;/code&gt;. Considering that inplace reallocation is a lot faster, there are scenarios where you&amp;rsquo;d like a reallocation, but it is not important enough for you to pay the &lt;code&gt;memcpy&lt;/code&gt;. Take &lt;code&gt;Vec&lt;/code&gt; for example, if it is &amp;ldquo;almost at capacity&amp;rdquo;, you&amp;rsquo;d like an extension, because waiting until it is filled up might mean that said memory is already taken. On the other hand, doing an early reallocation is bad style, since it provides no real benefit over doing it when the capacity is full.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is more complex than libc&amp;rsquo;s &lt;code&gt;malloc&lt;/code&gt; API, but Rust programmers rarely use these symbols directly. Rather, it is abstracted away under safe primitives. The implementations of such primitives might exploit it for performance.&lt;/p&gt;

&lt;p&gt;Now, this is only an API, and Rust currently uses Jemalloc, so not all of these features are supported. I&amp;rsquo;ll take the liberty to shamelessly plug &lt;a href=&#34;https://github.com/redox-os/ralloc&#34;&gt;ralloc&lt;/a&gt;, a pure Rust memory allocator making full use of the Rust allocation API.&lt;/p&gt;

&lt;p&gt;¹ Note that various optimizations (such as seperated segments and specialized bookkeeping for small blocks) can be done (especially for small allocations), but you ultimatively cannot remove the cost without changing the API.&lt;/p&gt;

&lt;h1 id=&#34;hashmap&#34;&gt;&lt;code&gt;HashMap&lt;/code&gt;&lt;/h1&gt;

&lt;h2 id=&#34;collision-resolution&#34;&gt;Collision resolution&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Hash_table_5_0_1_1_1_1_0_SP.svg/380px-Hash_table_5_0_1_1_1_1_0_SP.svg.png&#34; alt=&#34;Open addressing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;http://ticki.github.io/blog/horrible/&#34;&gt;the last post&lt;/a&gt;, I strongly criticized the hash table implementation, but after extensive discussion on Reddit and IRC, I changed my mind. The only thing I really disagree with for &lt;code&gt;HashMap&lt;/code&gt; is the choice of Sip-hasher.&lt;/p&gt;

&lt;p&gt;My analysis of the implementations had one big flaw: not differentiating between linear Robin Hood hashing and double Robin Hood hashing. Rust is using the former. My claim that it had a lot of cache misses was completely wrong: It actually have excellent cache behavior.&lt;/p&gt;

&lt;p&gt;Due to the terrible naming, it can be very confusing (even &lt;a href=&#34;https://en.wikipedia.org/wiki/Hash_table#Robin_Hood_hashing&#34;&gt;it&amp;rsquo;s Wikipedia page&lt;/a&gt; is wrong), but there are two variants: The classical, double-hashing variant (which was described in Pedro&amp;rsquo;s original paper), and the modern linear probing variant. I wrongly said that Rust is using the former, but there are good news: It doesn&amp;rsquo;t!&lt;/p&gt;

&lt;p&gt;What Robin Hood does is minimizing the probe lengths by giving up the slot if the probe length is longer. In other words, the worst-case is improved and it is more equally distributed making clustering less damaging.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider a table like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    [0|neat|feet]
    [1|good|hood]
    [0|load|boat]
    [ |    |    ]
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then let&amp;rsquo;s say we want to insert &lt;code&gt;fool&lt;/code&gt; at &lt;code&gt;cool&lt;/code&gt;. Let&amp;rsquo;s say that &lt;code&gt;hash(&amp;quot;cool&amp;quot;) == 1&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    [0|neat|feet]
   *[1|good|hood]
    [0|load|boat]
    [ |    |    ]
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see the entry marked with &lt;code&gt;*&lt;/code&gt; is already taken, so we need to probe forward, but that&amp;rsquo;s taken too. Well, right now we have probed one time, so our probe length is one, compared to the probe length of the &lt;code&gt;load&lt;/code&gt; slot, which is zero. To minimize our probe length, we replace &lt;code&gt;load&lt;/code&gt; with &lt;code&gt;cool&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    [0|neat|feet]
    [1|good|hood]
    [1|cool|fool]
    [ |    |    ]
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We still need to find a place for our old entry:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    [0|load|boat]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Luckily, the next slot is empty, so we get:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    [0|neat|feet]
    [1|good|hood]
    [1|cool|fool]
    [1|load|boat]
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we had simply done linear probing, we&amp;rsquo;d have:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    [0|neat|feet]
    [1|good|hood]
    [0|load|boat]
    [2|cool|fool]
    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then looking up &lt;code&gt;cool&lt;/code&gt; would need two jumps, i.e. a bad worst case.&lt;/p&gt;

&lt;p&gt;In other words, you save the slowness of rehashing/cache missing, and yet preserve the cool features of Robin Hood hashing.&lt;/p&gt;

&lt;p&gt;I think it&amp;rsquo;s a good choice.&lt;/p&gt;

&lt;h2 id=&#34;siphasher&#34;&gt;&lt;code&gt;SipHasher&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m still very strongly opposed to having &lt;code&gt;SipHasher&lt;/code&gt; as default hash function, but it is not entirely without advantages. Security is one.&lt;/p&gt;

&lt;p&gt;What I really want to talk about is a sane compromise between &lt;code&gt;SipHasher&lt;/code&gt; and a fast hashing functions which ensures the same security/DoS-resistance properties of &lt;code&gt;SipHasher&lt;/code&gt;, while having better performance in most cases. This is not yet implemented, but it has been floating around in the community for a while now, and it is definitely a plausible alternative, which we will likely see in Rust in the future.&lt;/p&gt;

&lt;p&gt;The proposal is called &lt;em&gt;adaptive hashing&lt;/em&gt;, and it is in fact a relatively simple strategy: Use a fast hash function initially, and when a probe length in the table surpasses some threshold, switch to &lt;code&gt;SipHasher&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A long probe sequence is relatively rare to occur naturally, so it is a good indicator for exploitation.&lt;/p&gt;

&lt;p&gt;A more complicated version of the proposal is to make it slot-specific, in such a way that the switch only happens when a specific probe length exceeds some threshold.&lt;/p&gt;

&lt;p&gt;An initial implementation (outside the standard library) along with benchmarks can be found &lt;a href=&#34;https://github.com/contain-rs/hashmap2/pull/5&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;b-trees&#34;&gt;B-trees&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tUmeDcr.png&#34; alt=&#34;An example B-tree.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;B-trees are one of those structures that people always implement wrongly in one way or another. Fortunately, Rust&amp;rsquo;s standard library gets this one right.&lt;/p&gt;

&lt;p&gt;The code is clear and the performance is good. The only catch is the cache efficiency, which there are still room for improvement of.&lt;/p&gt;

&lt;h1 id=&#34;binary-heap&#34;&gt;Binary heap&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Heap_delete_step0.svg/500px-Heap_delete_step0.svg.png&#34; alt=&#34;A binary tree satisfying the heap property.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;std::collections::BinaryHeap&lt;/code&gt; is a fairly standard implementations of a flat-array binary heap. Only one thing here really stand out: the &lt;code&gt;Hole&lt;/code&gt; struct.&lt;/p&gt;

&lt;p&gt;The definition looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;struct Hole&amp;lt;&#39;a, T: &#39;a&amp;gt; {
    data: &amp;amp;&#39;a mut [T],
    /// `elt` is always `Some` from new until drop.
    elt: Option&amp;lt;T&amp;gt;,
    pos: usize,
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It represents some segment from the heap array which is invalid (i.e. shouldn&amp;rsquo;t be read). This is enforced through the static checking of rustc, which verifies that mutable references are mutually exclusive.&lt;/p&gt;

&lt;p&gt;I like this approach over the more standard ones, due to eliminating many bugs you easily encounter while writing a flat binary heap.&lt;/p&gt;

&lt;h1 id=&#34;code-quality&#34;&gt;Code quality&lt;/h1&gt;

&lt;p&gt;The code quality of &lt;code&gt;std::collections&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; is general, is very good. It contains a lot of comments explaining optimizations implementation strategies and other things important for understanding the code:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;Files&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Total&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Blanks&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Comments&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Code&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18868&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8567&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8771&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here&amp;rsquo;s an example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// &amp;gt; Why a load factor of approximately 90%?
//
// In general, all the distances to initial buckets will converge on the mean.
// At a load factor of α, the odds of finding the target bucket after k
// probes is approximately 1-α^k. If we set this equal to 50% (since we converge
// on the mean) and set k=8 (64-byte cache line / 8-byte hash), α=0.92. I round
// this down to make the math easier on the CPU and avoid its FPU.
// Since on average we start the probing in the middle of a cache line, this
// strategy pulls in two cache lines of hashes on every lookup. I think that&#39;s
// pretty good, but if you want to trade off some space, it could go down to one
// cache line on average with an α of 0.84.
//
// &amp;gt; Wait, what? Where did you get 1-α^k from?
//
// On the first probe, your odds of a collision with an existing element is α.
// The odds of doing this twice in a row is approximately α^2. For three times,
// α^3, etc. Therefore, the odds of colliding k times is α^k. The odds of NOT
// colliding after k tries is 1-α^k.
//
// [...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Such comments can be found in many places and provides great insight into the small tricks used for speeding things up.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;We&amp;rsquo;ve seen a few insights and techniques used, as well as the reasoning behind it and ways to improve it. It&amp;rsquo;s not horrible (or even bad) as a whole. It has its rough edges (some of which were pointed out in the last post), but it&amp;rsquo;s in constant improvement and has many great primitives.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d like to thank all the people who&amp;rsquo;ve implemented these collections, as well as the on going out-of-tree implementations of collections.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Critique of Rust&#39;s `std::collections`</title>
      <link>http://ticki.github.io/blog/a-critique-of-rusts-stdcollections/</link>
      <pubDate>Mon, 12 Sep 2016 22:50:08 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/a-critique-of-rusts-stdcollections/</guid>
      <description>

&lt;p&gt;Rust is by far my favorite language, and I am very familiar with it, but there is one aspect that annoys me at times: &lt;code&gt;std::collections&lt;/code&gt;, a part of the opt-out standard library.&lt;/p&gt;

&lt;p&gt;This post will go through the short-fallings of the API and implementation of &lt;code&gt;std::collections&lt;/code&gt;. I&amp;rsquo;ll try to present alternatives and way to improve it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The title was previously &amp;ldquo;Why &lt;code&gt;std::collections&lt;/code&gt; is absolutely horrible&amp;rdquo;. It was in the hope to spark critical discussion, however people were rather annoyed by this title (and I understand why), so I changed it to something less provocative.&lt;/p&gt;

&lt;h1 id=&#34;what-it-contains&#34;&gt;What it contains&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;std::collections&lt;/code&gt; has a rather small set of collections (which is a legitimate choice to make to preserve minimality), the catch being that it&amp;rsquo;s an odd choice of collections:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;B-tree based map and set.&lt;/li&gt;
&lt;li&gt;Binary heap.&lt;/li&gt;
&lt;li&gt;Hash table and set.&lt;/li&gt;
&lt;li&gt;Doubly-linked list.&lt;/li&gt;
&lt;li&gt;Ring buffer.&lt;/li&gt;
&lt;li&gt;Random-acess vectors (strictly speaking not in &lt;code&gt;std::collections&lt;/code&gt; but instead in &lt;code&gt;std::vec&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;That seems fine, doesn&amp;rsquo;t it? No, it doesn&amp;rsquo;t. If you consider what it lacks of these are very weird choices of structures.&lt;/p&gt;

&lt;p&gt;Take binary heap. It is incredibly useful at times, but is it really fit for a standard library with focus on being minimal? Let&amp;rsquo;s look at the statistics:&lt;/p&gt;

&lt;p&gt;444 examples of usage of this structure (in Rust) on GitHub. Now, we obviously cannot be sure that this sample is representative, but it should give a pretty good insight on the usage.&lt;/p&gt;

&lt;p&gt;Looking through these, approximately 50 of these are tests of &lt;code&gt;BinaryHeap&lt;/code&gt; itself. Another 50 are reimplementations of it. Around 100 of them are duplicates of other codes (e.g. downloaded libraries). This leaves us with around 250 usages, and that&amp;rsquo;s only slightly more than the incredibly useful &lt;code&gt;VecMap&lt;/code&gt;, which isn&amp;rsquo;t even in the standard library!&lt;/p&gt;

&lt;p&gt;If minimalism really is a goal (which I am going to criticize in a minute), it seems rather weird to have a collection which is barely used more than a non-libstd collection.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s move on to doubly-linked list. Searching on GitHub gives you 534 results. I was unable to find &lt;em&gt;a single place where it was used in a manner that could not be replaced by singly-linked lists&lt;/em&gt;. Chances are that there are some, but they&amp;rsquo;re incredibly rare, and it is odd given that there are no singly-linked list structures in the standard libraries.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: To clarify here. I&amp;rsquo;m not arguing that the primitives I propose later deserves a place above these, rather that for a minimal set of collections, the choice is rather odd, given that some collections are even more common than some of these.&lt;/p&gt;

&lt;h1 id=&#34;what-it-doesn-t-contain&#34;&gt;What it doesn&amp;rsquo;t contain&lt;/h1&gt;

&lt;h2 id=&#34;concurrent-data-structures&#34;&gt;Concurrent data structures&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/GmoylkC.png&#34; alt=&#34;How it compares&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The whole standard library contains exactly two concurrent data structures (note that data structures are different from containers), namely the MPSC-queues (the blocking queue with a limited buffer and the non-blocking with an unlimited buffer). These are used for cross-thread message passing and the alike.&lt;/p&gt;

&lt;p&gt;But where are all the other concurrent primitives?&lt;/p&gt;

&lt;p&gt;People tend to wrap their structure in &lt;code&gt;Mutex&lt;/code&gt;, like &lt;code&gt;Mutex&amp;lt;HashMap&amp;lt;...&amp;gt;&amp;gt;&lt;/code&gt;, but that is often an order of magnitude slower than a concurrent hash table.&lt;/p&gt;

&lt;p&gt;Then there&amp;rsquo;s the multithreaded push/pop stacks (as opposed to the queue/unqueue lists), and so on.&lt;/p&gt;

&lt;p&gt;There are quite a few implementations of structures as the ones described above, but they are more often than not poorly implemented. The leading library (which has a pretty good implementation quality) is &lt;a href=&#34;https://github.com/aturon/crossbeam&#34;&gt;crossbeam&lt;/a&gt;, but unfortunately it only implements a very limited set of synchronization primitives (no maps, no tables, no skip lists, etc.).&lt;/p&gt;

&lt;h2 id=&#34;singly-linked-lists&#34;&gt;Singly linked lists&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve already mentioned this, but singly linked list are often useful.&lt;/p&gt;

&lt;h2 id=&#34;keyed-priority-queues&#34;&gt;Keyed priority queues&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://www.csit.parkland.edu/~mvanmoer/CSC220/2014/images/heap-9.2.f.jpg&#34; alt=&#34;An example of a keyed heap&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Keyed priority queues is the structure everyone ask about and looks for, but no one can name it (here&amp;rsquo;s an exercise: go on Google and try to vaguely describe this structure, you will for sure find at least one thread asking for exactly that description, and often no one is able to answer the question or misguidedly proposes binary heaps instead).&lt;/p&gt;

&lt;p&gt;Say you have an ordered list of elements, each of which has a priority. Now, you want to be able to retrieve the element with the highest or the lowest priority, with a reasonable performance. Note that mere heaps are not sufficient, since they are not arbitrarily ordered, in the sense that you cannot index them without traversing all elements.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: People think I&amp;rsquo;m talking about binary heaps, but they&amp;rsquo;re fundamentally very different. A keyed priority queue is some arbitrarily ordered list (or map) such that elements with high or low priority can be retrieved quickly. Binary heaps are not associative arrays or lists, they do not allow ordering of the elements, and are thus conceptually simpler than keyed priority queues. Note that keyed priority queues are almost always implemented with binary heaps as the backbone. See [this paper]() for an in-depth description of a general-purpose keyed priority queue&lt;/p&gt;

&lt;p&gt;Keyed priority queues are used everywhere from cache level regulation to efficient scheduling, and are in my opinion one of the most useful data structures of all.&lt;/p&gt;

&lt;p&gt;It is hard to find out exactly how much it is used in the Rust community, given its many names and reimplementations. Only 83 occurrences of the name &amp;ldquo;PriorityQueue&amp;rdquo; in Rust code can be found on GitHub, but I suspect the real number to be much higher.&lt;/p&gt;

&lt;p&gt;The most famous form of keyed priority queues are Fibonacci heaps, which are used in most major database systems, as well as the Linux scheduler, and many memory allocators.&lt;/p&gt;

&lt;h2 id=&#34;treaps&#34;&gt;Treaps&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://bluehawk.monmouth.edu/rclayton/web-pages/s10-305-503/treapsf4.png&#34; alt=&#34;Insertion in treaps&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Treaps are generally faster than other self-balanced trees (on the average), but the really killer feature is the bulk operations. These are highly efficient algorithms for union, intersections, and set differences.&lt;/p&gt;

&lt;p&gt;When the programmer is manipulating sets like this (union, intersections, and so on) and iterators aren&amp;rsquo;t sufficient (i.e., it is for storage, not iteration), these can be incredibly useful as a high-performance data structure.&lt;/p&gt;

&lt;h2 id=&#34;skip-lists&#34;&gt;Skip lists&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://igoro.com/wordpress/wp-content/uploads/2008/07/skiplist.png&#34; alt=&#34;An illustration of skip lists&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Skip lists are more niche than the structures described above, but they have excellent performance characteristics.&lt;/p&gt;

&lt;p&gt;Skip lists are conceptually similar to N-ary trees, but in the representation of a list. They&amp;rsquo;re a probabilistic data structure, which holds a list and some number of sublists such that the &lt;em&gt;n&lt;/em&gt;&amp;lsquo;th sublist is a sublist of the &lt;em&gt;n - 1&lt;/em&gt;&amp;lsquo;th sublist. Search can be visualized as binary search by observing how two paths can be taken: A) go to the next sublist B) follow the link.&lt;/p&gt;

&lt;p&gt;The reason a good implementation outperforms a good implementation of classical binary search trees has to do with two reasons:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;On average, 50% of the links followed under a search are cache local, whereas B-trees, for example, are around 20% (&lt;strong&gt;update&lt;/strong&gt;: The original number said 0%, turns out my tests were wrong) cache local.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;No tree rotations or equivalent operations are needed during insertion.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Skip lists has its use cases, and a &lt;em&gt;good implementation&lt;/em&gt; (flat array, SLOBs, and unrolled lists) can easily outperform B-trees. For really big sets, however, skip lists tends to be slower due to not being as rigidly balanced.&lt;/p&gt;

&lt;h2 id=&#34;self-balancing-trees&#34;&gt;Self-balancing trees&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/x4QQlZp.png&#34; alt=&#34;An example of a left-leaning red-black tree&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As mentioned, Rust&amp;rsquo;s standard library already has an excellent implementation of B-trees, a popular form of self-balancing trees.&lt;/p&gt;

&lt;p&gt;The other popular self-balancing trees are good candidates as well (AVL and LLRB/Red-black). While they do essentially the same, they can have very different performance characteristics, and switching can influence the program&amp;rsquo;s performance vastly.&lt;/p&gt;

&lt;p&gt;Having a diverse set of such structures can be good, especially if the documentation details which one to use based on your use case.&lt;/p&gt;

&lt;h2 id=&#34;slobs-aka-pointer-lists-memory-pools-typed-arenas-etc&#34;&gt;SLOBs (aka. pointer lists, memory pools, typed arenas, etc.)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/MoaaFPc.png&#34; alt=&#34;An example SLOB&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is a very simple, and yet very powerful, data structure. In fact, they are nothing but a glorified singly linked list of pointers to some type. The dealbreaker is the fact that it requires no storage aside from the data it holds.&lt;/p&gt;

&lt;p&gt;That is, no allocation is needed to push and pop pointers from this list. This is possible by letting the data which is inactive hold the list itself.&lt;/p&gt;

&lt;p&gt;So what&amp;rsquo;s the big deal here? It turns out to be extremely useful for region-based memory management. If you have a lot of allocations of the same type, it is often multiple orders of magnitude faster than allocating each of them seperately, and what&amp;rsquo;s even cooler is the data locality it provides: Since it is based on one big contagious segment broken down into pieces, it will only cover a few pages, and consequently it is cache efficient (this fact will be abused in a minute).&lt;/p&gt;

&lt;h1 id=&#34;we-can-just-leave-it-to-other-libraries&#34;&gt;&amp;ldquo;We can just leave it to other libraries&amp;rdquo;&lt;/h1&gt;

&lt;p&gt;A common talking point is that we can simply outsource it to external libraries. Unfortunately, they cannot provide an essential property of the standard library: standardization. Standard libraries serves for making sure t
he ecosystem is uniform. If something is crucial for keeping the ecosystem together, it deserves a place in the standard library. These are severely underused due to the stigma around adding new dependencies.&lt;/p&gt;

&lt;p&gt;Standardization is absolutely crucial for adoptation.&lt;/p&gt;

&lt;h1 id=&#34;criticizing-the-structures-it-do-have&#34;&gt;Criticizing the structures it do have&lt;/h1&gt;

&lt;h2 id=&#34;hashmap&#34;&gt;&lt;code&gt;HashMap&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Hash_table_5_0_1_1_1_1_0_SP.svg/380px-Hash_table_5_0_1_1_1_1_0_SP.svg.png&#34; alt=&#34;Open addressing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Rust&amp;rsquo;s hash table implementation is perhaps the criticized part, not because it is a bad implementation, but because it is a very performance-critical component, and yet has its rough edges (including a, for some, odd choice of hash functions).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The code is very well-written. The best way to study it is reading it. I recommend doing that.&lt;/p&gt;

&lt;p&gt;A quick overview of the Rust &lt;code&gt;HashMap&lt;/code&gt;/&lt;code&gt;HashSet&lt;/code&gt; implementation is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Open addressing (Robin Hood hashing)&lt;/li&gt;
&lt;li&gt;90.9% load factor before reallocation&lt;/li&gt;
&lt;li&gt;Defaults to Sip-hasher (cryptographic hash function)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s just go through these one-by-one and see what&amp;rsquo;s wrong:&lt;/p&gt;

&lt;h3 id=&#34;robin-hood-hashing&#34;&gt;Robin Hood hashing&lt;/h3&gt;

&lt;p&gt;Robin Hood hashing is a double-hashing variant quite, in which you rehash until the slot is free. Robin Hood hashing improves plain double-hashing by making sure the slots occupants are ordered by the probe length.&lt;/p&gt;

&lt;p&gt;So what&amp;rsquo;s the problem here? Well, the cache efficiency is not exactly ideal, but we get freedom from clustering in return.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;opposite&amp;rdquo; approach is linear probing (where you add some constant - often 1 - to the slot number until it is free), which has the opposite nature: Cache efficiency is really good, but it is very sensitive to clustering.&lt;/p&gt;

&lt;p&gt;A reasonable alternative which takes the best of each of these solutions is quadratic probing, which simply uses a quadratic polynomial to jump between slots (or, in analogy to the one given above, the constant in question increases linearly).&lt;/p&gt;

&lt;p&gt;In most scenarios (especially for large tables), quadratic probing has fewer cache misses, due to better data locality.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no reason to have strong opinions on this subject. The difference is rather small, but interesting nonetheless.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: As people pointed out, this section contains the error of confusing Rust&amp;rsquo;s implementation of Robin Hood hashing with the original paper. In fact, it is very different: Rust&amp;rsquo;s implementation doesn&amp;rsquo;t rehash, but instead linearly probes, that simply makes the difference even smaller, and it makes it a pretty good choice. I don&amp;rsquo;t know enough to be able to judge if it is better or worse than quadratic probing.&lt;/p&gt;

&lt;h3 id=&#34;a-high-reallocation-threshold&#34;&gt;A high reallocation threshold&lt;/h3&gt;

&lt;p&gt;This mostly comes down to a trade-off between between memory and CPU. If you think about it, 1:9 empty slots is a pretty dense table with an average probe length of 6 rehashes. Potentially (for very large tables) that can lead to 3-6 (avg. 3 or 4, depending on which test) cache misses for just a single lookup.&lt;/p&gt;

&lt;p&gt;The advantage is that it is relatively memory efficient, but that should really only be a concern for really big tables. For most tables, this is way too high, and it trades CPU cycles for memory (which is almost unimportant these days).&lt;/p&gt;

&lt;p&gt;I personally think that having a constant factor is a bad idea. I think it should be some function of the number of elements in the table, such that the factor is lower for small tables (where memory isn&amp;rsquo;t a concern). This way you get memory efficiency when it matters.&lt;/p&gt;

&lt;h3 id=&#34;sip-hasher&#34;&gt;Sip-hasher&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/smjww32.png&#34; alt=&#34;A round of Sip&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Sip-hasher is a cryptographic hash function, and as with most cryptographic hash functions, it is slower than the non-cryptographic counterpart.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Some people have pointed out that it is in fact not a cryptographic hash functions, but rather an efficient and yet sufficiently secure hash keyed function. In contrary to DJB2 or xxHash, it is not easy to invert/generate a preimage, which has the consequence of being hard to generate collisions on purpose, a property that can be important for publicly exposed structures (such as databases, KV stores for servers, etc.).&lt;/p&gt;

&lt;p&gt;And it doesn&amp;rsquo;t even have a measurable better quality. I tried giving three different hash functions various data sets. Collision-wise they did equally on every data set, with exception of the English dictionary (as seen below). In every single test, my home-made hash function &amp;ldquo;long hasher&amp;rdquo; beats sip-hasher on performance &lt;em&gt;by a significant factor&lt;/em&gt; (around 30%)&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;~ SipHasher
    Filled buckets: 2048
    Max bucket: 245
    Time: 0 s. 39232 ms.
    GB/s: 0.8565382742517557
~ DJB2
    Filled buckets: 2048
    Max bucket: 238
    Time: 0 s. 39463 ms.
    GB/s: 0.784737479297558
~ LongHasher
    Filled buckets: 2048
    Max bucket: 239
    Time: 0 s. 29562 ms.
    GB/s: 4.95375004585191
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Some investigation shows that the vast majority (98%) of the time of retrieval is used on hashing (it&amp;rsquo;s not clear if the same is true for insertions, but it still have a major influence)). Say you used hash maps for caching database queries. That could potentially translate to 30% faster retrieval on cached queries.&lt;/p&gt;

&lt;p&gt;My point isn&amp;rsquo;t that LongHasher is fantastic, but my point is that there are hash functions which vastly beats sip hasher performance-wise.&lt;/p&gt;

&lt;p&gt;On a side note: These numbers are quite impressive, and if you don&amp;rsquo;t believe me &lt;a href=&#34;https://gist.github.com/anonymous/3b0b489137af9006d5c498f10d42514a&#34;&gt;you can run it yourself&lt;/a&gt;. The reason that long hasher is able to outperform them both is that it consumes eight bytes at once. Otherwise, it is really just multiplying by a prime, adding some constant, multiplying by another byte, rotating right and then XORing by some key.&lt;/p&gt;

&lt;p&gt;Now, if it isn&amp;rsquo;t quality, then what&amp;rsquo;s the reason for using a cryptographic hash function? One reason often cited is Denial-of-Service resistance, and that&amp;rsquo;s a valid concern, but is it really something that everyone should pay for?&lt;/p&gt;

&lt;p&gt;You may know the famous &amp;ldquo;What you don&amp;rsquo;t use, you don&amp;rsquo;t pay for&amp;rdquo; idiom. This is a core part of the &amp;ldquo;abstraction without overhead&amp;rdquo; principle. It is relatively rare to actually need DoS-protection, and you pay for this whenever you use &lt;code&gt;HashMap&lt;/code&gt; without overwriting the hash function.&lt;/p&gt;

&lt;p&gt;And, ignoring that point for a moment, The idea that your code is &amp;lsquo;secure by default&amp;rsquo; is a dangerous one and promotes ignorance about security. You code is &lt;em&gt;not&lt;/em&gt; secure by default.&lt;/p&gt;

&lt;p&gt;If you really do need a fast and yet secure hash function, Sip-hasher is a wonderful choice. To be clear, I&amp;rsquo;m not arguing against Sip-hasher (I actually like the function), but rather against having it as a default choice.&lt;/p&gt;

&lt;h2 id=&#34;btreemap&#34;&gt;&lt;code&gt;BTreeMap&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;BTreeMap&lt;/code&gt; and &lt;code&gt;BTreeSet&lt;/code&gt; are generally a good implementation. My only criticism has to do with cache efficiency, which is relatively bad when compared to other implementations (Java, mainly). In fact, around 60% (&lt;strong&gt;update&lt;/strong&gt;: Some have obtained other results, ranging from 10%-60%.) of the links followed leads to cache misses. For a map 1000 elements, a lookup would result in approximately 6 cache misses. For 10000, the number is 8.&lt;/p&gt;

&lt;p&gt;These can be quite expensive. A solution is proposed in the section about &amp;ldquo;Cache-efficient structures&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;vecdeque&#34;&gt;&lt;code&gt;VecDeque&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;VecDeque&lt;/code&gt; is a decent implementation. The only problem is that you cannot range index (slice) it. This is due to the very nature of ring buffers.&lt;/p&gt;

&lt;p&gt;An alternative to conventional ring buffers is &lt;a href=&#34;http://www.codeproject.com/Articles/3479/The-Bip-Buffer-The-Circular-Buffer-with-a-Twist&#34;&gt;biparite buffers&lt;/a&gt;, which has essentially the same performance, but allows this (and other interesting) API.&lt;/p&gt;

&lt;h2 id=&#34;mpsc&#34;&gt;MPSC&lt;/h2&gt;

&lt;p&gt;MPSC is a popular tool for message passing, but a critical point is often overlooked: Every queuing/dequeueing involves a malloc call. That sounds pretty bad, doesn&amp;rsquo;t it?&lt;/p&gt;

&lt;p&gt;MPSC is supposed to be lock-less, but that isn&amp;rsquo;t the case if the tcache is empty. Then it involves a lock.&lt;/p&gt;

&lt;p&gt;Since you effectively queue and dequeue all of the time, you actually waste allocations going in and out the allocator. That&amp;rsquo;s a major overhead, and totally unreflected in the API, giving an illusion of zero-cost.&lt;/p&gt;

&lt;p&gt;And, it turns out that it isn&amp;rsquo;t necessary. Because of the ring-buffer-like structure of MPSC, you can effectively store it all in a concurrent SLOB list, making malloc calls incredibly rare (ideally only upon the first queue).&lt;/p&gt;

&lt;p&gt;My benchmarks I&amp;rsquo;ve made on &lt;a href=&#34;https://github.com/redox-os/ralloc&#34;&gt;ralloc&lt;/a&gt;, a memory allocator I wrote, (which uses mpsc internally for cross-thread frees) shows a significant performance gain. An exercise for the reader is to do the same for Servo and try to see if it affects performance.&lt;/p&gt;

&lt;h2 id=&#34;vec&#34;&gt;&lt;code&gt;Vec&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;My criticism of &lt;code&gt;Vec&lt;/code&gt; is the lack of API for manual management. One particular missing thing is the ability to replace the reallocation strategy with a custom one.&lt;/p&gt;

&lt;p&gt;Vectors are used everywhere and they often have different usage patterns, many of which can be exploited to improve performance and memory efficiency.&lt;/p&gt;

&lt;p&gt;Another thing I sometimes need is the ability to get a mutable reference to the element I pushed without needing extra bound checks (note that in most cases this is a trivial optimization for LLVM, but it adds a lot of convenience). This could simply be solved by having &lt;code&gt;push&lt;/code&gt; return &lt;code&gt;&amp;amp;mut T&lt;/code&gt;. This is technically a breaking change but I doubt it will affect anybody.&lt;/p&gt;

&lt;h1 id=&#34;cache-efficient-structures&#34;&gt;Cache-efficient structures&lt;/h1&gt;

&lt;p&gt;I spoke a little about SLOB-based arenas previously. It turns out to have major impact on the cache efficiency, and thereby the performance, of the structure.&lt;/p&gt;

&lt;p&gt;The idea is that each structure holds an arena which only spans a few memory pages, ensure data locality. Obviously, this is more memory hunky, but it is conceptually similar to vectors which reserve extra memory to avoid reallocation. In this case, we are looking for avoiding allocation instead.&lt;/p&gt;

&lt;p&gt;Depending on what you&amp;rsquo;re doing it affects the performance positively by 3-10% (B-trees), 5-15% (linked lists), or 40-80% (mpsc). Those numbers are quite impressive (especially the last one).&lt;/p&gt;

&lt;h1 id=&#34;replacing-the-allocator&#34;&gt;Replacing the allocator&lt;/h1&gt;

&lt;p&gt;Another lacking feature is an &lt;code&gt;Allocator&lt;/code&gt; trait, which is intended to be the bound of some generic parameter in all the collections, allowing you to replace the allocator to exploit allocation patterns of the structure.&lt;/p&gt;

&lt;p&gt;An RFC for exactly this &lt;a href=&#34;https://github.com/rust-lang/rfcs/pull/1398&#34;&gt;already exists&lt;/a&gt; and is merged, but the implementation is incomplete.&lt;/p&gt;

&lt;h1 id=&#34;hiding-box&#34;&gt;Hiding &lt;code&gt;Box&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;An unfortunate thing is hiding the overhead by letting the function itself allocate, instead of letting the caller do it. This is (or at least, should be) considered bad style, because the API ought to reflect the semantics and performance characteristics. If the allocation is hidden to the programmer, she might not realize the expensive operations behind the scenes.&lt;/p&gt;

&lt;h1 id=&#34;the-good-parts&#34;&gt;The good parts&lt;/h1&gt;

&lt;p&gt;The implementations them self are really good and well-tested, and many of the points I made above are only relevant, when you are looking for very fine-grained performance. I have not criticized the API itself, because I think it does very well. Rust&amp;rsquo;s collection API is one of the most well-designed I&amp;rsquo;ve seen.&lt;/p&gt;

&lt;p&gt;It is also worth noting that the ecosystem contains lots of wonderful structures and implementations of these. Rust&amp;rsquo;s ecosystem is getting more mature by every day, and to this day, it even contains very niche structures for very specific purposes, and that&amp;rsquo;s really great since it means expansion of Rust&amp;rsquo;s domain.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Rust&amp;rsquo;s &amp;ldquo;collection of collections&amp;rdquo; has quite a few short-fallings. Fortunately, Most of the problems described above are not inherent, and can be fixed. The first step through fixing a problem is diagnosing it, and I hope that this post is able to initiate some critical discussion around the implementation, API, and choice of collections for the standard library.&lt;/p&gt;

&lt;p&gt;There are already many different proposals floating around in the community, as well as implementations and so on. I hope we can look into lifting these or replacing them in libstd.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/contain-rs&#34;&gt;contain-rs&lt;/a&gt; GitHub organization does a great job at providing a collection of various data structures, most of which are incredibly well-written. It would be interesting to see some of those in the standard library.&lt;/p&gt;

&lt;h1 id=&#34;an-apology&#34;&gt;An apology&lt;/h1&gt;

&lt;p&gt;The original title was exaggerated and inflammatory. To clarify, I don&amp;rsquo;t think it is horrible, nor even bad, rather I&amp;rsquo;d want a discussion/critical examination of the module and ways to improve it. I do realize that many people have spend a lot of time on implementing all these, and I admire it, calling it horrible (even if it is only in the title) is not fair, and it will likely distract the reader from the message of the post instead of inciting discussion. The mistake was entirely mine, and I should have given it a better title. Sorry.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lambda crabs (part 3): Region-based alias analysis</title>
      <link>http://ticki.github.io/blog/lambda-crabs-part-3-region-based-alias-analysis/</link>
      <pubDate>Wed, 08 Jun 2016 11:24:24 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/lambda-crabs-part-3-region-based-alias-analysis/</guid>
      <description>

&lt;p&gt;In the &lt;a href=&#34;http://ticki.github.io/blog/lambda_crabs_2/&#34;&gt;last post&lt;/a&gt;, we saw how to
infer regions and their span. In this post, we will cover aliasing and how to
ensure guarantees through region analysis.&lt;/p&gt;

&lt;h2 id=&#34;aliasing-mutable-aliasing-and-unsafety&#34;&gt;Aliasing, mutable aliasing, and unsafety.&lt;/h2&gt;

&lt;p&gt;Two pointers are said to be &lt;em&gt;aliased&lt;/em&gt;, if they refer to the same object. Alias
analysis is essential to program verification, optimizers, and compiler theory.&lt;/p&gt;

&lt;p&gt;Alias analysis is the study of which pointers are aliased and, more
importantly, which pointers &lt;em&gt;aren&amp;rsquo;t aliased&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Rust guarantees that no mutable reference is aliased. This is statically
checked, and we will show how in this post.&lt;/p&gt;

&lt;p&gt;So, why is aliasing guarantees even needed?&lt;/p&gt;

&lt;p&gt;The answer is that I need to be able to reason about the invariants of the
pointers content, while being sure that those aren&amp;rsquo;t broken in the period of
accessibility.&lt;/p&gt;

&lt;p&gt;Furthermore, we want strict thread-safety, which requires guarantees about
shared mutable state.&lt;/p&gt;

&lt;h2 id=&#34;different-values-different-namespaces&#34;&gt;Different values, different namespaces&lt;/h2&gt;

&lt;p&gt;To reason about mutability overlaps and aliasing through regions, we need a
notion of different namespaces.&lt;/p&gt;

&lt;p&gt;For example, say some variable X is referenced in a lifetime &lt;code&gt;&#39;a&lt;/code&gt;. Does that
mean another variable Y living in the same scope can&amp;rsquo;t be mutated?&lt;/p&gt;

&lt;p&gt;Of course not! Let&amp;rsquo;s consider:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;{
    let mut a = 2; // ------+ &#39;a
    let b = &amp;amp;a;    // ----+ | &#39;b
    let mut c = 0; // --+ | | &#39;c
    c = 1;         //   | | |
    c = 2;         //   | | |
} // -------------------+-+-+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see &lt;code&gt;a&lt;/code&gt; is aliased, thus mutating it is not allowed. However, &lt;code&gt;&#39;a:
&#39;c&lt;/code&gt;, but that doesn&amp;rsquo;t mean they refer to the same.&lt;/p&gt;

&lt;p&gt;Holding a global namespace would make the above example fail, since it has no
distinction between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; and their respective lifetimes.&lt;/p&gt;

&lt;p&gt;For that reason, we need to segregate the regions, such that we can effectively
reason about aliasing without mixing values up.&lt;/p&gt;

&lt;h2 id=&#34;sublattices&#34;&gt;Sublattices&lt;/h2&gt;

&lt;p&gt;We talked a bit about lattices and their applications in the last part. I
recommend reading that if you do not know what a lattice is.&lt;/p&gt;

&lt;p&gt;Now, let&amp;rsquo;s introduce the notion of a &lt;em&gt;sublattice&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;M&lt;/em&gt; is a sublattice of &lt;em&gt;L&lt;/em&gt;, if &lt;em&gt;M&lt;/em&gt; is a nonempty subset of &lt;em&gt;L&lt;/em&gt; forming a
lattice under &lt;em&gt;L&lt;/em&gt;&amp;rsquo;s meet and join operators.&lt;/p&gt;

&lt;p&gt;Take a lattice,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       Join(a, b, c)
           /\
          /  \
         /    \
   Join(a, b)  \c
       /\      /
      /  \    /
     /    \  /
   a/      \/b
    \      /
     \    /
      \  /
       \/
    Meet(a, b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(by the way, this is why it is called a lattice)&lt;/p&gt;

&lt;p&gt;then&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Join(a, b)
       /\
      /  \
     /    \
  a /      \b
    \      /
     \    /
      \  /
       \/
    Meet(a, b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;is a sublattice, since it holds all the conditions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It is a nonempty subset.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;It shares meet and join, while preserving closure (you can easily check this
yourself).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;region-classes&#34;&gt;Region classes&lt;/h2&gt;

&lt;p&gt;Region classes has many names, but none which is universally agreed upon, I
prefer the name region classes. As we say, a rose by any other name would still
smell as sweet.&lt;/p&gt;

&lt;p&gt;Let &lt;em&gt;L&lt;/em&gt; be our region lattice, define a &lt;em&gt;region class&lt;/em&gt; of &lt;em&gt;L&lt;/em&gt; as a sublattice
of &lt;em&gt;L&lt;/em&gt;, in the context of segregating regions.&lt;/p&gt;

&lt;p&gt;In particular, assign each value a region class. Say the value has the bounds
(outlives) &lt;code&gt;{a, b, c, d...}&lt;/code&gt;, then we derive our region class as the cyclic
sublattice, &lt;code&gt;&amp;lt;a, b, c, d...&amp;gt;&lt;/code&gt;. In particular, this means &lt;em&gt;the smallest
extension which forms a sublattice of L&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The compiler keeps a log of the region class of every variable. This is then
used for alias analysis:&lt;/p&gt;

&lt;h2 id=&#34;pointers-and-references&#34;&gt;Pointers and references&lt;/h2&gt;

&lt;p&gt;Taking an immutable reference extends our region class to contain the region of
this particular reference, denoted &lt;code&gt;M[N]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mutable references, on the other hand, works slightly different. The region
class and the region of the reference &lt;em&gt;must be disjoint&lt;/em&gt;, unless we get shared
mutability. With this requirement satisfied, we can proceed to extend the
region class with the new region.&lt;/p&gt;

&lt;h2 id=&#34;mutating-a-local-variable&#34;&gt;Mutating a local variable&lt;/h2&gt;

&lt;p&gt;You may ask, &amp;ldquo;Can you mutate a local variable while it is borrowed?&amp;rdquo;, the
answer is, &amp;ldquo;No, you cannot&amp;rdquo;. The reason is the same for the mutable aliasing:
it introduce shared mutable state.&lt;/p&gt;

&lt;p&gt;But, how do we handle such mutations?&lt;/p&gt;

&lt;p&gt;We introduced &lt;code&gt;empty(x)&lt;/code&gt;, the empty region at &lt;code&gt;x&lt;/code&gt;, in the earlier blog posts.
And we can use this to interpret local mutations as well: as taking a mutable
reference for region &lt;code&gt;empty(x)&lt;/code&gt; and simply mutate it through the reference.&lt;/p&gt;

&lt;h2 id=&#34;applying-this-method&#34;&gt;Applying this method&lt;/h2&gt;

&lt;p&gt;If we get back to our example,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;{
    let mut a = 2; // ------+ &#39;a
    let b = &amp;amp;a;    // ----+ | &#39;b
    let mut c = 0; // --+ | | &#39;c
    c = 1;         //   | | |
    c = 2;         //   | | |
} // -------------------+-+-+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can see that the region class of &lt;code&gt;&#39;a&lt;/code&gt; is an extension of &lt;code&gt;&#39;b&lt;/code&gt;, but &lt;code&gt;&#39;c&lt;/code&gt; is
not entangled with &lt;code&gt;&#39;a&lt;/code&gt;&amp;rsquo;s region class. In particular, &lt;code&gt;&#39;c&lt;/code&gt; and &lt;code&gt;&#39;a&lt;/code&gt; belong to
different namespaces and thus, there is no shared mutability.&lt;/p&gt;

&lt;h2 id=&#34;region-classes-and-their-relations&#34;&gt;Region classes and their relations&lt;/h2&gt;

&lt;p&gt;A natural question that arise is: Why don&amp;rsquo;t we do region inference seperately
for each region class?&lt;/p&gt;

&lt;p&gt;The answer is that distinct region classes are far from unrelated. Each region
class simply defines a value and its aliases, but that doesn&amp;rsquo;t make it isolated
for the rest of &lt;em&gt;L&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If you look at our example above, you may notice that &lt;code&gt;&#39;a&lt;/code&gt; outlives &lt;code&gt;&#39;c&lt;/code&gt;,
despite being associated with a different region class.&lt;/p&gt;

&lt;h2 id=&#34;questions-and-errata&#34;&gt;Questions and errata&lt;/h2&gt;

&lt;p&gt;Ping me at #rust in Mozilla IRC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lambda crabs (part 2): Region inference is (not) magic.</title>
      <link>http://ticki.github.io/blog/lambda-crabs-part-2-region-inference-is-not-magic./</link>
      <pubDate>Mon, 06 Jun 2016 11:06:21 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/lambda-crabs-part-2-region-inference-is-not-magic./</guid>
      <description>

&lt;p&gt;This post will cover region (lifetime) inference with a mathematical and type
theoretical focus.&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The problem&lt;/h2&gt;

&lt;p&gt;Inference is a very handy concept. We no longer have to annotate redundant
types, which is a major pain point in languages, that lacks of type inference.&lt;/p&gt;

&lt;p&gt;Now, we want such an inference scheme for regions as well.&lt;/p&gt;

&lt;p&gt;We described the problem of region inference in &lt;a href=&#34;http://ticki.github.io/blog/lambda_crabs_1/&#34;&gt;last post&lt;/a&gt; as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So, this is just a classical optimization problem:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;  minimize    &#39;a
  subject to  A, B, C...

  A, B, C… are outlives relations. ‘a may or may not be free in those.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Namely, we want to minimize some lifetimes, while holding some conditions.&lt;/p&gt;

&lt;h2 id=&#34;adding-regions&#34;&gt;&amp;ldquo;Adding&amp;rdquo; regions&lt;/h2&gt;

&lt;p&gt;One thing we will use throughout the region inference algorithm is the notion of &amp;ldquo;adding&amp;rdquo; regions.&lt;/p&gt;

&lt;p&gt;You may have seen &lt;code&gt;&#39;a + &#39;b&lt;/code&gt; before. Intuitively, &lt;code&gt;&#39;a: &#39;b + &#39;c&lt;/code&gt; is equivalent to
&lt;code&gt;&#39;a: &#39;b, &#39;a: &#39;c&lt;/code&gt;, but we can go further and use &lt;code&gt;&#39;a + &#39;b&lt;/code&gt; as a way to construct
new regions:&lt;/p&gt;

&lt;p&gt;Define &lt;code&gt;&#39;a + &#39;b&lt;/code&gt; as the smallest region that outlives both &lt;code&gt;&#39;a&lt;/code&gt; and &lt;code&gt;&#39;b&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In a sense, you &amp;ldquo;widen&amp;rdquo; the region until it covers both regions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a:       I---------I
&#39;b:            I------------I
&#39;a + &#39;b:  I-----------------I
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;funky-but-useless-regions-under-addition-as-an-abelian-semigroup&#34;&gt;Funky but useless: Regions under addition as an abelian semigroup&lt;/h2&gt;

&lt;p&gt;A semigroup is an algebraic structure satisfying two properties:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Closure, for any a, b in S, a + b is contained in S.&lt;/li&gt;
&lt;li&gt;Associativity, for any a, b, and c in S, (a + b) + c = a + (b + c).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But in contrary to monoids, there is no identity element.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Abelian&amp;rdquo; means commutative. That is, a + b = b + a.&lt;/p&gt;

&lt;p&gt;And, in fact, regions follows all these rules, making it an abelian semigroup.&lt;/p&gt;

&lt;p&gt;We know two additional facts about our operator:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It follows from the fact &lt;code&gt;&#39;a: &#39;a&lt;/code&gt;, that a + a = a&lt;/li&gt;
&lt;li&gt;It follows from the fact &lt;code&gt;&#39;static: &#39;a&lt;/code&gt; for all &lt;code&gt;&#39;a&lt;/code&gt;, that &lt;code&gt;∃s∈L  ∀a∈L  s + a = s&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;regions-as-a-lattice&#34;&gt;Regions as a lattice&lt;/h2&gt;

&lt;p&gt;It makes much more sense to think of regions as a lattice. A lattice is a poset
with two operators defined on it:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Join, an unique supremum (that is, least upper-bound). This is our &lt;code&gt;+&lt;/code&gt;
operator.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Meet, an unique infimum (that is, greatest lower-bound). This isn&amp;rsquo;t very
useful for the matter of regions, but it is still defined on them.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;which follows a set of laws:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The law of commutativity: Both meet and join are commutative operators.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The law of associativity: Both meet and join are associative operators.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The law of absorption: Meet(a, Meet(a, b)) = Meet(a, b) and Join(a, Join(a, b)) = Join(a, b).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In fact, this describes our structure perfectly. In particular, L is an
&lt;em&gt;upper-bounded lattice&lt;/em&gt;, i.e. we have a maximal element (&lt;code&gt;&#39;static&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Lattice theory, which we will cover in-depth in a later post is perfect for
studying subtyping relations.&lt;/p&gt;

&lt;h2 id=&#34;directed-acyclic-graphs&#34;&gt;Directed Acyclic Graphs&lt;/h2&gt;

&lt;p&gt;A directed acyclic graph is a finite directed graph with no directed cycles.
That is, any arbitrary directed walk in the graph will &amp;ldquo;end&amp;rdquo; at some point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/6/61/Polytree.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s forget the &lt;code&gt;&#39;a: &#39;a&lt;/code&gt; case for a moment. As such, the regions under our
&lt;em&gt;strict&lt;/em&gt; outlive relation, &lt;em&gt;&amp;lt;&lt;/em&gt;, forms a directed acyclic graph (DAG).&lt;/p&gt;

&lt;p&gt;In particular, if two node are connected, with a directed edge A → B, A
represents a region, which &lt;em&gt;outlives&lt;/em&gt; B.&lt;/p&gt;

&lt;p&gt;Consider we take a reference &lt;code&gt;&amp;amp;&#39;b T&lt;/code&gt; where &lt;code&gt;T: &#39;a&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; &#39;static
 |
 v
&#39;a &amp;lt;---------\
 |           |
 |           |
 |           |
 v           |
&#39;b &amp;lt;------- &#39;a + &#39;b
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;handling-cycles&#34;&gt;Handling cycles&lt;/h2&gt;

&lt;p&gt;Every lifetime outlives itself, as explained in the last post. So our outlives
relation doesn&amp;rsquo;t form a DAG, due to these cycles.&lt;/p&gt;

&lt;p&gt;The solution is relatively simple, though.&lt;/p&gt;

&lt;p&gt;Let &lt;code&gt;{&#39;a, &#39;b, &#39;c, ...}&lt;/code&gt; be cycle such that &lt;code&gt;&#39;a &amp;lt; &#39;b &amp;lt; &#39;c ... &amp;lt; &#39;a&lt;/code&gt;. Due to
transitivity and antisymmetry, we can assume that &lt;code&gt;&#39;a = &#39;b = &#39;c = ...&lt;/code&gt;, thus we
can, without loss of generality, collapse the cycle into a single node.&lt;/p&gt;

&lt;p&gt;This lets us interpret the graph, where edges represents outlives relations, as
a DAG.&lt;/p&gt;

&lt;h2 id=&#34;recursively-widening-the-regions&#34;&gt;Recursively widening the regions&lt;/h2&gt;

&lt;p&gt;Say we want to infer the span of some node &lt;code&gt;&#39;a&lt;/code&gt;. Assume &lt;code&gt;&#39;a&lt;/code&gt; neighbors (outlives)
&lt;code&gt;&#39;b, &#39;c, &#39;d...&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Since we know the bound, we can say &lt;code&gt;&#39;a = &#39;b + &#39;c + &#39;d + ...&lt;/code&gt;, since this is the
smallest &amp;lsquo;a subject to the outlives conditions.&lt;/p&gt;

&lt;p&gt;Now, recursively do the same with &lt;code&gt;&#39;b, &#39;c, &#39;d, ...&lt;/code&gt; Since the graph is acyclic,
this will terminate at some point.&lt;/p&gt;

&lt;p&gt;On an implementation note: you can optimize this process by 1. deduplicating
the regions, 2. collapsing sums containing &lt;code&gt;&#39;static&lt;/code&gt; into &lt;code&gt;&#39;static&lt;/code&gt;, 3. caching the
nodes to avoid redundant calculations.&lt;/p&gt;

&lt;h2 id=&#34;going-further-liveness&#34;&gt;Going further: liveness&lt;/h2&gt;

&lt;p&gt;Now that we have a closed form for inferring lifetimes, we can do lots of cool stuff.&lt;/p&gt;

&lt;p&gt;Liveness of a value is the span starting where the value is declared and ending
where the last access to it is made. This is in contrary to the classical
lexical approach, where the initial lifetimes are assigned as the scopes of the
variables.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start by defining &lt;code&gt;empty(x)&lt;/code&gt; as the region spanning from x to x (that is,
an empty region at x). Assign every value declared at x a region, &lt;code&gt;empty(x)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Whenever a value of lifetime &lt;code&gt;&#39;x&lt;/code&gt; is used at some point y, we add a bound &lt;code&gt;&#39;x:
empty(y)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;So we essentially expand the region whenever used, effectively yielding the
liveness of the value.&lt;/p&gt;

&lt;h2 id=&#34;a-happy-ending&#34;&gt;A happy ending&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s it&amp;hellip; The algorithm is really that simple. In fact, you can implement it
in only a 100-200 lines.&lt;/p&gt;

&lt;h2 id=&#34;questions-and-errata&#34;&gt;Questions and errata&lt;/h2&gt;

&lt;p&gt;Ping me at #rust in Mozilla IRC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lambda crabs (part 1): A mathematical introduction to lifetimes and regions</title>
      <link>http://ticki.github.io/blog/lambda-crabs-part-1-a-mathematical-introduction-to-lifetimes-and-regions/</link>
      <pubDate>Mon, 06 Jun 2016 09:12:56 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/lambda-crabs-part-1-a-mathematical-introduction-to-lifetimes-and-regions/</guid>
      <description>

&lt;p&gt;This post will cover lifetimes and regions in depth, with a focus on the mathematical background of regions. That is, what is a region? What rules do they follow? How does the compiler handle them? And how are they inferred?&lt;/p&gt;

&lt;h2 id=&#34;regions-and-their-ordering&#34;&gt;Regions and their ordering&lt;/h2&gt;

&lt;p&gt;So, let&amp;rsquo;s briefly investigate what a region is. A region (or in Rust lingo, a lifetime) is a span of some form, e.g. the token stream. Regions have an outlive relation defined on them.&lt;/p&gt;

&lt;p&gt;A region &lt;code&gt;&#39;a&lt;/code&gt; outlives &lt;code&gt;&#39;b&lt;/code&gt; if &lt;code&gt;&#39;b&lt;/code&gt;&amp;rsquo;s span is covered by &lt;code&gt;&#39;a&lt;/code&gt;. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: I----------------I
&#39;b: I---------I
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see &lt;code&gt;&#39;a: &#39;b&lt;/code&gt; since the first span covers the second. But what is the nature of the outlives relation?&lt;/p&gt;

&lt;h2 id=&#34;regions-a-poset&#34;&gt;Regions: a poset&lt;/h2&gt;

&lt;p&gt;One could mistakenly believe that regions are ordered over their outlives relation. An totally ordered set A under ≤ means that any elements a, b ∈ A satisfy all of the following statements:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;If a ≤ b and b ≤ a are both satisfied, a = b.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;If a ≤ b and b ≤ c are both satisfied, a ≤ c.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;At least one of a ≤ b and b ≤ a is true.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To see why the outlives relation is not a total order over the set of regions, consider the case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: I---------I
&#39;b:    I------------I
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The third condition is not met here: neither &lt;code&gt;&#39;a: &#39;b&lt;/code&gt; or &lt;code&gt;&#39;b: &#39;a&lt;/code&gt; is true.&lt;/p&gt;

&lt;p&gt;It turns out that weakening the last condition to only consider reflexivity gives us a structure, that L (the set of regions) classifies. Replace 3. by a ≤ a, and you get a partially ordered set, or a poset.&lt;/p&gt;

&lt;h2 id=&#34;outlive-relation-as-a-partial-order&#34;&gt;Outlive relation as a partial order&lt;/h2&gt;

&lt;p&gt;So, let&amp;rsquo;s briefly explain how the rules of outliving mirrors the rules of partial orders.&lt;/p&gt;

&lt;p&gt;The first rule, the rule of antisymmetry, reads&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: &#39;b
&#39;b: &#39;a
-------
&#39;a = &#39;b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So if two regions (lifetimes, borrows, scopes, etc.) outlives each other symmetrically (&amp;lsquo;a: &amp;lsquo;b and &amp;lsquo;b: &amp;lsquo;a), they are, in fact, the same.&lt;/p&gt;

&lt;p&gt;The second rule, the rule of transitivity, is crucial to understanding the semantics of regions:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: &#39;b
&#39;b: &#39;c
------
&#39;a: &#39;c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In other words, regions are hierarchical. It might seem very simple, but the implications are in fact very important: it allows us to conclude things from transitivity. Think of it like you can &amp;ldquo;inherit&amp;rdquo; bounds from outliving regions.&lt;/p&gt;

&lt;p&gt;For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;a: I--------------------I
&#39;b:   I----------------I
&#39;c:      I---------I
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Say we know that, &lt;code&gt;&#39;a: &#39;b&lt;/code&gt;, and &lt;code&gt;&#39;b: &#39;c&lt;/code&gt;. We can then conclude that &lt;code&gt;&#39;a: &#39;c&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The last rule simply states that &amp;lsquo;a outlives itself. This might seem counterintuitive due to the odd terminology, but think of outlives as &amp;ldquo;outlives or equals to&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;In fact, there is only one more thing we know about regions: they have an unique maximal extrema, which outlives all other regions, &lt;code&gt;&#39;static&lt;/code&gt;. Namely, &lt;code&gt;&#39;static&lt;/code&gt; outlives any region, &lt;code&gt;&#39;a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;And that&amp;rsquo;s all the &amp;ldquo;axioms&amp;rdquo; of lifetimes.&lt;/p&gt;

&lt;h2 id=&#34;what-subtyping-is&#34;&gt;What subtyping is&lt;/h2&gt;

&lt;p&gt;Before we go to next section, we will just have to briefly cover subtyping. τ is said to be a subtype of υ (denoted &lt;code&gt;τ &amp;lt;: υ&lt;/code&gt;), if a &lt;em&gt;type mismatch&lt;/em&gt;, such that τ is inferred to be of type υ, makes the value of type τ coerce into a value of type υ.&lt;/p&gt;

&lt;p&gt;In other words, you can replace your subtype by a supertype (the parent type) without getting a type mismatch error.&lt;/p&gt;

&lt;h2 id=&#34;regions-are-just-types-outlive-relation-as-a-subtyping-rule&#34;&gt;Regions are just types: Outlive relation as a subtyping rule&lt;/h2&gt;

&lt;p&gt;If you think about it, you may notice that lifetimes are used in type positions &lt;em&gt;a lot&lt;/em&gt;. This is no coincidence, since &lt;em&gt;regions are just types with a subtyping relation&lt;/em&gt;, which is the very reason you are allowed to do e.g. &lt;code&gt;MyStruct&amp;lt;&#39;a&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In fact, the outlive relation defines a subtyping rule. That is, you can always &amp;ldquo;shrink&amp;rdquo; a region span. Let &lt;em&gt;c&lt;/em&gt; be a type constructor, &amp;lsquo;a → *, then &amp;lsquo;a: &amp;lsquo;b implies that &lt;code&gt;&#39;a &amp;lt;: &#39;b&lt;/code&gt;, that is &lt;code&gt;&#39;a&lt;/code&gt; can coerce into &lt;code&gt;&#39;b&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, &lt;code&gt;&amp;amp;&#39;static str&lt;/code&gt; can coerce to any &lt;code&gt;&amp;amp;&#39;a str&lt;/code&gt;, since &lt;code&gt;&#39;static&lt;/code&gt; outlives any lifetime.&lt;/p&gt;

&lt;p&gt;Due to the implementation, there are a few limits, though. You can for example not do &lt;code&gt;let a: &#39;a&lt;/code&gt; which would be useless anyways.&lt;/p&gt;

&lt;p&gt;Syntactically, there is a confusion: lifetimes appears in certain trait places, especially in &lt;em&gt;trait bounds&lt;/em&gt;. But, in fact, that is only a syntactic sugar for an imaginary trait, let&amp;rsquo;s call it &lt;code&gt;Scope&lt;/code&gt;, which takes a lifetime.&lt;/p&gt;

&lt;p&gt;This represents the scope of a type, so when writing &lt;code&gt;fn my_func::&amp;lt;T: &#39;static&amp;gt;()&lt;/code&gt; you can think of it as writing &lt;code&gt;fn my_func::&amp;lt;T: Scope&amp;lt;&#39;static&amp;gt;&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Due to the coercion rules (which will be covered in a future post), this means that if &lt;code&gt;T: Scope&amp;lt;&#39;a&amp;gt;&lt;/code&gt; and &lt;code&gt;U: Scope&amp;lt;&#39;b&amp;gt;&lt;/code&gt; with &lt;code&gt;&#39;a: &#39;b&lt;/code&gt;, then &lt;code&gt;T&lt;/code&gt; is a subtype of &lt;code&gt;U&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;inferring-regions&#34;&gt;Inferring regions.&lt;/h2&gt;

&lt;p&gt;This is the exciting part. Rust has region inference, allowing it to infer the lifetimes in your program.&lt;/p&gt;

&lt;p&gt;Due to Rust&amp;rsquo;s aliasing guarantees, it tries to &lt;em&gt;minimize&lt;/em&gt; the region&amp;rsquo;s span, while still satisfying the conditions (outlives relations) given.&lt;/p&gt;

&lt;p&gt;So, this is just a classical optimization problem:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;minimize    &#39;a
subject to  A, B, C...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A, B, C&amp;hellip; are outlives relations. &lt;code&gt;&#39;a&lt;/code&gt; may or may not be free in those.&lt;/p&gt;

&lt;p&gt;We will cover how we actually solve this optimization problem in a future blog post, but until then you can see if you can find an algorithm to do so ;).&lt;/p&gt;

&lt;h2 id=&#34;questions-and-errata&#34;&gt;Questions and errata&lt;/h2&gt;

&lt;p&gt;Ping me at #rust in Mozilla IRC.&lt;/p&gt;

&lt;h2 id=&#34;credits&#34;&gt;Credits&lt;/h2&gt;

&lt;p&gt;Credits to Yaniel on IRC for the idea for the name of this series. It is based on the famous &amp;ldquo;lambda cats&amp;rdquo; series, but since Ferris, the crab, is our Rust mascot, we do lambda crabs, instead.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>