<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Ticki&#39;s blog</title>
    <link>http://ticki.github.io/post/index.xml</link>
    <description>Recent content in Posts on Ticki&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 May 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://ticki.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>You Are (Probably) Doing Login Systems Wrong</title>
      <link>http://ticki.github.io/blog/you-are-probably-doing-login-systems-wrong/</link>
      <pubDate>Fri, 12 May 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/you-are-probably-doing-login-systems-wrong/</guid>
      <description>&lt;p&gt;A thing, most programmers have tried at least once, is login systems. Despite
being seemingly a simple task, it is in fact very hard to do right.&lt;/p&gt;

&lt;p&gt;So, let&#39;s look into, how we can actually do this right.&lt;/p&gt;

&lt;h1 id=&#34;storing-passwords&#34;&gt;Storing passwords&lt;/h1&gt;

&lt;p&gt;Okay, this is common knowledge: Salt and hash your passwords.&lt;/p&gt;

&lt;p&gt;However, it is often done wrong. You&#39;ll see code like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hash(password + salt)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is better than unsalted, unhashed passwords, but it&#39;s far from bruteforce
resistant. Why? Because hashing is cheap. With proper machines, you can do
billions of them in a second. A dictionary attack is piece of cake.&lt;/p&gt;

&lt;p&gt;So how do we solve this? Well, we use a KDF. A KDF (key derivation function)
acts like a slow hash function. A hash function, where calculating takes maybe
100 ms. or more.&lt;/p&gt;

&lt;p&gt;In general, two kinds of KDFs exists, the CPU-based and the CPU-memory hybrids.
The CPU based are still in use, but I don&#39;t recommend using them, as they can
easily be calculated with ASICs. The CPU-memory hybrids requires some amount of
memory for calculating the hash value, often making it substantially harder to
create ASICs.&lt;/p&gt;

&lt;p&gt;For the reasons stated above, I recommend &lt;code&gt;scrypt&lt;/code&gt;, for a modern, well-known
an secure KDF.&lt;/p&gt;

&lt;h1 id=&#34;sessions&#34;&gt;Sessions&lt;/h1&gt;

&lt;p&gt;In general, sessions should be assigned a token by the server. This token is
shared with the client (e.g. through a cookie) as the way to prove, that they
are logged in with a given account to the server.&lt;/p&gt;

&lt;p&gt;There is a few things to keep in mind, though.&lt;/p&gt;

&lt;p&gt;The session token shall have an expiration date, for security reasons.
Furthermore, the session token shall be deactivated when the user logs out.&lt;/p&gt;

&lt;p&gt;Lastly, the session must not be shared through GET or other logged means.
Rather, it should be stored in a cookie or &lt;code&gt;localStorage&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&#34;clientside-hashing&#34;&gt;Client-side hashing&lt;/h1&gt;

&lt;p&gt;A pretty uncommon, but really good practice, is the client-side hashing. It is
supposed act as another layer of security, hiding the password from the server.&lt;/p&gt;

&lt;p&gt;The idea is that the client-side should hash (e.g. &lt;code&gt;scrypt&lt;/code&gt;) the password
before sending it to the server.&lt;/p&gt;

&lt;p&gt;This can seem pretty pointless, as (in the case of the web) the server could
simply change the JavaScript to leak the password, but there is a reason: If
the server-side has a bug that allows to read certain chosen memory locations
(a buffer overflow, for example), it could be exploited to read the plaintext
password.&lt;/p&gt;

&lt;p&gt;Instead, with client-side hashing, it can only read the hashed value. This of
course doesn&#39;t stop the hacker from logging in to the user&#39;s account, but it
stops them from obtaining the potentially reused plain-text password.&lt;/p&gt;

&lt;h1 id=&#34;rate-limiting&#34;&gt;Rate limiting&lt;/h1&gt;

&lt;p&gt;Rate limiting is really, really important, even though it is often understated.
It prevents someone from bruteforcing common passwords.&lt;/p&gt;

&lt;p&gt;But how should the rate limiting work?&lt;/p&gt;

&lt;p&gt;One neat way is the &amp;quot;leaky bucket algorithm&amp;quot;. It works by having requests
&amp;quot;dripping&amp;quot; into a bucket like drops of water. When the leaky bucket is filled,
no more requessts can be made, until the bucket has leaked to empty.&lt;/p&gt;

&lt;p&gt;In less figurative language, you have a counter on every visitor IP address,
which is incremented on every request (e.g. login, create account etc.). When
this counter is above some level (say 5), a timeout is set (e.g. the time when
it expire is set as a field of the user). First when this timeout expires, the
counter resets, and new requests can be made.&lt;/p&gt;

&lt;h1 id=&#34;resetting-accounts&#34;&gt;Resetting accounts&lt;/h1&gt;

&lt;p&gt;Of course a proper system must have the ability to reset accounts. There are
many ways of doing this.&lt;/p&gt;

&lt;p&gt;I would recommend to have the user provide username and E-mail, as it—contrary
to other approaches—does not allow spamming or denial of service attacks. It is
important that you do not reveal whether or not the E-mail matched, as that can
be a breach of privacy of the user (such thing can be used to check if the
E-mail address matched).&lt;/p&gt;

&lt;p&gt;If the E-mail is only going to be used for resetting, I strongly recommend that
you store a fingerprint (e.g. scrypt) of the E-mail address rather than the
plaintext version. This still ensures that you can check if the given E-mail
address of the resetting user matches, without the server side knowing anything
about the address. It prevents the database being misused or sold for spam
purposes, and also helps to protect the user&#39;s identity.&lt;/p&gt;

&lt;p&gt;When the user resets, a token shall be generated. This token is used in a link,
sent to the user through an E-mail. It is important that this token is
sufficiently long, random, and furthermore, that it expires (ideally within only
a few hours).&lt;/p&gt;

&lt;p&gt;Note that upon resetting, the token should removed from the map.&lt;/p&gt;

&lt;h1 id=&#34;other-tips&#34;&gt;Other tips&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Consider implementing a common two-factor key-sharing algorithm, such as
&lt;a href=&#34;https://en.wikipedia.org/wiki/Time-based_One-time_Password_Algorithm&#34;&gt;TOTP&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I recommend that changing E-mail and password are done under the &amp;quot;reset
password&amp;quot; formula, such that E-mail confirmation is required.&lt;/li&gt;
&lt;li&gt;It is extremely important that the connection, where the login happens is
secure, for example &lt;code&gt;https&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Well, that&#39;s it.&lt;/p&gt;

&lt;p&gt;You have to be careful about what you do, though. This is a minefield of
vulnerabilities, and you have to be very careful not to introduce subtle bugs
in your code. I recommend that you let somebody other than yourself review your
code, to give you another&#39;s perspective on the code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Lazy Argument of Human Nature</title>
      <link>http://ticki.github.io/blog/the-lazy-argument-of-human-nature/</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/the-lazy-argument-of-human-nature/</guid>
      <description>&lt;p&gt;When you discuss socialism, you are guaranteed to hear someone say &amp;quot;socialism won&#39;t work because of human nature&amp;quot;. This argument is particularly lazy, but I keep hearing it, so I wanted to address it in my own words.&lt;/p&gt;

&lt;h2 id=&#34;what-the-argument-entails&#34;&gt;What the argument entails&lt;/h2&gt;

&lt;p&gt;It is incredibly vague, but it usually goes something like&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Greed and selfishness is an essential part of human nature, hence socialism will fail.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It often follows that capitalism is the only system that works.&lt;/p&gt;

&lt;p&gt;So where does this argument fail?&lt;/p&gt;

&lt;h2 id=&#34;human-nature-is-not-capitalistic&#34;&gt;Human nature is not capitalistic&lt;/h2&gt;

&lt;p&gt;First off all, human nature is not capitalist. Capitalism means that the means of production is privately owned, and the surplus value of the labour goes to the owner of the production.&lt;/p&gt;

&lt;p&gt;The human race has lived far longer without capitalism than with it. In fact, capitalism is a relatively recent invention. In most countries it did not exist until the 19th century. It is simply ahistorical to say that capitalism is a part of human nature, unless you think that human nature is something recent.&lt;/p&gt;

&lt;p&gt;It is &lt;strong&gt;not&lt;/strong&gt; human nature to spend most of your day producing surplus value for a small elite, who gives you little to nothing in return. Nor is it human nature to exploit child workers in the third world in order to consume endlessly. This is the inevitable result of capitalism - a system of exploitation.&lt;/p&gt;

&lt;p&gt;I want to direct your to following quote from The Conquest of Bread (by Peter Kropotkin):&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;However, the moment we consider human history more attentively, it loses its strength. We see, first, that hundreds of millions¹ of men have succeeded in maintaining amongst themselves, in their village communities, for many hundreds of years, one of the main elements of Socialism—the common ownership of the chief instrument of production, the land, and the apportionment of the same according to the labour capacities of the different families; and we learn that if the communal possession of the land has been destroyed in Western Europe, it was not from within, but from without, by the governments which created a land monopoly in favour of the nobility and the middle classes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;¹before someone points out that this number is in fact not true: This book was written in the late 19th century and thus not with the knowledge from contemporary sciences.&lt;/p&gt;

&lt;h2 id=&#34;capitalism-does-not-mean-trade&#34;&gt;Capitalism does not mean trade&lt;/h2&gt;

&lt;p&gt;Trade has indeed been going on since the early days of civilization. It is however distinct from capitalism in an important way, that many people seem to miss: Capitalism means that the means of production is not controlled by the workers, but instead a system of investors, capitalists, etc.. Trade simply means the exchange of goods for money or other goods.&lt;/p&gt;

&lt;p&gt;Socialism is in no way incompatible with trade nor ownership². In fact, many socialists (like myself) believes in trade of goods and services.&lt;/p&gt;

&lt;p&gt;²there is a distinction between private and personal ownership in socialist theory. &lt;a href=&#34;https://www.youtube.com/watch?v=eknoQYrgq60&#34;&gt;Here&#39;s&lt;/a&gt; a video, which does a good job at explaining this distinction.&lt;/p&gt;

&lt;h2 id=&#34;human-nature-is-adaptive&#34;&gt;Human nature is adaptive&lt;/h2&gt;

&lt;p&gt;Many socialists thinks that human nature is a &amp;quot;spook&amp;quot; (to invoke the wonderful terminology of Max Stirner) and outright nonexistent. I tend to disagree with that. Human certainly have some nature, however there is one particular thing that makes it radically different from other species.&lt;/p&gt;

&lt;p&gt;Ability to adaption. This is why the human species is so successful: We are able to adapt to almost any environment we are put in. There is a reason that humans are the only mammal to walk all continents -- it&#39;s the adaptive human nature.&lt;/p&gt;

&lt;p&gt;The human nature adapts to whatever environment, such as economic system, it&#39;s under. If a system A rewards behavior B, then this behavior will naturally become more common under this system.&lt;/p&gt;

&lt;h2 id=&#34;greed-is-exactly-why-capitalism-fails&#34;&gt;Greed is exactly why capitalism fails&lt;/h2&gt;

&lt;p&gt;What is particularly weird about the argument is the point about &amp;quot;greed&amp;quot;. Any system that rewards greed will obviously take basis in greed, and humans are greedy, which is why you want to avoid rewarding such behavior.&lt;/p&gt;

&lt;p&gt;This is the opposite of what capitalism does: It rewards greed. If you want to build a successful business, you must create as inhumane work condition as possible³.&lt;/p&gt;

&lt;p&gt;Capitalism is responsible for this -- and it&#39;s not an issue of &amp;quot;free markets&amp;quot;, it is at the core of capitalism. Rosa Luxemburg wrote a good book on &lt;a href=&#34;https://www.marxists.org/archive/luxemburg/1900/reform-revolution/&#34;&gt;why reformism fails&lt;/a&gt;: Reformism fails to actually address the problems with capitalism. Reformed capitalism is still capitalism, and it will still have the same fundamental properties of capitalism.&lt;/p&gt;

&lt;p&gt;The failure of capitalism isn&#39;t in its functioning: It clearly works, but it&#39;s in its ethical aspect. It relies on exploitation and fails to distribute wealth fairly, causing mass starvation next to extreme wealth.&lt;/p&gt;

&lt;p&gt;³although recently this oppression have been moved to the third world. I&#39;ll elaborate on this in a future post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A general construction for rolling hash functions</title>
      <link>http://ticki.github.io/blog/a-general-construction-for-rolling-hash-functions/</link>
      <pubDate>Thu, 02 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/a-general-construction-for-rolling-hash-functions/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;h1 id=&#34;what-is-a-rolling-hash-function&#34;&gt;What is a rolling hash function?&lt;/h1&gt;

&lt;p&gt;A hash function is a function &lt;span  class=&#34;math&#34;&gt;\(h : S^\times \to F\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(S, F\)&lt;/span&gt; being some finite sets.&lt;/p&gt;

&lt;p&gt;A rolling hash function is really a set of functions &lt;span  class=&#34;math&#34;&gt;\((h, u)\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(u\)&lt;/span&gt; allows retroactively updated a symbol&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[h(\ldots a \ldots) \mapsto h(\ldots a&#39; \ldots)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To put it more formally, a rolling hash function has an associated function &lt;span  class=&#34;math&#34;&gt;\(u : F \times S^2 \times \mathbb N \to F\)&lt;/span&gt;, satisfying&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[u(n, a, a&#39;, h(\underbrace{\ldots}_n a \ldots)) = h(\underbrace{\ldots}_n a&#39; \ldots)\]&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&#34;an-example&#34;&gt;An example&lt;/h2&gt;

&lt;p&gt;One of my favorite examples of a rolling hash function is the Rabin-Karp rolling hash.&lt;/p&gt;

&lt;p&gt;Essentially, you pick some prime &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; and do following operation (over &lt;span  class=&#34;math&#34;&gt;\(\mathbb Z_n\)&lt;/span&gt;):&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[h(\{c_n\}) = c_1 p^{k - 1} + c_2 p^{k - 2} + \ldots + c_k\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;You might be able to figure out how you can construct &lt;span  class=&#34;math&#34;&gt;\(u\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[u(n, x, x&#39;, H) = H + (x&#39; - x) p^{k - n}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So why isn&#39;t this a pretty good choice? Well, it&#39;s&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Slow. Doing the exponentation can be quite expensive.&lt;/li&gt;
&lt;li&gt;It&#39;s relatively poor quality. This can be shown by looking at the behavior of the bits: Multiplication never affects lower bits, so it&#39;s avalanche effect is very weak.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It has a really nice property though, you can use an arbitrary substring of the input and the substring&#39;s hash and replace it in &lt;span  class=&#34;math&#34;&gt;\(O(1)\)&lt;/span&gt;, whereas most other rolling hash functions requires &lt;span  class=&#34;math&#34;&gt;\(O(n)\)&lt;/span&gt;.&lt;/p&gt;

&lt;h1 id=&#34;a-generalpurpose-construction&#34;&gt;A general-purpose construction&lt;/h1&gt;

&lt;p&gt;So, is there a general way we can come up with these?&lt;/p&gt;

&lt;p&gt;Well, what if we had some family of permutations, &lt;span  class=&#34;math&#34;&gt;\(\sigma_n : F \to F\)&lt;/span&gt;?&lt;/p&gt;

&lt;p&gt;Assume our &lt;span  class=&#34;math&#34;&gt;\(F\)&lt;/span&gt; is an abelian group with some operation &lt;span  class=&#34;math&#34;&gt;\(+\)&lt;/span&gt; (could be addition or XOR or a third option).&lt;/p&gt;

&lt;p&gt;Then, construct the hash function&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[h(\{x_n\}) = \sum_n \sigma_n(x_n)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Now, we can easily construct &lt;span  class=&#34;math&#34;&gt;\(u\)&lt;/span&gt;:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[u(n, x, x&#39;, H) = H - \sigma_n(x_n) + \sigma_n(x&#39;_n)\]&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;xor-special-case&#34;&gt;XOR special case&lt;/h1&gt;

&lt;p&gt;As programmers, we love XOR, because it is so simple, and even better: Every element is its own inverse, under XOR.&lt;/p&gt;

&lt;p&gt;Namely, under XOR, &lt;span  class=&#34;math&#34;&gt;\(u\)&lt;/span&gt; would look like&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[u(n, x, x&#39;, H) = H \oplus \sigma_n(x_n) \oplus \sigma_n(x&#39;_n)\]&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&#34;rabinkarp-as-a-special-case&#34;&gt;Rabin-Karp as a special case&lt;/h1&gt;

&lt;p&gt;The interesting thing is that we can see Rabin-Karp as a special case, namely the family of permutations,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\sigma_n(x) \equiv xp^{n} \pmod m\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The reason this is a permutation is because &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; is odd, hence &lt;span  class=&#34;math&#34;&gt;\(p^n\)&lt;/span&gt; is odd, and must therefore have a multiplicative inverse in &lt;span  class=&#34;math&#34;&gt;\(\mathbb Z/m \mathbb Z\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Now, why does &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; have to be a prime? Well, Every permutation must be distinct, &lt;span  class=&#34;math&#34;&gt;\(f(x) \equiv p^x \pmod m\)&lt;/span&gt; is a permutation itself (which can be shown relatively easily through basic group theory).&lt;/p&gt;

&lt;h1 id=&#34;statistical-properties-and-qualities&#34;&gt;Statistical properties and qualities&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Flipping a single bit will change the output, no matter what: If &lt;span  class=&#34;math&#34;&gt;\(x \neq x&#39;\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(\sigma(x) \neq \sigma(x&#39;)\)&lt;/span&gt;, because &lt;span  class=&#34;math&#34;&gt;\(\sigma\)&lt;/span&gt; is a permutation.&lt;/li&gt;
&lt;li&gt;It has perfect collision property: Pick some &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt;-bit sequence, &lt;span  class=&#34;math&#34;&gt;\(s\)&lt;/span&gt;. The number of &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt;-bit sequences colliding with &lt;span  class=&#34;math&#34;&gt;\(s\)&lt;/span&gt; is independent of the choice of &lt;span  class=&#34;math&#34;&gt;\(s\)&lt;/span&gt; (all equivalence class have equal size).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;reduction-to-the-permutation-family&#34;&gt;Reduction to the permutation family&lt;/h2&gt;

&lt;p&gt;A lot of properties of the function are directly inherited from the quality of the permutation family. In fact, it can be shown that if the permutation family is a family of random oracles, the function is a perfect PRF.&lt;/p&gt;

&lt;p&gt;Similarly, if the permutations are uniformly distributed over some input, the constructed function will be as well.&lt;/p&gt;

&lt;p&gt;Almost all of the statistical properties, I can think of, has this kind of reductive property allowing us to prove it on the constructed property.&lt;/p&gt;

&lt;h2 id=&#34;a-good-family-of-permutations&#34;&gt;A good family of permutations&lt;/h2&gt;

&lt;p&gt;This is a really hard question. Analyzing a single permutation is easy, but analyzing a family of permutations can be pretty hard. Why? Because you need to show their independence.&lt;/p&gt;

&lt;p&gt;If one permutation had some dependence on another, the hash function could have poor quality, even if the permutations are pseudorandom, when studied individually.&lt;/p&gt;

&lt;h1 id=&#34;parallelization&#34;&gt;Parallelization&lt;/h1&gt;

&lt;p&gt;I&#39;m the author of &lt;a href=&#34;https://ticki.github.io/blog/seahash-explained/&#34;&gt;SeaHash&lt;/a&gt;, and a big part of the design of SeaHash was to parallelize it.&lt;/p&gt;

&lt;p&gt;And I could, pretty well. But with its design, there will always be a limit to this parallelization. In case of SeaHash, this limit is 4 (as there are 4 lanes). However, one could imagine hardware where such parallelization ideally should be say 32.&lt;/p&gt;

&lt;p&gt;This construction allows for exactly this, without changing the specification. The function is adaptive: The implementation can choose whatever number of parallel lanes to hash in.&lt;/p&gt;

&lt;p&gt;This can be done by simply breaking the input up in &lt;span  class=&#34;math&#34;&gt;\(k\)&lt;/span&gt; strings, and hashing each individually, starting with &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt; being the offset of the string.&lt;/p&gt;

&lt;p&gt;This is a fairly nice property, as it also allows combination of threaded parallelization and ILP without any constant overhead. Say I&#39;m hashing 4 TB of data, then I could spawn 4 threads (depending on your hardware) and still exploit the 4 CPU pipelines, while not hurting the performance of hashing only a few bytes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collision Resolution with Nested Hash Tables</title>
      <link>http://ticki.github.io/blog/collision-resolution-with-nested-hash-tables/</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/collision-resolution-with-nested-hash-tables/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;h1 id=&#34;collision-resolution&#34;&gt;Collision resolution&lt;/h1&gt;

&lt;p&gt;Hash collisions in hash tables are unevitable, and therefore every proper implementation needs a form of &lt;em&gt;collision resolution&lt;/em&gt;. Collision resolution is the name of the class of algorithms and techniques used to organize and resolve the case where two entries in the table hash to the same bucket.&lt;/p&gt;

&lt;p&gt;It turns out that the choice and implementation of collision resolution is absolutely critical for the performance of the table, because while hash tables are often mistaken for having &lt;span  class=&#34;math&#34;&gt;\(O(1)\)&lt;/span&gt; lookups, they do in reality and theory have a sligthly more complicated behavior.&lt;/p&gt;

&lt;p&gt;There are really two big competing families of algorithms of resolving collisions:&lt;/p&gt;

&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Open addressing: This keeps all entries in the bucket array, but applies some way of finding a new bucket if the ideal one is occupied. To keep the load factor low, they generally reallocate before they get full. This is an OK solution when doing things in-memory, but on-disk this absolutely sucks, since you potentially have millions of entries. If you plot the lookup time over the number of entries, it will look like an increasing line, which suddenly peaks and then falls, with the peaks getting more and more uncommon. This rather complex performance behavior can also make them unfit for certain purposes.&lt;/li&gt;
&lt;li&gt;Chaining: This uses some structure for storing multiple entries in a bucket. Often through B-trees or linked lists.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this post, we will look at chaining.&lt;/p&gt;

&lt;h1 id=&#34;nested-tables&#34;&gt;Nested tables&lt;/h1&gt;

&lt;p&gt;What if we used another hash table for multi-entry buckets?&lt;/p&gt;

&lt;p&gt;The idea is that we have some sequence of independent (this is a very important invariant) hash functions &lt;span  class=&#34;math&#34;&gt;\(\{h_n(x)\}\)&lt;/span&gt;, and the root table uses hash function &lt;span  class=&#34;math&#34;&gt;\(h_0(k)\)&lt;/span&gt; to assign a bucket to key &lt;span  class=&#34;math&#34;&gt;\(k\)&lt;/span&gt;. If the entry is empty, the value is simply inserted there and the bucket&#39;s tag is set to &amp;quot;single&amp;quot;.&lt;/p&gt;

&lt;p&gt;If however the bucket already has at least one entry, it will be inserted into a hash table, placed in said bucket, and the bucket where the entry will be (in the new hash table) is determined by &lt;span  class=&#34;math&#34;&gt;\(h_1(k)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;This process (&lt;span  class=&#34;math&#34;&gt;\(h_0(k)\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(h_1(k)\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(h_2(k)\)&lt;/span&gt;...) repeats until a free bucket is found.&lt;/p&gt;

&lt;p&gt;This seems like a pretty obvious thing, but when you think about it it has some interesting properties for block-based on-disk structures, as we will discuss later.&lt;/p&gt;

&lt;h1 id=&#34;analysis&#34;&gt;Analysis&lt;/h1&gt;

&lt;p&gt;The analysis is pretty simple: If the hash functions are sufficiently good and independent, the space usage has a (amortized) linear upper-bound.&lt;/p&gt;

&lt;p&gt;The lookup speed is perhaps more important than the space, and it is in many ways similar to B+ trees, except that this is simpler and perhaps faster. Both have logarithmic complexity of lookups, and more importantly, the base of the logarithm is usually pretty high (although it depends on the choice of default table size).&lt;/p&gt;

&lt;h1 id=&#34;blockbased-ondisk-storage&#34;&gt;Block-based on-disk storage&lt;/h1&gt;

&lt;p&gt;Because block-based storage is so convenient, it is used in many database systems and file systems. It is one of the reasons B+ trees is such a popular indexing method.&lt;/p&gt;

&lt;p&gt;In particular, if a single block can store N pointers, we can simply let one table have N buckets, meaning there is no waste of space.&lt;/p&gt;

&lt;p&gt;Compared to B+ trees, there is a pretty clear advantage in the case of dynamically sized keys, since comparing requires loading, which is generally a very expensive task. Hashing on the other hand is easy, and never requires any comparison (with the exception of the last table, perhaps¹).&lt;/p&gt;

&lt;p&gt;¹it is popular to use cryptographic fingerprints to avoid the inconvenience of arbitrary-sized keys.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Portfolio of Logos and Icons</title>
      <link>http://ticki.github.io/blog/portfolio-of-logos-and-icons/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/portfolio-of-logos-and-icons/</guid>
      <description>&lt;p&gt;This is a collection of the logos and icons, I created (and used/not drafts) during the last years.&lt;/p&gt;

&lt;h1 id=&#34;opensea&#34;&gt;Open-Sea&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ticki/Open-Sea&#34;&gt;Open Sea&lt;/a&gt; is a dead game project I was working on:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;figure&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ticki/Open-Sea/master/data/graphics/logo_scaled.png&#34; alt=&#34;Open-Sea logo&#34;&gt;&lt;/figure&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Simple pixel art.&lt;/li&gt;
&lt;li&gt;Stylistic and minimalistic text.&lt;/li&gt;
&lt;li&gt;Simple icon similar to the graphics in the game.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;redox&#34;&gt;Redox&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/redox-os&#34;&gt;Redox&lt;/a&gt; is an operating-system I work on. The icon was one of the first things I contributed to Redox (before code):&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;figure&gt;&lt;img src=&#34;https://rawgit.com/redox-os/assets/master/vectorized_icon.svg&#34; alt=&#34;Redox vectorized icon&#34;&gt;&lt;/figure&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;It is a circle construction formed after an atom.&lt;/li&gt;
&lt;li&gt;The electrons are placed to resemble a sodium atom, which together with e.g. fluorine reacts in a redox reaction.&lt;/li&gt;
&lt;li&gt;It&#39;s SVG!&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;sodium&#34;&gt;Sodium&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/redox-os/sodium&#34;&gt;Sodium&lt;/a&gt; is a text editor I wrote. The logo is&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/redox-os/assets/master/Sodium_logo.png&#34; height=&#34;200&#34; width=&#34;200&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;It is based on a basic square with rounded edges (square construction).&lt;/li&gt;
&lt;li&gt;It uses text as a form of background noise.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;xenotime&#34;&gt;XENOTIME&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/neon-sector/xenotime&#34;&gt;XENOTIME&lt;/a&gt; is a game engine, which I created a logo for:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/neon-sector/xenotime/master/res/xenotime/logo.png&#34; height=&#34;200&#34; width=&#34;200&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;It is a text-based logo.&lt;/li&gt;
&lt;li&gt;It uses red-black colorscheme because it reflects the colorful game world.&lt;/li&gt;
&lt;li&gt;It preserves simplicity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;tfs&#34;&gt;TFS&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ticki/tfs&#34;&gt;TFS&lt;/a&gt; is a filesystem I&#39;m working on. The icon is:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;figure&gt;&lt;img src=&#34;https://rawgit.com/ticki/tfs/master/icon.svg&#34; alt=&#34;TFS icon&#34;&gt;&lt;/figure&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;It is a hexagone looking like an isometric projection of a cube, giving it a 3D feel.&lt;/li&gt;
&lt;li&gt;The tetragon gives it a &amp;quot;glassy&amp;quot; feel (looking modern).&lt;/li&gt;
&lt;li&gt;One of my favorite logo creations.&lt;/li&gt;
&lt;li&gt;Just 284 bytes, in total (optimized SVG). Compressed, it is around 100 bytes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;seahash&#34;&gt;SeaHash&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ticki/tfs/tree/master/seahash&#34;&gt;SeaHash&lt;/a&gt; is a hash function I designed:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ticki/tfs/master/seahash/logo.png&#34; height=&#34;200&#34; width=&#34;200&#34; /&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Very simple and not so fancy (showing stability and robustness).&lt;/li&gt;
&lt;li&gt;Waves symbolizing the sea and performance.&lt;/li&gt;
&lt;li&gt;Basic monospace font.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;termion&#34;&gt;Termion&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/ticki/termion&#34;&gt;Termion&lt;/a&gt; is a terminal manipulation library I wrote:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;a href=&#34;https://rawgit.com/ticki/termion/master/logo.svg&#34;&gt;&lt;figure&gt;&lt;img src=&#34;https://rawgit.com/ticki/termion/master/logo.svg&#34; alt=&#34;Termion logo&#34;&gt;&lt;/figure&gt;&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;It&#39;s animated, which is really cool. Note that in some browsers, the animation doesn&#39;t work in IMG tags, so if it is static, click it.&lt;/li&gt;
&lt;li&gt;The animation gives it an &amp;quot;interactive&amp;quot; feel.&lt;/li&gt;
&lt;li&gt;It uses common terminal colors and font.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;tip-of-the-iceberg&#34;&gt;Tip of the iceberg&lt;/h1&gt;

&lt;p&gt;These are just the ones I used. I&#39;ve created many other drafts and candidates, which I might show in another blog post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Eudex Algorithm</title>
      <link>http://ticki.github.io/blog/the-eudex-algorithm/</link>
      <pubDate>Sun, 11 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/the-eudex-algorithm/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;Half a year ago, I designed &lt;a href=&#34;https://github.com/ticki/eudex&#34;&gt;Eudex&lt;/a&gt; as a modern
replacement for Soundex, which is still widely used today. Eudex supports a
wide range of special-cases of European languages, while preserving the spirit
of simplicity, Soundex has.&lt;/p&gt;

&lt;p&gt;Both Eudex and Soundex are phonetic algorithms that produce a representation of
the sound of some string. Eudex is fundamentally different from Soundex in that
it is not a phonetic classifier. It is a phonetic locality-sensitive hash,
which means that two similarly-sounding strings are not mapped to the same
value, but instead to values near to each other.&lt;/p&gt;

&lt;p&gt;This technically makes it a string similarity index, but it one should be
careful with this term, given that it doesn&#39;t produce a typing distance, but a
phonetic/pronounciation distance.&lt;/p&gt;

&lt;p&gt;What this blog post aims to do is to describe the rationale behind Eudex,
hopefully sparking new ideas and thoughts for the reader.&lt;/p&gt;

&lt;h1 id=&#34;the-output-and-the-input&#34;&gt;The output and the input&lt;/h1&gt;

&lt;p&gt;So, the input is any Unicode string in a Latin-family alphabet.&lt;/p&gt;

&lt;p&gt;The output is fixed-width integer (we&#39;ll use 64-bit, but that is in some cases
a very narrow width), which has following characteristic:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If the string &lt;span  class=&#34;math&#34;&gt;\(a\)&lt;/span&gt; sounds similar to a string &lt;span  class=&#34;math&#34;&gt;\(b\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(f(a) \oplus f(b)\)&lt;/span&gt;
has low Hamming weight.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, two similarly sounding words maps to numbers with only a few
bits flipped, whereas words without similar sound maps to numbers with many
bits flipped.&lt;/p&gt;

&lt;h1 id=&#34;the-algorithm&#34;&gt;The algorithm&lt;/h1&gt;

&lt;p&gt;The algorithm itself is fairly simple. It outputs an 8 byte array (an unsigned
64 bit integer):&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\underbrace{A}_{\text{First phone}} \quad \underbrace{00}_{\text{Padding}} \quad \underbrace{BBBBB}_{\text{Trailing phones}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The crucial point here is that all the characters are mapped through a table
carefully derived by their phonetic classification, to make similar sounding
phones have a low Hamming distance.&lt;/p&gt;

&lt;p&gt;If two consecutive phones shares all the bits, but the parity bit, (i.e, &lt;span  class=&#34;math&#34;&gt;\(a \gg 1
= b \gg 1\)&lt;/span&gt;), the second is skipped. This allows us to &amp;quot;collapse&amp;quot; similar or
equal phones into one, kind of equivalence to the collapse stage of Soundex:
Similar phones next to each other can often be collapsed to one of the phones
without losing the pronounciation.&lt;/p&gt;

&lt;h1 id=&#34;deriving-the-tables&#34;&gt;Deriving the tables&lt;/h1&gt;

&lt;p&gt;The tables are what makes it interesting. There are four tables: one for ASCII
letters (not characters, letters) in the first slot (&#39;A&#39;), one for C1 (Latin
Supplement) characters in the first slot, one for ASCII letters in the trailing
phones, and one for the C1 (Latin Supplement) characters for the trailing
phones.&lt;/p&gt;

&lt;p&gt;There is a crucial distinction between consonants and vowels in Eudex. The
first phone treat vowels as first-class citizens by making distinctions between
all the properties of vowels. The trailing phones only have a distinction
between open and close vowels.&lt;/p&gt;

&lt;h2 id=&#34;trailing-character-table&#34;&gt;Trailing character table&lt;/h2&gt;

&lt;p&gt;Let&#39;s start with the tables for the trailing characters. Consonants&#39; bytes are
treated such that each bit represent a property of the phone (i.e.,
pronunciation) with the exception of the rightmost bit, which is used for
tagging duplicates (it acts as a discriminant).&lt;/p&gt;

&lt;p&gt;Let&#39;s look at the classification of IPA consonants:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/5/5e/IPA_consonants_2005.png&#34; alt=&#34;IPA&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;As you may notice, characters often represent more than one phone, and
reasoning about which one a given character in a given context represents can
be very hard. So we have to do our best in fitting each character into the
right phonetic category.&lt;/p&gt;

&lt;p&gt;We have to pick the classification intelligently. There are certain groups the
table doesn&#39;t contain, one of which turns out to be handy in a classification:
liquid consonants (lateral consonants + rhotics), namely &lt;code&gt;r&lt;/code&gt; and &lt;code&gt;l&lt;/code&gt;. Under
ideal conditions, these should be put into to distinct bits, but unfortunately
there are only 8 bits in a byte, so we have to limit ourselves.&lt;/p&gt;

&lt;p&gt;Now, every good phonetic hasher should be able to segregate important
characters (e.g., hard to mispell, crucial to the pronunciation of the word)
from the rest. Therefore we add a category we call &amp;quot;confident&amp;quot;, this will
occupy the most significant bit. In our category of &amp;quot;confident&amp;quot; characters we
put l, r, x, z, and q, since these are either:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Crucial to the sound of the word (and thus easier to hear, and harder to
misspell).&lt;/li&gt;
&lt;li&gt;Rare to occur, and thus statistically harder to mistake.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So our final trailing consonant table looks like:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Position&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Modifier&lt;/th&gt;
&lt;th&gt;Property&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Phones&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Discriminant&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;(for tagging duplicates)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Nasal&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;mn&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td&gt;Fricative&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;fvsjxzhct&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td&gt;Plosive&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;pbtdcgqk&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td&gt;Dental&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;tdnzs&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td&gt;Liquid&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;lr&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td&gt;Labial&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;bfpv&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128&lt;/td&gt;
&lt;td&gt;Confident¹&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;lrxzq&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The more &amp;quot;important&amp;quot; the characteristic is to the phone&#39;s sound the higher
place it has.&lt;/p&gt;

&lt;p&gt;We then have to treat the vowels. In particular, we don&#39;t care much of vowels
in trailing position, so we will simply divide them into two categories: open
and close. It is worth noting that not all vowels fall into these categories,
therefore we will simply place it in the category it is &amp;quot;nearest to&amp;quot;, e.g. a,
(e), o gets 0 for &amp;quot;open&amp;quot;.&lt;/p&gt;

&lt;p&gt;So our final ASCII letter table for the trailing phones looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                (for consonants)
      +--------- Confident
      |+-------- Labial
      ||+------- Liquid
      |||+------ Dental
      ||||+----- Plosive
      |||||+---- Fricative
      ||||||+--- Nasal
      |||||||+-- Discriminant
      ||||||||
   a* 00000000
   b  01001000
   c  00001100
   d  00011000
   e* 00000001
   f  01000100
   g  00001000
   h  00000100
   i* 00000001
   j  00000101
   k  00001001
   l  10100000
   m  00000010
   n  00010010
   o* 00000000
   p  01001001
   q  10101000
   r  10100001
   s  00010100
   t  00011101
   u* 00000001
   v  01000101
   w  00000000
   x  10000100
   y* 00000001
   z  10010100
             |  (for vowels)
             +-- Close
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we extend our table to C1 characters by the same method:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                (for consonants)
      +--------- Confident
      |+-------- Labial
      ||+------- Liquid
      |||+------ Dental
      ||||+----- Plosive
      |||||+---- Fricative
      ||||||+--- Nasal
      |||||||+-- Discriminant
      ||||||||
   ß  -----s-1  (use &#39;s&#39; from the table above with the last bit flipped)
   à  00000000
   á  00000000
   â  00000000
   ã  00000000
   ä  00000000  [æ]
   å  00000001  [oː]
   æ  00000000  [æ]
   ç  -----z-1  [t͡ʃ]
   è  00000001
   é  00000001
   ê  00000001
   ë  00000001
   ì  00000001
   í  00000001
   î  00000001
   ï  00000001
   ð  00010101  [ð̠]   (represented as a non-plosive T)
   ñ  00010111  [nj]  (represented as a combination of n and j)
   ò  00000000
   ó  00000000
   ô  00000000
   õ  00000000
   ö  00000001  [ø]
   ÷  11111111  (placeholder)
   ø  00000001  [ø]
   ù  00000001
   ú  00000001
   û  00000001
   ü  00000001
   ý  00000001
   þ  -----ð--  [ð̠]   (represented as a non-plosive T)
   ÿ  00000001
             |  (for vowels)
             +-- Close
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;first-phone-table&#34;&gt;First phone table&lt;/h2&gt;

&lt;p&gt;So far we have considered the trailing phones, now we need to look into the
first phone. The first phone needs a table with minimal collisions, since you
hardly ever misspell the first letter in the word. Ideally, the table should be
injective, but due to technical limitations it is not possible.&lt;/p&gt;

&lt;p&gt;We will use the first bit to distinguish between vowels and consonants.&lt;/p&gt;

&lt;p&gt;Previously we have only divided vowels into to classes, we will change that
now, but first: the consonants. To avoid repeating ourselves, we will use a
method for reusing the above tables.&lt;/p&gt;

&lt;p&gt;Since the least important property is placed to the left, we will simply shift
it to the right (that is, truncating the rightmost bit). The least significant
bit will then be flipped when encountering a duplicate. This way we preserve
the low Hamming distance, while avoiding collisions.&lt;/p&gt;

&lt;p&gt;The vowels are more interesting. We need a way to distinguish between vowels
and their sounds.&lt;/p&gt;

&lt;p&gt;Luckily, their classification is quite simple:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/en/5/5a/IPA_vowel_chart_2005.png&#34; alt=&#34;IPA&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;If a vowel appears as two phones (e.g., dependent on language), we OR them, and
possibly modify the discriminant if it collides with another phone.&lt;/p&gt;

&lt;p&gt;We need to divide each of the axises into more than two categories, to utilize
all our bits, so some properties will have to occupy multiple bits.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Position&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Modifier&lt;/th&gt;
&lt;th&gt;Property (vowel)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td&gt;Discriminant&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td&gt;Is it open-mid?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td&gt;Is it central?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td&gt;Is it close-mid?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td&gt;Is it front?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td&gt;Is it close?&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td&gt;More close than [ɜ]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128&lt;/td&gt;
&lt;td&gt;Vowel?&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;So we make use of both properties, namely both the openness and &amp;quot;frontness&amp;quot;.
Moreover, we allow more than just binary choices:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; Class     Close       Close-mid  Open-mid    Open
          +----------+----------+-----------+---------+
 Bits      .11.....    ...11...   ......1.   .00.0.0.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&#39;s do the same for the other axis:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; Class     Front       Central    Back
          +----------+----------+----------+
 Bits      ...1.0..    ...0.1..   ...0.0..
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To combine the properties we OR these tables. Applying this technique, we get:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;                (for vowels)
      +--------- Vowel
      |+-------- Closer than ɜ
      ||+------- Close
      |||+------ Front
      ||||+----- Close-mid
      |||||+---- Central
      ||||||+--- Open-mid
      |||||||+-- Discriminant
      ||||||||
   a* 10000100
   b  00100100
   c  00000110
   d  00001100
   e* 11011000
   f  00100010
   g  00000100
   h  00000010
   i* 11111000
   j  00000011
   k  00000101
   l  01010000
   m  00000001
   n  00001001
   o* 10010100
   p  00100101
   q  01010100
   r  01010001
   s  00001010
   t  00001110
   u* 11100000
   v  00100011
   w  00000000
   x  01000010
   y* 11100100
   z  01001010
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then extend it to C1 characters:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;      +--------- Vowel?
      |+-------- Closer than ɜ
      ||+------- Close
      |||+------ Front
      ||||+----- Close-mid
      |||||+---- Central
      ||||||+--- Open-mid
      |||||||+-- Discriminant
      ||||||||
   ß  -----s-1 (use &#39;s&#39; from the table above with the last bit flipped)
   à  -----a-1
   á  -----a-1
   â  10000000
   ã  10000110
   ä  10100110  [æ]
   å  11000010  [oː]
   æ  10100111  [æ]
   ç  01010100  [t͡ʃ]
   è  -----e-1
   é  -----e-1
   ê  -----e-1
   ë  11000110
   ì  -----i-1
   í  -----i-1
   î  -----i-1
   ï  -----i-1
   ð  00001011  [ð̠]   (represented as a non-plosive T)
   ñ  00001011  [nj]  (represented as a combination of n and j)
   ò  -----o-1
   ó  -----o-1
   ô  -----o-1
   õ  -----o-1
   ö  11011100  [ø]
   ÷  11111111  (placeholder)
   ø  11011101  [ø]
   ù  -----u-1
   ú  -----u-1
   û  -----u-1
   ü  -----y-1
   ý  -----y-1
   þ  -----ð--  [ð̠]   (represented as a non-plosive T)
   ÿ  -----y-1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;distance-operator&#34;&gt;Distance operator&lt;/h1&gt;

&lt;p&gt;Now that we have our tables. We now need the distance operator. A naïve
approach would be to simply use Hamming distance. This has the disadvantage of
all the bytes having the same weight, which isn&#39;t ideal, since you are more
likely to misspell later characters, than the first ones.&lt;/p&gt;

&lt;p&gt;For this reason, we use weighted Hamming distance:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Byte:&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;4&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;5&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;6&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;7&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;8&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Weight:&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;128&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Namely, we XOR the two values and then add each of the bytes&#39; Hamming weight,
using the coefficients from the table above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SeaHash: Explained</title>
      <link>http://ticki.github.io/blog/seahash-explained/</link>
      <pubDate>Thu, 08 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/seahash-explained/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;So, not so long ago, I designed &lt;a href=&#34;https://github.com/ticki/tfs/tree/master/seahash&#34;&gt;SeaHash&lt;/a&gt;, an alternative hash algorithm with performance better than most (all?) of the existing non-cryptographic hash functions available. I designed it for checksumming for a file system, I&#39;m working on, but I quickly found out it was sufficient for general hashing.&lt;/p&gt;

&lt;p&gt;It blew up. I got a lot of cool feedback, and yesterday it was picked as &lt;a href=&#34;https://this-week-in-rust.org/blog/2016/12/06/this-week-in-rust-159/&#34;&gt;crate of the week&lt;/a&gt;. It shows that there is some interest in it, so I want to explain the ideas behind it.&lt;/p&gt;

&lt;h1 id=&#34;hashing-an-introduction&#34;&gt;Hashing: an introduction&lt;/h1&gt;

&lt;p&gt;The basic idea of hashing is to map data with patterns in it to pseudorandom values. It should be designed such that only few collisions happen.&lt;/p&gt;

&lt;p&gt;Simply put, the hash function should behave like a pseudorandom function (PRF) producing a seemingly random stream from a non-random stream. It is similar to pseudorandom functions, in a sense, with difference that they must take variable input.&lt;/p&gt;

&lt;p&gt;Formally, perfect PRFs are defined as follows:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(f : \{0, 1\}^n \to \{0, 1\}^n\)&lt;/span&gt; is a perfect PRF if and only if given a distribution &lt;span  class=&#34;math&#34;&gt;\(d : \{0, 1\}^n \to \left[0,1\right]\)&lt;/span&gt;, &lt;span  class=&#34;math&#34;&gt;\(f\)&lt;/span&gt; maps inputs following the distribution &lt;span  class=&#34;math&#34;&gt;\(d\)&lt;/span&gt; to the uniform distribution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that there is a major difference between cryptographic and non-cryptographic hash functions. SeaHash is not cryptographic, and that&#39;s very important to understand: It doesn&#39;t aim to be. It aims to give a good hash quality, but not cryptographic guarentees.&lt;/p&gt;

&lt;h1 id=&#34;constructing-a-hash-function-from-a-prf&#34;&gt;Constructing a hash function from a PRF&lt;/h1&gt;

&lt;p&gt;There are various ways to construct a variable-length hash function from a PRF. The most famous one is Merkle–Damgård construction. We will focus on this.&lt;/p&gt;

&lt;p&gt;There are multiple ways to do Merkle–Damgård construction. The most famous one is the wide-pipe construction. It works by having a state, which combined with one block of the input data at a time. The final state will then be the hash value. This combining function is called a &amp;quot;compression function&amp;quot;. It takes two blocks of same length and maps it to one: &lt;span  class=&#34;math&#34;&gt;\(f : \{0, 1\}^n \times \{0, 1\}^n \to \{0, 1\}^n\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[h = f(f(f(\ldots, b_0), b_1), b_2)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It is important that this compression emits pseudorandom behavior, and that&#39;s where PRFs comes in. For general-purpose hash function where we don&#39;t care about security, the construction usually looks like this:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(a, b) = p(a \oplus b)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This of course is commutative, but that doesn&#39;t matter, because we don&#39;t need non-commutativity in the Merkle–Damgård construction.&lt;/p&gt;

&lt;h1 id=&#34;choosing-a-prf&#34;&gt;Choosing a PRF&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&#34;http://www.pcg-random.org/&#34;&gt;PCG family of PRFs&lt;/a&gt; is my favorite PRF I&#39;ve seen so far:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;\gets p \otimes x \\
x &amp;\gets x \oplus ((x \gg 32) \gg (x \gg 60)) \\
x &amp;\gets p \otimes x
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(&lt;span  class=&#34;math&#34;&gt;\(\otimes\)&lt;/span&gt; means modular multiplication)&lt;/p&gt;

&lt;p&gt;The PCG paper goes into depth on why this. In particular, it is a quite uncommon to use these kinds of non-fixed shifts.&lt;/p&gt;

&lt;p&gt;This is a bijective function, which means that we can&#39;t ever have less entropy than the input, which is a good property to have in a hash function.&lt;/p&gt;

&lt;h1 id=&#34;parallelism&#34;&gt;Parallelism&lt;/h1&gt;

&lt;p&gt;This construction of course relies on dependencies between the states, rendering it impossible to parallelize.&lt;/p&gt;

&lt;p&gt;We need a way to be able to independently calculate multiple block updates. With a single state, this is simply not possible, but fear not, we can add multiple states.&lt;/p&gt;

&lt;p&gt;Instruction-level parallelism means that we don&#39;t even need to fire up multiple threads (which would be quite expensive), but simply exploit CPU&#39;s instruction pipelines.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/seahash_state_update_diagram.svg&#34; alt=&#34;A diagram of the new design.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;In the above diagram, you can see a 4-state design, where every state except the first is shifted down. The first state (&lt;span  class=&#34;math&#34;&gt;\(a\)&lt;/span&gt;) gets the last one (&lt;span  class=&#34;math&#34;&gt;\(d\)&lt;/span&gt;) after being combined with the input block (&lt;span  class=&#34;math&#34;&gt;\(D\)&lt;/span&gt;) through our PRF:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
a&#39; &amp;= b \\
b&#39; &amp;= c \\
c&#39; &amp;= d \\
d&#39; &amp;= f(a \oplus D) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It isn&#39;t obvious how this design allows parallelism, but it has to do with the fact that you can unroll the loop, such that the shifts aren&#39;t needed. In particular, after 4 rounds, everything is back at where it started:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
a &amp;\gets f(a \oplus B_1) \\
b &amp;\gets f(b \oplus B_2) \\
c &amp;\gets f(c \oplus B_3) \\
d &amp;\gets f(d \oplus B_4) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;If we take 4 rounds every iteration, we get 4 independent state updates, which are run in parallel.&lt;/p&gt;

&lt;p&gt;This is also called an &lt;em&gt;alternating 4-state Merkle–Damgård construction&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;finalizing-the-four-states&#34;&gt;Finalizing the four states&lt;/h2&gt;

&lt;p&gt;Naively, we would just XOR the four states (which have difference initialization vectors, and hence would not commute).&lt;/p&gt;

&lt;p&gt;There are some issues: What if the input doesn&#39;t divide our 4 blocks? Well, the simple solution is of course padding, but that gives us another issue: how do we distinguish between padding and normal zeros?&lt;/p&gt;

&lt;p&gt;We XOR the length with the hash value. Unfortunately, this is obviously not discrete, since appending another zero would then only affect the value slightly, so we need to run it through our PRF:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/seahash_finalization_diagram.svg&#34; alt=&#34;SeaHash finalization&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;One concern I&#39;ve heard is that XOR is commutative, and hence permuting the states wouldn&#39;t affect the output. But that&#39;s simply not true: Each state has distinct initial value, making each lane hash differently.&lt;/p&gt;

&lt;h1 id=&#34;putting-it-all-together&#34;&gt;Putting it all together&lt;/h1&gt;

&lt;p&gt;We can finally put it all together:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ticki.github.io/img/seahash_construction_diagram.svg&#34;&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/seahash_construction_diagram.svg&#34; alt=&#34;SeaHash construction&#34;&gt;&lt;/figure&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(click to zoom)&lt;/p&gt;

&lt;p&gt;You can see the code and benchmarks &lt;a href=&#34;https://github.com/ticki/tfs/tree/master/seahash&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Trump Effect</title>
      <link>http://ticki.github.io/blog/the-trump-effect/</link>
      <pubDate>Wed, 16 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/the-trump-effect/</guid>
      <description>

&lt;p&gt;So, many have noted the disparity in standards set for the candidates. Indeed, Trump has said and done certain things which would end any other campaign, yet it has had relatively small effect on Trump&amp;rsquo;s campaign.&lt;/p&gt;

&lt;p&gt;This is my take on why.&lt;/p&gt;

&lt;h1 id=&#34;defining-the-trump-effect&#34;&gt;Defining the Trump effect&lt;/h1&gt;

&lt;p&gt;Here&amp;rsquo;s how I would define the Trump effect:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Trump effect is the phenomenon in which a group or individual lowers the standard such that diverging or abnormal behavior does not hurt one&amp;rsquo;s reputation to the same extent as other groups or individuals acting similarly.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Essentially humans judge an individual based on their expected behavior, and if poor behavior is expected, it is naturally to assume they&amp;rsquo;re behaving well if they beat their expectation.&lt;/p&gt;

&lt;p&gt;An analogy is children: No one expects a child to behave like an adult, but if they do, they have overperformed their expected behavior.&lt;/p&gt;

&lt;h1 id=&#34;trump-s-campaign&#34;&gt;Trump&amp;rsquo;s campaign&lt;/h1&gt;

&lt;p&gt;When Trump announced his campaign, it was declared a joke. Joke candidates are expected to behave so, and thus it is no surprise when they do.&lt;/p&gt;

&lt;p&gt;Trump has consistently behaved unpresidential, but the damage done by such behavior was relatively limited.&lt;/p&gt;

&lt;p&gt;As an example, take Trump mocking a disabled reporter:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://ticki.github.io/thumb/the-trump-effect.png&#34; alt=&#34;Image of Trump&#39;s imitation of the reporter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In any other campaign, this would have severely damaged it, or even ended it. With Trump, it didn&amp;rsquo;t have this effect, at all.&lt;/p&gt;

&lt;p&gt;The second example is his comments on a woman accusing him of rape:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Take a look. You look at her, look at her words. You tell me what you think. I don’t think so. I don’t think so.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is quite clear what he tries to say here: The woman in question apparently wasn&amp;rsquo;t attractive enough for him to rape.&lt;/p&gt;

&lt;p&gt;These comments are absolutely terrifying, but did they end his campaign? No, they didn&amp;rsquo;t.&lt;/p&gt;

&lt;h2 id=&#34;the-myth-of-nature-defying-trump&#34;&gt;The myth of nature defying Trump&lt;/h2&gt;

&lt;p&gt;To be clear, they did hurt him, but they didn&amp;rsquo;t nearly do it as strongly as it would for any other campaign.&lt;/p&gt;

&lt;p&gt;Nate Silver did a quite interesting article on this &lt;a href=&#34;https://fivethirtyeight.com/features/trump-isnt-teflon/&#34;&gt;&amp;ldquo;Trump isn&amp;rsquo;t teflon&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Trump&amp;rsquo;s action tends to hurt him, and not the opposite, as some have claimed. But there is a major difference in the degree to which the news affects the campaign.&lt;/p&gt;

&lt;h2 id=&#34;overperforming-the-expectations&#34;&gt;Overperforming the expectations&lt;/h2&gt;

&lt;p&gt;The week when &lt;a href=&#34;http://edition.cnn.com/videos/politics/2016/08/31/donald-trump-in-mexico-murray-dnt-tsr.cnn&#34;&gt;Trump visited Mexico&lt;/a&gt;, he performed exceptionally well in the polls, even beating Clinton.&lt;/p&gt;

&lt;p&gt;But why?&lt;/p&gt;

&lt;p&gt;He didn&amp;rsquo;t did the exceptional thing of acting presidential and normal. Overperforming the expectations makes people greatly exaggerate the valuation.&lt;/p&gt;

&lt;p&gt;Hillary Clinton was expected to behave presidentially, Trump isn&amp;rsquo;t. As such, even a single week of acting normally results in a massive upswing in the polls.&lt;/p&gt;

&lt;h2 id=&#34;compare-to-howard-dean&#34;&gt;Compare to Howard Dean&lt;/h2&gt;

&lt;p&gt;Howard Dean ran in 2004. For comparison, his campaign was declared ended after his infamous awkward speech &lt;a href=&#34;https://www.youtube.com/watch?v=l6i-gYRAwM0&#34;&gt;&amp;ldquo;Dean&amp;rsquo;s scream speech&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The fact that this over-enthusiastic speech could end his campaign is insane when you look it in comparison to the Trump candidacy.&lt;/p&gt;

&lt;h1 id=&#34;lowering-the-standard&#34;&gt;Lowering the standard&lt;/h1&gt;

&lt;p&gt;The question is just: how is the low standard achieved?&lt;/p&gt;

&lt;p&gt;One could say that all that was need is to consistently act poorly, but I doubt this is enough.&lt;/p&gt;

&lt;p&gt;If one consistently acts poorly and then apologizes for it, they don&amp;rsquo;t etablish a low standard, because they admit they were underperforming themself.&lt;/p&gt;

&lt;p&gt;Compare Governor of Maine, Paul LePage, to president-elect, Donald Trump. LePage consistently acts poorly, like Trump. But he is still widely unpopular (in fact, very much so, he has historically low approval ratings), and he is still put to a high standard.&lt;/p&gt;

&lt;p&gt;Why?&lt;/p&gt;

&lt;p&gt;LePage apologizes for his actions, and I think this is where they differ. Trump never apologizes for his actions, as such he never admits actually being wrong and thus pushing up the standards again (&amp;ldquo;I didn&amp;rsquo;t mean to do this&amp;rdquo;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ternary as a prediction residue code</title>
      <link>http://ticki.github.io/blog/ternary-as-a-prediction-residue-code/</link>
      <pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/ternary-as-a-prediction-residue-code/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;If we look at how most lossless image compression formats works, they don&#39;t use deduplication compression (like LZ-class algorithms), because that&#39;s simply far from the nature of images. The same goes for audio and video. Instead, you have two maps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The approximative map (&lt;span  class=&#34;math&#34;&gt;\(a(\vec{v})\)&lt;/span&gt;): This should give a rough outline of the medium, that is, it is should predict predict the medium based on a small sequence of bytes (defined by the encoding). This could be a polynomial, linear map, bilinear map, Bézier curve, Fourier decomposition, or any class of functions which can be represented compactly.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The prediction residue (&lt;span  class=&#34;math&#34;&gt;\(p(\vec{v})\)&lt;/span&gt;): This map &amp;quot;corrects&amp;quot; &lt;span  class=&#34;math&#34;&gt;\(a(\vec{v})\)&lt;/span&gt; such that the combination of the two maps gives an exact result.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;sup&gt;Note that this is massively simplified, and a lot of additional steps are done in popular media compression algorithms.&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The final function is then defined as&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(\vec{v}) = a(\vec{v}) + k p(\vec{v})\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The question just remains: how can we efficiently represent the prediction residue? Well, we know a crucial thing. &lt;span  class=&#34;math&#34;&gt;\(p(\vec{v})\)&lt;/span&gt; tends to be small, because much of it is accounted for in &lt;span  class=&#34;math&#34;&gt;\(a(\vec{v})\)&lt;/span&gt;, so here&#39;s where coding theory comes in.&lt;/p&gt;

&lt;p&gt;In fact, we can reconsider the problem in terms of coding:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Create a prefix code such that lower bytes have smaller representation than higher bytes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is then natural to ask:&lt;/p&gt;

&lt;h1 id=&#34;why-not-just-stick-with-a-huffman-tree&#34;&gt;Why not just stick with a Huffman tree?&lt;/h1&gt;

&lt;p&gt;Well, we could, but because we often need live encoding and decoding, we need something faster. Huffman trees are slow because they involve cache misses and forward scans.&lt;/p&gt;

&lt;p&gt;The gain of using Huffman trees in this case is really small, because the basic distribution is known in advance.&lt;/p&gt;

&lt;p&gt;We want a code defined by a small set of rules which can be encoded and decoded with extreme performance, so we want to avoid Huffman codes for these reasons.&lt;/p&gt;

&lt;h1 id=&#34;candidate-rice-coding&#34;&gt;Candidate: Rice coding&lt;/h1&gt;

&lt;p&gt;Rice coding is a special case of Golomb coding. It is often used in Audio codecs, where it works fine.&lt;/p&gt;

&lt;p&gt;Here&#39;s how it works:&lt;/p&gt;
&lt;figure role=&#34;group&#34;&gt;
&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;p&gt;To encode a byte &lt;span  class=&#34;math&#34;&gt;\(b\)&lt;/span&gt;, do the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Write &lt;span  class=&#34;math&#34;&gt;\(b \ll n\)&lt;/span&gt; 1 bits.&lt;/li&gt;
&lt;li&gt;Write a 0 bit.&lt;/li&gt;
&lt;li&gt;Write &lt;span  class=&#34;math&#34;&gt;\(b % n\)&lt;/span&gt; in base two (&lt;span  class=&#34;math&#34;&gt;\(b\)&lt;/span&gt; bits).&lt;/li&gt;
&lt;/ol&gt;
&lt;/figure&gt;

&lt;p&gt;But it has multiple shortfalls:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Often there is no fitting constants, because of the trade-off. You need to choose between low minimum and high maximum or high minimum and low maximum. There&#39;s little middle-ground.&lt;/li&gt;
&lt;li&gt;It is slow to encode and decode, because reading new bits requires you to branch to see if a new byte is needed (in particular, the atomic units are not aligned with the byte).&lt;/li&gt;
&lt;li&gt;It encodes arbitrarily high numbers, meaning that some redundancy exists when coding bytes-only.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is used in certain MPEG variants as well as FLAC and lossless JPEG.&lt;/p&gt;

&lt;h1 id=&#34;an-alternative-approach-base-3-in-base-4&#34;&gt;An alternative approach: Base 3 in base 4.&lt;/h1&gt;

&lt;p&gt;So, what if we encode everything in base 3?&lt;/p&gt;

&lt;p&gt;First of all, why would we do that?&lt;/p&gt;

&lt;p&gt;Well, for it to be a prefix code, you need a way to determine if the token is over, i.e. a terminator, so you really need 4 different symbols (0, 1, 2, T). Enumerating these symbols is trivial to do in binary: It&#39;s just two bits!&lt;/p&gt;

&lt;p&gt;This means that we positively know that every byte contains exactly 4 symbols, greatly improving decoding/encoding performance.&lt;/p&gt;

&lt;h1 id=&#34;eliminating-redundancy&#34;&gt;Eliminating redundancy&lt;/h1&gt;

&lt;p&gt;There is a redundancy in this system, though. It is not a bijection. &lt;code&gt;01T&lt;/code&gt; and &lt;code&gt;1T&lt;/code&gt; gives the same (1), but a simple trick can eliminate this.&lt;/p&gt;

&lt;p&gt;In particular, we can say that if the two first bits in the token are 0 (which is redundant), the final value produced is 0, but then &lt;code&gt;00T&lt;/code&gt; would be equivalent to &lt;code&gt;0&lt;/code&gt;. Fortunately, this redundancy can easily be solved by adding 1 to the decoded result, and hence making 0 impossible unless the first symbol is &lt;code&gt;0&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;overflows&#34;&gt;Overflows&lt;/h2&gt;

&lt;p&gt;Since we cannot encode above 255, there is a class of trap values, where it overflows. Fortunately, there is a nice solution to it: just skip the terminator and go to the next token.&lt;/p&gt;

&lt;h1 id=&#34;table&#34;&gt;Table&lt;/h1&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;N&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Ternary representation&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Binary representation&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Reduction (of original size)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;00&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;11&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;1T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;0111&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;2T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;1011&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;10T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;010011&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;11T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;010111&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;12T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;011011&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;20T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;100011&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;21T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;100111&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;22T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;101011&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;75%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;100T&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;10000011&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;...&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;...&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;...&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;255&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;100110&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;010000010100&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As you may notice, ten is the splitting point where it goes from less than 100% to 100%, which means that if there is a significant number of bytes above or equals to 10, it expands.&lt;/p&gt;

&lt;h1 id=&#34;moving-the-tipping-point&#34;&gt;Moving the tipping point&lt;/h1&gt;

&lt;p&gt;10 is a bit too low for many purposes, so it is natural to ask if we can modify the code to expand it. In fact, a technique similar to Rice coding can be used: Encode the &lt;span  class=&#34;math&#34;&gt;\(k\)&lt;/span&gt; last bits seperately than the first ones.&lt;/p&gt;

&lt;p&gt;In particular, we split the byte into two and then encode the first part through the described technique, and the second part through usual little-endian encoding.&lt;/p&gt;

&lt;p&gt;Put &lt;span  class=&#34;math&#34;&gt;\(k=2\)&lt;/span&gt;, and the tipping point moved to 40. &lt;span  class=&#34;math&#34;&gt;\(k=4\)&lt;/span&gt;, and the tipping point is 160.&lt;/p&gt;

&lt;h1 id=&#34;usecases&#34;&gt;Usecases&lt;/h1&gt;

&lt;p&gt;Well, really, it depends on the nature of the data you compress. The best way to find out if it fits is to experiment with it.&lt;/p&gt;

&lt;p&gt;Rice code tends to be better if a lot of the bytes are low and outliers are relatively rare, but when there are outliers, it tends to hurt in terms of efficiency.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Assuming you approximative function is precise enough, this type coding would give a fairly good ratio. If it is imprecise or doesn&#39;t follow the right distribution, you might end up expanding the size.&lt;/p&gt;

&lt;p&gt;In the end, it depends on what you&#39;re compressing, and you should consider all the various possiblities. Often Rice coding is fitting, but sometimes more complex codes (like the one presented here) are needed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What we learned from the election results</title>
      <link>http://ticki.github.io/blog/what-we-have-learned-from-the-election-results/</link>
      <pubDate>Wed, 09 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/what-we-have-learned-from-the-election-results/</guid>
      <description>

&lt;p&gt;It was a surprising and intense outcome to watch yesterday night. As you probably know by now, Trump won, contrary to all predictions and calls.&lt;/p&gt;

&lt;p&gt;But why?&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not an US citzen, but I have followed the race with great passion, and I want to give my outsider two-cents on &amp;lsquo;what the hell is going&amp;rsquo; in United States of America.&lt;/p&gt;

&lt;p&gt;So, what did we actually learn?&lt;/p&gt;

&lt;h1 id=&#34;1-usa-is-a-post-fact-society&#34;&gt;1. USA is a post-fact society&lt;/h1&gt;

&lt;p&gt;Many of the things Trump have said are lies, and it is often trivial to prove so. Problem is just, people seem not to care.&lt;/p&gt;

&lt;p&gt;He isn&amp;rsquo;t a serial liar. He&amp;rsquo;s a bullshit artist. There is a very subtle distinction between the two. Lies are when the messenger knows the truth but intentionally tries to mislead the receiver and conceal the truth. Bullshitting is when the messenger are not necessarily trying to conceal the truth, but rather giving misinformation to advance the messenger&amp;rsquo;s personal agenda.&lt;/p&gt;

&lt;p&gt;Philosopher Harry Frankfurt wrote a great piece, &lt;a href=&#34;https://en.wikipedia.org/wiki/On_Bullshit&#34;&gt;&amp;ldquo;On Bullshit&amp;rdquo;&lt;/a&gt;, which analyze the structure of bullshit and how it is different from lying. It&amp;rsquo;s a beautiful, relatively short text, and I encourage the reader to go read it.&lt;/p&gt;

&lt;p&gt;When Trump says that people were cheering on &lt;sup&gt;9&lt;/sup&gt;&amp;frasl;&lt;sub&gt;11&lt;/sub&gt;, or when he claims that unvetted refugees are pouring into USA, or when he claims he is popular among African-American voters, people surprisingly don&amp;rsquo;t seem to care. It doesn&amp;rsquo;t matter if what he says is true or not. If it feels right, it is considered true.&lt;/p&gt;

&lt;p&gt;The wonderful term &amp;lsquo;Truthiness&amp;rsquo;, describing exactly this phenomena, was coined in 2005 by comedian, Stephen Colbert. It seems that people just believe what they want to. An interesting, but also exhausting, exercise is to go to Twitter and find some Trump supporter with a conspiracy theory, then call it out. You will notice how they just wave away from their original claim, rather than defend it. They know it&amp;rsquo;s false, and so do I, but they won&amp;rsquo;t accept it subconsciously.&lt;/p&gt;

&lt;p&gt;The same goes on for Trump: he says whatever pleases people. According to PolitiFact, he is amongst the most lying politicians.&lt;/p&gt;

&lt;p&gt;Interestingly, not all the bullshit he tells is false: it is often just vague and empty. If we take his plan to defeat Daesh, you will notice, well, that there is no plan at all, yet he is able to sell it. No falsehoods are really told in this case, it&amp;rsquo;s just vague, but people seem to embrace it.&lt;/p&gt;

&lt;h1 id=&#34;2-authoritarianism-fascism-is-easier-to-sell-than-previously-thought&#34;&gt;2. Authoritarianism (fascism?) is easier to sell than previously thought&lt;/h1&gt;

&lt;p&gt;United States is largely a liberal democracy, but if we assume his words are true, it will not be so much longer.&lt;/p&gt;

&lt;p&gt;His seemingly anti-democratic and, honestly, borderline fascist policies are very worrying, especially in the biggest democracy in the world.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m interesting in seeing if he will actually implement the things he has been flirting with. I suspect he will at least try to do some of these things. Although, I am doubtful that it will succeed.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m not in doubt of one thing though: Fascism is on the rise, both in America and in Europe. It is seen as the wave of alternative far-right politicians around the world. It is a dangerous trend, and does much more damage than conventional fanatic right-wing politicians.&lt;/p&gt;

&lt;h1 id=&#34;3-the-media-and-voters-have-the-attention-span-of-a-dead-fish&#34;&gt;3. The media and voters have the attention span of a dead fish&lt;/h1&gt;

&lt;p&gt;Every other day in the election cycle, there have been another major scandal on Trump, but the only thing that have had major impact was the Billy Bush/Trump hotmic leak, and even that died after a few news cycles.&lt;/p&gt;

&lt;p&gt;Why?&lt;/p&gt;

&lt;p&gt;Both media and voters have attention span of a dead fish.&lt;/p&gt;

&lt;p&gt;Because there are constantly new scandals, they have not had the same effect as they would in a vacuum, because they tend to bury each other.&lt;/p&gt;

&lt;p&gt;Hillary Clinton has at most 4-6 major scandals, which are significantly less than Trump&amp;rsquo;s, yet they seem to have the same effect.&lt;/p&gt;

&lt;p&gt;Clinton&amp;rsquo;s scandals are definitely noteworthy, but they have been repeated till the point where I get sick. Because no new scandals appear (only new information about old ones are revealed), they have a stronger effect than Trump&amp;rsquo;s. Repeat it enough, and it becomes true.&lt;/p&gt;

&lt;h1 id=&#34;4-the-gop-has-no-nobility&#34;&gt;4. The GOP has no nobility&lt;/h1&gt;

&lt;p&gt;I hate the GOP and everything it stands for, but I have never questioned that they had strong values they&amp;rsquo;d defend for any price, until now. No party have sold out as much as the GOP.&lt;/p&gt;

&lt;p&gt;Trump stands for nothing, but the GOP&amp;rsquo;s platform, yet they (or at least the majority of them) accept their nominee, often while not standing behind him fully.&lt;/p&gt;

&lt;p&gt;It seems that they value the label &amp;lsquo;republican&amp;rsquo; over what the candidate stands for. The GOP is fully guilty them self for not showing any form of integrity.&lt;/p&gt;

&lt;h1 id=&#34;5-racism-is-not-over-yet&#34;&gt;5. Racism is not over yet&lt;/h1&gt;

&lt;p&gt;A part of the disease that Trump has taken advantage of is racism. A central part of his platform is based around racist policies, and he has taken advantage of openly racist groups, like the KKK, who have been strong supporters of Trump.&lt;/p&gt;

&lt;p&gt;It seems that the average voter is largely ignoring the problem of racial inequality, either pretending it doesn&amp;rsquo;t exist or simply accepting that it exist, but refusing to make any actions to solve it.&lt;/p&gt;

&lt;p&gt;Because it doesn&amp;rsquo;t affect them, they simply have no reason to care, but the divide is still there, and Trump has no plans to solve it, rather he has policies which have the opposite effect (such as more police and stronger punishment for non-violent offenses).&lt;/p&gt;

&lt;h1 id=&#34;6-sexism-is-not-over-yet&#34;&gt;6. Sexism is not over yet&lt;/h1&gt;

&lt;p&gt;Hillary Clinton has been haunted her whole carrer for being too independent and not feminine enough, and this cycle has merely been a continuation of that.&lt;/p&gt;

&lt;p&gt;There is an obvious bias against women in many aspects of the election. Women are portrayed as feminine, which carries the expectations of being motherly, soft, and everything she is not.&lt;/p&gt;

&lt;p&gt;A lot of the things that have hurt Hillary Clinton have not been caused by her sex, but been greatly accelerated due to her sex.&lt;/p&gt;

&lt;h1 id=&#34;7-things-are-harder-to-predict-than-thought&#34;&gt;7. Things are harder to predict than thought&lt;/h1&gt;

&lt;p&gt;This is a major things learned. I personally was way too overconfident in a strong Clinton victory, but I did so based on the polls, which rather interestingly turned out to be wrong.&lt;/p&gt;

&lt;p&gt;The polls were pretty clear, and if the election was held a week ago Clinton would have won (the margin would be so big between them than even a 4 times MOE polling error would still have Clinton win).&lt;/p&gt;

&lt;p&gt;But things changed quickly, and Comey is largely to blame for the Trump victory. His letter had a quite strong effect on the polls, an influence so strong that it couldn&amp;rsquo;t be recovered on 48 hours.&lt;/p&gt;

&lt;p&gt;I think Comey&amp;rsquo;s actions were extremely irresponsible, but it&amp;rsquo;s not something we can change. What is really the lesson here is that things can change quickly, especially with the media completely overblowing things with false headlines. The numbers dropped from D+10 to D+2, so a difference so high that no one would have expected it to happen.&lt;/p&gt;

&lt;p&gt;The question to remain really is why did D+2 result in a republican victory? While the republicans would not win in a proportional system, the polls still underestimated Trump&amp;rsquo;s supporters. A week ago I laughed of the idea of a &amp;lsquo;silent majority&amp;rsquo;, but there might actually be something to it. While not a majority, some segment of the voters have likely been silent about the support of Trump.&lt;/p&gt;

&lt;h1 id=&#34;8-the-electoral-college-is-deeply-flawed&#34;&gt;8. The electoral college is deeply flawed&lt;/h1&gt;

&lt;p&gt;Suuuuuprise! I guess you didn&amp;rsquo;t knew that already.&lt;/p&gt;

&lt;p&gt;Hillary Clinton is at 3-4% over Trump in the popular vote, yet he won by a big margin in the electoral votes. The system isn&amp;rsquo;t rigged, it&amp;rsquo;s flawed.&lt;/p&gt;

&lt;p&gt;The majority is ultimately who should make the decision. With electoral college, that isn&amp;rsquo;t the case. Your voice was determined by a small number of tipping-point states. It&amp;rsquo;s not fair, and it isn&amp;rsquo;t the first time, we&amp;rsquo;ve seen scenarios, where the popular and electoral majority doesn&amp;rsquo;t match.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d go so far and say that literally any system is better than the electoral college. It is a failed system, which favors only two major parties, and voting anything outside those is equivalent to throwing out your vote.&lt;/p&gt;

&lt;p&gt;A better alternative is range voting, which I might do a post on later.&lt;/p&gt;

&lt;h1 id=&#34;9-trump-is-untouchable&#34;&gt;9. Trump is untouchable&lt;/h1&gt;

&lt;p&gt;If you ask a Trump supporter about some particular controversial statement Trump has made, they will often admit that they disagree, but you often hear the same line of response:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;He isn&amp;rsquo;t a polished politician.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;He&amp;rsquo;s an outsider&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Matter of the fact is, you don&amp;rsquo;t count it into your total judgement if every single thing can be waved away with incoherent, effective responses as the ones above.&lt;/p&gt;

&lt;p&gt;Trump can say anything, and the damage done to him is often smaller than the equivalent damage done to other people for saying similar things (however, it is a myth that Trump is completely unaffected by his controversies).&lt;/p&gt;

&lt;p&gt;Trump is above the rules of common sense, which &amp;ndash; in part &amp;ndash; is why he won the election. The bar is set so low that virtually no effort has to be done to overperform the expectations.&lt;/p&gt;

&lt;p&gt;In the week where Trump visited Mexico, Trump gained massively, simply by acting normally. The standards of acting &amp;lsquo;presidential&amp;rsquo; simply don&amp;rsquo;t apply to him. He is above the rules of the game.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Designing a good non-cryptographic hash function</title>
      <link>http://ticki.github.io/blog/designing-a-good-non-cryptographic-hash-function/</link>
      <pubDate>Fri, 04 Nov 2016 16:28:44 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/designing-a-good-non-cryptographic-hash-function/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;So, I&#39;ve been needing a hash function for various purposes, lately. None of the existing hash functions I could find were sufficient for my needs, so I went and designed my own. These are my notes on the design of hash functions.&lt;/p&gt;

&lt;h1 id=&#34;what-is-a-hash-function-really&#34;&gt;What is a hash function &lt;em&gt;really&lt;/em&gt;?&lt;/h1&gt;

&lt;p&gt;Hash functions are functions which maps a infinite domain to a finite codomain. Two elements in the domain, &lt;span  class=&#34;math&#34;&gt;\(a, b\)&lt;/span&gt; are said to collide if &lt;span  class=&#34;math&#34;&gt;\(h(a) = h(b)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The ideal hash functions has the property that the distribution of image of a a subset of the domain is statistically independent of the probability of said subset occuring. That is, collisions are not likely to occur even within non-uniform distributed sets.&lt;/p&gt;

&lt;p&gt;Consider you have an english dictionary. Clearly, &lt;code&gt;hello&lt;/code&gt; is more likely to be a word than &lt;code&gt;ctyhbnkmaasrt&lt;/code&gt;, but the hash function must not be affected by this statistical redundancy.&lt;/p&gt;

&lt;p&gt;In a sense, you can think of the ideal hash function as being a function where the output is uniformly distributed (e.g., chosen by a sequence of coinflips) over the codomain no matter what the distribution of the input is.&lt;/p&gt;

&lt;p&gt;With a good hash function, it should be hard to distinguish between a truely random sequence and the hashes of some permutation of the domain.&lt;/p&gt;

&lt;p&gt;Hash function ought to be as chaotic as possible. A small change in the input should appear in the output as if it was a big change. This is called the hash function butterfly effect.&lt;/p&gt;

&lt;h2 id=&#34;noncryptographic-and-cryptographic&#34;&gt;Non-cryptographic and cryptographic&lt;/h2&gt;

&lt;p&gt;One must make the distinction between cryptographic and non-cryptographic hash functions. In a cryptographic hash function, it must be infeasible to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Generate the input from its hash output.&lt;/li&gt;
&lt;li&gt;Generate two inputs with the same output.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Non-cryptographic hash functions can be thought of as approximations of these invariants. The reason for the use of non-cryptographic hash function is that they&#39;re significantly faster than cryptographic hash functions.&lt;/p&gt;

&lt;h1 id=&#34;diffusions-and-bijection&#34;&gt;Diffusions and bijection&lt;/h1&gt;

&lt;p&gt;The basic building block of good hash functions are difussions. Difussions can be thought of as bijective (i.e. every input has one and only one output, and vice versa) hash functions, namely that input and output are uncorrelated:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/bijective_diffusion_diagram.svg&#34; alt=&#34;A diagram of a diffusion.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;This diffusion function has a relatively small domain, for illustrational purpose.&lt;/p&gt;

&lt;h2 id=&#34;building-a-good-diffusion&#34;&gt;Building a good diffusion&lt;/h2&gt;

&lt;p&gt;Diffusions are often build by smaller, bijective components, which we will call &amp;quot;subdiffusions&amp;quot;.&lt;/p&gt;

&lt;h3 id=&#34;types-of-subdiffusions&#34;&gt;Types of subdiffusions&lt;/h3&gt;

&lt;p&gt;One must distinguish between the different kinds of subdiffusions.&lt;/p&gt;

&lt;p&gt;The first class to consider is the &lt;strong&gt;bitwise subdiffusions&lt;/strong&gt;. These are quite weak when they stand alone, and thus must be combined with other types of subdiffusions. Bitwise subdiffusions might flip certain bits and/or reorganize them:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = \sigma(x) \oplus m\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(we use &lt;span  class=&#34;math&#34;&gt;\(\sigma\)&lt;/span&gt; to denote permutation of bits)&lt;/p&gt;

&lt;p&gt;The second class is &lt;strong&gt;dependent bitwise subdiffusions&lt;/strong&gt;. These are diffusions which permutes the bits and XOR them with the original value:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = \sigma(x) \oplus x\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(exercise to reader: prove that the above subdivision is revertible)&lt;/p&gt;

&lt;p&gt;Another similar often used subdiffusion in the same class is the XOR-shift:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = (x \ll m) \oplus x\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(note that &lt;span  class=&#34;math&#34;&gt;\(m\)&lt;/span&gt; can be negative, in which case the bitshift becomes a right bitshift)&lt;/p&gt;

&lt;p&gt;The next subdiffusion are of massive importance. It&#39;s the class of &lt;strong&gt;linear subdiffusions&lt;/strong&gt; similar to the LCG random number generator:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) \equiv ax + c \pmod m, \quad \gcd(x, m) = 1\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(&lt;span  class=&#34;math&#34;&gt;\(\gcd\)&lt;/span&gt; means &amp;quot;greatest common divisor&amp;quot;, this constraint is necessary in order to have &lt;span  class=&#34;math&#34;&gt;\(a\)&lt;/span&gt; have an inverse in the ring)&lt;/p&gt;

&lt;p&gt;The next are particularly interesting, it&#39;s the &lt;strong&gt;arithmetic subdiffusions&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[d(x) = x \oplus (x + c)\]&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&#34;combining-subdiffusions&#34;&gt;Combining subdiffusions&lt;/h3&gt;

&lt;p&gt;Subdiffusions themself are quite poor quality. Combining them is what creates a good diffusion function.&lt;/p&gt;

&lt;p&gt;Indeed if you combining enough different subdiffusions, you get a good diffusion function, but there is a catch: The more subdiffusions you combine the slower it is to compute.&lt;/p&gt;

&lt;p&gt;As such, it is important to find a small, diverse set of subdiffusions which has a good quality.&lt;/p&gt;

&lt;h3 id=&#34;zerosensitivity&#34;&gt;Zero-sensitivity&lt;/h3&gt;

&lt;p&gt;If your diffusion isn&#39;t zero-sensitive (i.e., &lt;span  class=&#34;math&#34;&gt;\(f(0) = \{0, 1\}\)&lt;/span&gt;), you should &lt;del&gt;panic&lt;/del&gt; come up with something better. In particular, make sure your diffusion contains at least one zero-sensitive subdiffusion as component.&lt;/p&gt;

&lt;h3 id=&#34;avalanche-diagrams&#34;&gt;Avalanche diagrams&lt;/h3&gt;

&lt;p&gt;Avalanche diagrams are the best and quickist way to find out if your diffusion function has a good quality.&lt;/p&gt;

&lt;p&gt;Essentially, you draw a grid such that the &lt;span  class=&#34;math&#34;&gt;\((x, y)\)&lt;/span&gt; cell&#39;s color represents the probability that flipping &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;&#39;th bit of the input will result of &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt;&#39;th bit being flipped in the output. If &lt;span  class=&#34;math&#34;&gt;\((x, y)\)&lt;/span&gt; is very red, the probability that &lt;span  class=&#34;math&#34;&gt;\(d(a&#39;)\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(a&#39;\)&lt;/span&gt; is &lt;span  class=&#34;math&#34;&gt;\(a\)&lt;/span&gt; with the &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;&#39;th bit flipped,&#39; has the &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt;&#39;th bit flipped is very high.&lt;/p&gt;

&lt;p&gt;Here&#39;s an example of the identity function, &lt;span  class=&#34;math&#34;&gt;\(f(x) = x\)&lt;/span&gt;:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/identity_function_avalanche_diagram.svg&#34; alt=&#34;The identity function.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;So why is it a straight line?&lt;/p&gt;

&lt;p&gt;Well, if you flip the &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt;&#39;th bit in the input, the only bit flipped in the output is the &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt;&#39;th bit. That&#39;s kind of boring, let&#39;s try adding a number:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/addition_avalanche_diagram.svg&#34; alt=&#34;Adding a big number.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Meh, this is kind of obvious. Let&#39;s try multiplying by a prime:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/prime_multiplication_avalanche_diagram.svg&#34; alt=&#34;Multiplying by a non-even prime is a bijection.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Now, this is quite interesting actually. We call all the black area &amp;quot;blind spots&amp;quot;, and you can see here that anything with &lt;span  class=&#34;math&#34;&gt;\(x &gt; y\)&lt;/span&gt; is a blind spot. Why is that? Well, if I flip a high bit, it won&#39;t affect the lower bits because you can see multiplication as a form of overlay:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;100011101000101010101010111
      :
    111
↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕↕
100000001000101010101010111
      :
    111
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Flipping a single bit will only change the integer forward, never backwards, hence it forms this blind spot. So how can we fix this (we don&#39;t want this bias)?&lt;/p&gt;

&lt;h4 id=&#34;designing-a-diffusion-function--by-example&#34;&gt;Designing a diffusion function -- by example&lt;/h4&gt;

&lt;p&gt;If we throw in (after prime multiplication) a dependent bitwise-shift subdiffusions, we have&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;\gets x + 1 \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \ll z) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(note that we have the &lt;span  class=&#34;math&#34;&gt;\(+1\)&lt;/span&gt; in order to make it zero-sensitive)&lt;/p&gt;

&lt;p&gt;This generates following avalanche diagram&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/shift_xor_multiply_avalanche_diagram.svg&#34; alt=&#34;Shift-XOR then multiply.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;What can cause these? Clearly there is some form of bias. Turns out that this bias mostly originates in the lack of hybrid arithmetic/bitwise sub.
Without such hybrid, the behavior tends to be relatively local and not interfering well with each other.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[x \gets x + \text{ROL}_k(x)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;At this point, it looks something like&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/shift_xor_multiply_rotate_avalanche_diagram.svg&#34; alt=&#34;Shift-XOR then multiply.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;That&#39;s good, but we&#39;re not quite there yet...&lt;/p&gt;

&lt;p&gt;Let&#39;s throw in the following bijection:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[x \gets px \oplus (px \gg z)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And voilà, we now have a perfect bit independence:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/perfect_avalanche_diagram.svg&#34; alt=&#34;Everything is red!&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;So our finalized version of an example diffusion is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;\gets x + 1 \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \ll z) \\
x &amp;\gets x + \text{ROL}_k(x) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \gg z) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;That seems like a pretty lengthy chunk of operations. We will try to boil it down to few operations while preserving the quality of this diffusion.&lt;/p&gt;

&lt;p&gt;The most obvious think to remove is the rotation line. But it hurts quality:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/multiply_up_avalanche_diagram.svg&#34; alt=&#34;Here&#39;s the avalanche diagram of said line removed.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Where do these blind spot comes from? The answer is pretty simple: shifting left moves the entropy upwards, hence the multiplication will never really flip the lower bits. For example, if we flip the sixth bit, and trace it down the operations, you will how it never flips in the other end.&lt;/p&gt;

&lt;p&gt;So what do we do? Instead of shifting left, we need to shift right, since multiplication only affects upwards:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;\gets x + 1 \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \gg z) \\
x &amp;\gets px \\
x &amp;\gets x \oplus (x \gg z) \\
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And we&#39;re back again. This time with two less instructions.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_1_avalanche_diagram.svg&#34; alt=&#34;Stage 1&#34;&gt;&lt;/figure&gt;&lt;/th&gt;
&lt;th&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_2_avalanche_diagram.svg&#34; alt=&#34;Stage 2&#34;&gt;&lt;/figure&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_3_avalanche_diagram.svg&#34; alt=&#34;Stage 3&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_4_avalanche_diagram.svg&#34; alt=&#34;Stage 4&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/cakehash_stage_5_avalanche_diagram.svg&#34; alt=&#34;Stage 5&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td&gt;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/perfect_avalanche_diagram.svg&#34; alt=&#34;Stage 6&#34;&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&#34;combining-diffusions&#34;&gt;Combining diffusions&lt;/h1&gt;

&lt;p&gt;Diffusions maps a finite state space to a finite state space, as such they&#39;re not alone sufficient as arbitrary-length hash function, so we need a way to combine diffusions.&lt;/p&gt;

&lt;p&gt;In particular, we can eat &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; bytes of the input at once and modify the state based on that:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[s&#39; = d(f(s&#39;, x))\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Or in graphic form,&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/hash_round_flowchart.svg&#34; alt=&#34;A flowchart.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(f(s&#39;, x)\)&lt;/span&gt; is what we call our combinator function. It serves for combining the old state and the new input block (&lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;). &lt;span  class=&#34;math&#34;&gt;\(d(a)\)&lt;/span&gt; is just our diffusion function.&lt;/p&gt;

&lt;p&gt;It doesn&#39;t matter if the combinator function is commutative or not, but it is crucial that it is not biased, i.e. if &lt;span  class=&#34;math&#34;&gt;\(a, b\)&lt;/span&gt; are uniformly distributed variables, &lt;span  class=&#34;math&#34;&gt;\(f(a, b)\)&lt;/span&gt; is too. Ideally, there should exist a bijection, &lt;span  class=&#34;math&#34;&gt;\(g(f(a, b), b) = a\)&lt;/span&gt;, which implies that it is not biased.&lt;/p&gt;

&lt;p&gt;An example of such combination function is simple addition.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(a, b) = a + b\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Another is&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(a, b) = a \oplus b\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I&#39;m partial towards saying that these are the only sane choices for combinator functions, and you must pick between them based on the characteristics of your diffusion function:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If your diffusion function is primarily based on arithmetics, you should use the XOR combinator function.&lt;/li&gt;
&lt;li&gt;If your diffusion function is primarily based on bitwise operations, you should use the additive combinator function.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The reason for this is that you want to have the operations to be as diverse as possible, to create complex, seemingly random behavior.&lt;/p&gt;

&lt;h1 id=&#34;simd-simd-simd&#34;&gt;SIMD, SIMD, SIMD&lt;/h1&gt;

&lt;p&gt;If you want good performance, you shouldn&#39;t read only one byte at a time. By reading multiple bytes at a time, your algorithm becomes several times faster.&lt;/p&gt;

&lt;p&gt;This however introduces the need for some finalization, if the total number of written bytes doesn&#39;t divide the number of bytes read in a round. One possibility is to pad it with zeros and write the total length in the end, however this turns out to be somewhat slow for small inputs.&lt;/p&gt;

&lt;p&gt;A better option is to write in the number of padding bytes into the last byte.&lt;/p&gt;

&lt;h1 id=&#34;instruction-level-parallelism&#34;&gt;Instruction level parallelism&lt;/h1&gt;

&lt;p&gt;Fetching multiple blocks and sequentially (without dependency until last) running a round is something I&#39;ve found to work well. This has to do with the so-called instruction pipeline in which modern processors run instructions in parallel when they can.&lt;/p&gt;

&lt;h1 id=&#34;testing-the-hash-function&#34;&gt;Testing the hash function&lt;/h1&gt;

&lt;p&gt;Multiple test suits for testing the quality and performance of your hash function. &lt;a href=&#34;https://github.com/aappleby/smhasher&#34;&gt;Smhasher&lt;/a&gt; is one of these.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Many relatively simple components can be combined into a strong and robust non-cryptographic hash function for use in hash tables and in checksumming. Deriving such a function is really just coming up with the components to construct this hash function.&lt;/p&gt;

&lt;p&gt;Breaking the problem down into small subproblems significantly simplifies analysis and guarantees.&lt;/p&gt;

&lt;p&gt;The key to a good hash function is to try-and-miss. Testing and throwing out candidates is the only way you can really find out if you hash function works in practice.&lt;/p&gt;

&lt;p&gt;Have fun hacking!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How LZ4 works</title>
      <link>http://ticki.github.io/blog/how-lz4-works/</link>
      <pubDate>Tue, 25 Oct 2016 23:25:15 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/how-lz4-works/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;LZ4 is a really fast compression algorithm with a reasonable compression ratio, but unfortunately there is limited documentation on how it works. The only explanation (not spec, explanation) &lt;a href=&#34;https://fastcompression.blogspot.com/2011/05/lz4-explained.html&#34;&gt;can be found&lt;/a&gt; on the author&#39;s blog, but I think it is less of an explanation and more of an informal specification.&lt;/p&gt;

&lt;p&gt;This blog post tries to explain it such that anybody (even new beginners) can understand and implement it.&lt;/p&gt;

&lt;h1 id=&#34;linear-smallinteger-code-lsic&#34;&gt;Linear small-integer code (LSIC)&lt;/h1&gt;

&lt;p&gt;The first part of LZ4 we need to explain is a smart but simple integer encoder. It is very space efficient for 0-255, and then grows linearly, based on the assumption that the integers used with this encoding rarely exceeds this limit, as such it is only used for small integers in the standard.&lt;/p&gt;

&lt;p&gt;It is a form of addition code, in which we read a byte. If this byte is the maximal value (255), another byte is read and added to the sum. This process is repeated until a byte below 255 is reached, which will be added to the sum, and the sequence will then end.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_int_encoding_flowchart.svg&#34; alt=&#34;We try to fit it into the next cluster.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;In short, we just keep adding bytes and stop when we hit a non-0xFF byte.&lt;/p&gt;

&lt;p&gt;We&#39;ll use the name &amp;quot;LSIC&amp;quot; for convinience.&lt;/p&gt;

&lt;h1 id=&#34;block&#34;&gt;Block&lt;/h1&gt;

&lt;p&gt;An LZ4 stream is divided into segments called &amp;quot;blocks&amp;quot;. Blocks contains a literal which is to be copied directly to the output stream, and then a back reference, which tells us to copy some number of bytes from the already decompressed stream.&lt;/p&gt;

&lt;p&gt;This is really were the compression is going on. Copying from the old stream allows deduplication and runs-length encoding.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;A block looks like:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\overbrace{\underbrace{t_1}_\text{4 bits}\  \underbrace{t_2}_\text{4 bits}}^\text{Token} \quad \underbrace{\overbrace{e_1}^\texttt{LISC}}_\text{If $t_1 = 15$} \quad \underbrace{\overbrace{L}^\text{Literal}}_{t_1 + e_1\text{ bytes }} \quad \overbrace{\underbrace{O}_\text{2 bytes}}^\text{Little endian} \quad \underbrace{\overbrace{e_2}^\texttt{LISC}}_\text{If $t_2 = 15$}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And decodes to the &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt; segment, followed by a &lt;span  class=&#34;math&#34;&gt;\(t_2 + e_2 + 4\)&lt;/span&gt; bytes sequence copied from position &lt;span  class=&#34;math&#34;&gt;\(l - O\)&lt;/span&gt; from the output buffer (where &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt; is the length of the output buffer).&lt;/p&gt;

&lt;p&gt;We will explain all of these in the next sections.&lt;/p&gt;

&lt;h2 id=&#34;token&#34;&gt;Token&lt;/h2&gt;

&lt;p&gt;Any block starts with a 1 byte token, which is divided into two 4-bit fields.&lt;/p&gt;

&lt;h2 id=&#34;literals&#34;&gt;Literals&lt;/h2&gt;

&lt;p&gt;The first (highest) field in the token is used to define the literal. This obviously takes a value 0-15.&lt;/p&gt;

&lt;p&gt;Since we might want to encode higher integer, as such we make use of LSIC encoding: If the field is 15 (the maximal value), we read an integer with LSIC and add it to the original value (15) to obtain the literals length.&lt;/p&gt;

&lt;p&gt;Call the final value &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Then we forward the next &lt;span  class=&#34;math&#34;&gt;\(L\)&lt;/span&gt; bytes from the input stream to the output stream.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_literals_copy_diagram.svg&#34; alt=&#34;We copy from the buffer directly.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;deduplication&#34;&gt;Deduplication&lt;/h2&gt;

&lt;p&gt;The next few bytes are used to define some segment in the already decoded buffer, which is going to be appended to the output buffer.&lt;/p&gt;

&lt;p&gt;This allows us to transmit a position and a length to read from in the already decoded buffer instead of transmitting the literals themself.&lt;/p&gt;

&lt;p&gt;To start with, we read a 16-bit little endian integer. This defines the so called offset, &lt;span  class=&#34;math&#34;&gt;\(O\)&lt;/span&gt;. It is important to understand that the offset is not the starting position of the copied buffer. This starting point is calculated by &lt;span  class=&#34;math&#34;&gt;\(l - O\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(l\)&lt;/span&gt; being the number of bytes already decoded.&lt;/p&gt;

&lt;p&gt;Secondly, similarly to the literals length, if &lt;span  class=&#34;math&#34;&gt;\(t_2\)&lt;/span&gt; is 15 (the maximal value), we use LSIC to &amp;quot;extend&amp;quot; this value and we add the result. This plus 4 yields the number of bytes we will copy from the output buffer. The reason we add 4 is because copying less than 4 bytes would result in a negative expansion of the compressed buffer.&lt;/p&gt;

&lt;p&gt;Now that we know the start position and the length, we can append the segment to the buffer itself:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_deduplicating_diagram.svg&#34; alt=&#34;Copying in action.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;It is important to understand that the end of the segment might not be initializied before the rest of the segment is appended, because overlaps are allowed. This allows a neat trick, namely &amp;quot;runs-length encoding&amp;quot;, where you repeat some sequence a given number of times:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/lz4_runs_encoding_diagram.svg&#34; alt=&#34;We repeat the last byte.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Note that the duplicate section is not required if you&#39;re in the end of the stream, i.e. if there&#39;s no more compressed bytes to read.&lt;/p&gt;

&lt;h1 id=&#34;compression&#34;&gt;Compression&lt;/h1&gt;

&lt;p&gt;Until now, we have only considered decoding, not the reverse process.&lt;/p&gt;

&lt;p&gt;A dozen of approaches to compression exists. They have the aspects that they need to be able to find duplicates in the already input buffer.&lt;/p&gt;

&lt;p&gt;In general, there are two classes of such compression algorithms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;HC: High-compression ratio algorithms, these are often very complex, and might include steps like backtracking, removing repeatation, non-greediy.&lt;/li&gt;
&lt;li&gt;FC: Fast compression, these are simpler and faster, but provides a slightly worse compression ratio.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will focus on the FC-class algorithms.&lt;/p&gt;

&lt;p&gt;Binary Search Trees (often B-trees) are often used for searching for duplicates. In particular, every byte iterated over will add a pointer to the rest of the buffer to a B-tree, we call the &amp;quot;duplicate tree&amp;quot;. Now, B-trees allows us to retrieve the largest element smaller than or equal to some key. In lexiographic ordering, this is equivalent to asking the element sharing the largest number of bytes as prefix.&lt;/p&gt;

&lt;p&gt;For example, consider the table:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;abcdddd =&amp;gt; 0
bcdddd  =&amp;gt; 1
cdddd   =&amp;gt; 2
dddd    =&amp;gt; 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we search for &lt;code&gt;cddda&lt;/code&gt;, we&#39;ll get a partial match, namely &lt;code&gt;cdddd =&amp;gt; 2&lt;/code&gt;. So we can quickly find out how many bytes they have in common as prefix. In this case, it is 4 bytes.&lt;/p&gt;

&lt;p&gt;What if we found no match or a bad match (a match that shares less than some threshold)? Well, then we write it as literal until a good match is found.&lt;/p&gt;

&lt;p&gt;As you may notice, the dictionary grows linearly. As such, it is important that you reduce memory once in a while, by trimming it. Note that just trimming the first (or last) &lt;span  class=&#34;math&#34;&gt;\(N\)&lt;/span&gt; entries is inefficient, because some might be used often. Instead, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Cache_Replacement_Policies&#34;&gt;cache replacement policy&lt;/a&gt; should be used. If the dictionary is filled, the cache replacement policy should determine which match should be replaced. I&#39;ve found PLRU a good choice of CRP for LZ4 compression.&lt;/p&gt;

&lt;p&gt;Note that you should add additional rules like being addressible (within &lt;span  class=&#34;math&#34;&gt;\(2^{16} + 4\)&lt;/span&gt; bytes of the cursor, which is required because &lt;span  class=&#34;math&#34;&gt;\(O\)&lt;/span&gt; is 16-bit) and being above some length (smaller keys have worse block-level compression ratio).&lt;/p&gt;

&lt;p&gt;Another faster but worse (compression-wise) approach is hashing every four bytes and placing them in a table. This means that you can only look up the latest sequence given some 4-byte prefix. Looking up allows you to progress and see how long the duplicate sequence match. When you can&#39;t go any longer, you encode the literals section until another duplicate 4-byte is found.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;LZ4 is a reasonably simple algorithm with reasonably good compression ratio. It is the type of algorithm that you can implement on an afternoon without much complication.&lt;/p&gt;

&lt;p&gt;If you need a portable and efficient compression algorithm which can be implement in only a few hundreds of lines, LZ4 would be my go-to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On Random-Access Compression</title>
      <link>http://ticki.github.io/blog/on-random-access-compression/</link>
      <pubDate>Sun, 23 Oct 2016 23:25:15 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/on-random-access-compression/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;This post will contains an algorithm I came up with, doing efficient rolling compression. It&#39;s going to be used in &lt;a href=&#34;https://github.com/ticki/tfs&#34;&gt;TFS&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;what-is-rolling-compression&#34;&gt;What is rolling compression?&lt;/h1&gt;

&lt;p&gt;Consider that you have a large file and you want to compress it. That&#39;s easy enough and many algorithms exists for doing so. Now, consider that you want to read or write a small part of the file.&lt;/p&gt;

&lt;p&gt;Most algorithms would require you to decompress, write, and recompress the whole file. Clearly, this gets expensive when the file is big.&lt;/p&gt;

&lt;h1 id=&#34;clusterbased-compression&#34;&gt;Cluster-based compression&lt;/h1&gt;

&lt;p&gt;A cluster is some small fixed-size block (often 512, 1024, or 4096 bytes). We can have a basic cluster allocator by linking unused clusters together. Cluster-centric compression is interesting, because it can exploit the allocator.&lt;/p&gt;

&lt;p&gt;So, the outline is that we compress every &lt;span  class=&#34;math&#34;&gt;\(n\)&lt;/span&gt; adjacent clusters to some &lt;span  class=&#34;math&#34;&gt;\(n&#39; &lt; n%&gt;\)&lt;/span&gt;, then we can free the excessive clusters in this compressed line.&lt;/p&gt;

&lt;h1 id=&#34;copyonwrite&#34;&gt;Copy-on-write&lt;/h1&gt;

&lt;p&gt;Our algorithm is not writable, but it can be written by allocating, copying, and deallocating. This is called copy-on-write, or COW for short. It is a common technique used in many file systems.&lt;/p&gt;

&lt;p&gt;Essentially, we never write a cluster. Instead, we allocate a new cluster, and copy the data to it. Then we deallocate the old cluster.&lt;/p&gt;

&lt;p&gt;This allows us to approach everything much more functionally, and we thus don&#39;t have to worry about make compressible blocks uncompressible (consider that you overwrite a highly compressible cluster with random data, then you extend a physical cluster containing many virtual clusters, these wouldn&#39;t be possible to have in one cluster).&lt;/p&gt;

&lt;h1 id=&#34;physical-and-virtual-clusters&#34;&gt;Physical and virtual clusters&lt;/h1&gt;

&lt;p&gt;Our goal is really fit multiple clusters into one physical cluster. Therefore, it is essential to distinguish between physical (the stored) and virtual (the compressed) clusters.&lt;/p&gt;

&lt;p&gt;A physical cluster can contain up to 8 virtual clusters. A pointer to a virtual cluster starts with 3 bits defining the index into the physical cluster, which is defined by the rest of the pointer.&lt;/p&gt;

&lt;p&gt;The allocated physical cluster contains 8 bitflags, defining which of the 8 virtual clusters in the physical cluster are used. This allows us to know how many virtual clusters we need to go over before we get the target decompressed cluster.&lt;/p&gt;

&lt;p&gt;When the integer hits zero (i.e. all the virtual clusters are freed), the physical cluster is freed.&lt;/p&gt;

&lt;p&gt;Since an active cluster will never have the state zero, we use this blind state to represent an uncompressed physical cluster. This means we maximally have one byte in space overhead for uncompressible clusters.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/virtual_physical_random_access_compression_diagram.svg&#34; alt=&#34;A diagram&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-physical-cluster-allocator&#34;&gt;The physical cluster allocator&lt;/h1&gt;

&lt;p&gt;The cluster allocator is nothing but a linked list of clusters. Every free cluster links to another free cluster or NIL (no more free clusters).&lt;/p&gt;

&lt;p&gt;This method is called SLOB (Simple List Of Objects) and has the advantage of being complete zero-cost in that there is no wasted space.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/slob_allocation_diagram.svg&#34; alt=&#34;Physical allocation is simply linked list of free objects.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;the-virtual-cluster-allocator&#34;&gt;The virtual cluster allocator&lt;/h1&gt;

&lt;p&gt;Now we hit the meat of the matter.&lt;/p&gt;

&lt;p&gt;When virtual cluster is allocated, we read from the physical cluster list. The first thing we will check is if we can fit in our virtual cluster into the cluster next to the head of the list (we wrap if we reach the end).&lt;/p&gt;

&lt;p&gt;If we can fit it in &lt;em&gt;and&lt;/em&gt; we have less than 8 virtual clusters in this physical cluster, we will put it into the compressed physical cluster at the first free virtual slot (and then set the respective bitflag):&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/allocating_compressed_virtual_page_into_next_diagram.svg&#34; alt=&#34;We try to fit it into the next cluster.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;If we cannot, we pop the list and use the fully-free physical cluster to store etablish a new stack of virtual clusters. It starts as uncompressed:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;http://ticki.github.io/img/pop_and_create_new_uncompressed_cluster_diagram.svg&#34; alt=&#34;We pop the list and put the virtual cluster in the physical uncompressed slot.&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;properties-of-this-approach&#34;&gt;Properties of this approach&lt;/h1&gt;

&lt;p&gt;This approach to writable random-access compression has some very nice properties.&lt;/p&gt;

&lt;h2 id=&#34;compression-miss&#34;&gt;Compression miss&lt;/h2&gt;

&lt;p&gt;We call it a compression miss when we need to pop from the freelist (i.e. we cannot fit it into the cluster next to the head). When you allocate you can maximally have one compression miss, and therefore allocation is constant-time.&lt;/p&gt;

&lt;h2 id=&#34;every-cluster-has-a-sister-cluster&#34;&gt;Every cluster has a sister cluster&lt;/h2&gt;

&lt;p&gt;Because the &amp;quot;next cluster or wrap&amp;quot; function is bijective, we&#39;re sure that we try to insert a virtual cluster to every cluster at least once. This wouldn&#39;t be true if we used a hash function or something else.&lt;/p&gt;

&lt;p&gt;This has the interesting consequence that filled clusters won&#39;t be tried to allocate in multiple times.&lt;/p&gt;

&lt;h1 id=&#34;limitations&#34;&gt;Limitations&lt;/h1&gt;

&lt;p&gt;A number of limitations are in this algorithms. The first and most obvious one is the limitation on the compression ratio. This is a minor one: it limits the ratio to maxmially slightly less than 1:8.&lt;/p&gt;

&lt;p&gt;A more important limitation is fragmentation. If I allocate many clusters and then deallocate some of them such that many adjacent physical clusters only contain one virtual cluster, this row will have a compression ratio of 1:1 until they&#39;re deallocated. Note that it is very rare that this happens, and will only marginally affect the global compression ratio.&lt;/p&gt;

&lt;h1 id=&#34;update-an-idea&#34;&gt;Update: An idea&lt;/h1&gt;

&lt;p&gt;A simple trick can improve performance in some cases. Instead of compressing all the virtual clusters in a physical cluster together, you should compress each virtual cluster seperately and place them sequentially (with some delimiter) in the physical cluster.&lt;/p&gt;

&lt;p&gt;If your compression algorithm is streaming, you can much faster iterate to the right delimiter, and then only decompress that virtual cluster.&lt;/p&gt;

&lt;p&gt;This has the downside of making the compression ratio worse. One solution is to have an initial dictionary (if using a dictionary-based compression algorithm).&lt;/p&gt;

&lt;p&gt;Another idea is to eliminate the cluster state and replace it by repeated delimiters. I need to investigate this some more with benchmarks and so on in order to tell if this is actually superior to having a centralized cluster state.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making Terminal Applications in Rust with Termion</title>
      <link>http://ticki.github.io/blog/making-terminal-applications-in-rust-with-termion/</link>
      <pubDate>Thu, 06 Oct 2016 10:12:22 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/making-terminal-applications-in-rust-with-termion/</guid>
      <description>

&lt;p&gt;This post will walk through the basics of implementing a terminal (TTY) application for both new beginners and experienced users of Rust.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Terminal applications play an important role in many programmers&amp;rsquo; toolchain, from text editors to minigames while your code is compiling. And it&amp;rsquo;s great to know and understand how to make these yourself, so you can create a customized TUI application for your needs.&lt;/p&gt;

&lt;p&gt;Escape codes and TTY I/O is messy, but fortunately there are libraries for this. We will use &lt;a href=&#34;https://github.com/ticki/termion&#34;&gt;Termion&lt;/a&gt;, which is the most feature-complete TUI library in pure Rust.&lt;/p&gt;

&lt;p&gt;Termion is pretty simple and straight-forward. This &amp;ldquo;tutorial&amp;rdquo; or guide is going to walk through these in a manner that even Rust new beginners can understand.&lt;/p&gt;

&lt;h1 id=&#34;understanding-the-tty&#34;&gt;Understanding the TTY&lt;/h1&gt;

&lt;p&gt;Ignoring historical facts, the TTY is the name of the virtual device that takes some stream of text and presents it to the user. As opposed to sophisticated UIs and graphics, it is incredibly simple to get started with.&lt;/p&gt;

&lt;p&gt;The terminal emulator keeps a grid of characters, and a cursor. When you write to the standard output the cell is overwritten with the new character and the cursor moves respectively.&lt;/p&gt;

&lt;p&gt;Take the code,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;println!(&amp;quot;Text here.&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All this does is writing some text to the standard output, and when you run this program, &amp;ldquo;Text here.&amp;rdquo; should appear before the TTY cursor.&lt;/p&gt;

&lt;p&gt;If this is all we can do, how can we create interactive TTY applications? Well, it turns out that there is a whole lot more, we can do.&lt;/p&gt;

&lt;p&gt;Certain sequences represents some operations to the TTY. These are called &amp;ldquo;escape sequences&amp;rdquo; and can do things like changing the color of the text, change the background, moving the cursor, clearing the screen, and so on. Writing these codes by hand quickly gets messy, so we let Termion do it for us:&lt;/p&gt;

&lt;h1 id=&#34;setting-up-termion&#34;&gt;Setting up Termion&lt;/h1&gt;

&lt;p&gt;Start by making sure &lt;code&gt;cargo&lt;/code&gt; is installed, then do&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Initialize a new cargo repository.
cargo new --bin my-tui-app
# Cd into it
cd my-tui-app
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then open the &lt;code&gt;Cargo.toml&lt;/code&gt; file with your favorite text editor, and add&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;termion = &amp;quot;1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To the file under the section &lt;code&gt;[dependencies]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then open up &lt;code&gt;src/lib.rs&lt;/code&gt; and add&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now everything is ready to start!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For documentation, see &lt;a href=&#34;https://github.com/ticki/termion&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;the-structure-of-termion&#34;&gt;The structure of Termion&lt;/h1&gt;

&lt;p&gt;Termion is divided into 8 different modules each providing different functions and primitives:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;clear&lt;/code&gt;: For clearing the screen or parts of the screen.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;color&lt;/code&gt;: For changing the foreground or background color of the text.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cursor&lt;/code&gt;: For moving the cursor around.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;event&lt;/code&gt;: For handling mouse cursor or modifiers.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;input&lt;/code&gt;: For getting more advanced user input (like asynchronous user input).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;raw&lt;/code&gt;: Switching to raw mode (we will get back to this later)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scroll&lt;/code&gt;: Scrolling up or down the text stream.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;style&lt;/code&gt;: Changing the text style or formatting.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;color&#34;&gt;Color&lt;/h2&gt;

&lt;p&gt;Since escapes really are nothing but just another text output, we use the &lt;code&gt;std::fmt::Display&lt;/code&gt; to generate the escape codes. This means that we can use it with macros like &lt;code&gt;write!&lt;/code&gt; or &lt;code&gt;println!&lt;/code&gt;. If we want red text for example, we can do simply:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

// Import the color module.
use termion::color;

fn main() {
    println!(&amp;quot;{red}more red than any comrade{reset}&amp;quot;,
             red   = color::Fg(color::Red),
             reset = color::Fg(color::Reset));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;color::Fg&lt;/code&gt; specifies that we want to change the &lt;em&gt;foreground color&lt;/em&gt; (i.e. the color of the text), &lt;code&gt;color::Fg(color::Reset)&lt;/code&gt; means that we &lt;em&gt;reset&lt;/em&gt; the foreground color.&lt;/p&gt;

&lt;h2 id=&#34;clear&#34;&gt;Clear&lt;/h2&gt;

&lt;p&gt;Clearing the screen allows you to remove text which is already written without overwriting it manually with spaces. For example, I can easily implement the &lt;code&gt;clear&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

// Import the `clear` module.
use termion::clear;

fn main() {
    println!(&amp;quot;{}&amp;quot;, clear::All);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It should be pretty obvious that &lt;code&gt;clear::All&lt;/code&gt; clears the whole grid, but what if we only want to clear the screen partially?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;clear::CurrentLine&lt;/code&gt; will leave the current line empty.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clear::AfterCursor&lt;/code&gt; clears from the cursor to the end of the grid.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;clear::BeforeCursor&lt;/code&gt; clears from the cursor to the beginning of the grid.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/clear/index.html&#34;&gt;and so on&amp;hellip;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cursor&#34;&gt;Cursor&lt;/h2&gt;

&lt;p&gt;What if I want to jump back and overwrite what I just wrote? The easy way is to use &lt;code&gt;\r&lt;/code&gt;, which will jump back to the start of the line:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::{color, clear};
use std::time::Duration;
use std::thread;

fn main() {
    println!(&amp;quot;{red}more red than any comrade{reset}&amp;quot;,
             red   = color::Fg(color::Red),
             reset = color::Fg(color::Reset));
    // Sleep for a short period of time.
    thread::sleep(Duration::from_millis(300));
    // Go back;
    println!(&amp;quot;\r&amp;quot;);
    // Clear the line and print some new stuff
    print!(&amp;quot;{clear}{red}g{blue}a{green}y{red} space communism{reset}&amp;quot;,
            clear = clear::CurrentLine,
            red   = color::Fg(color::Red),
            blue  = color::Fg(color::Blue),
            green = color::Fg(color::Green),
            reset = color::Fg(color::Reset));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But actually, &lt;code&gt;\r&lt;/code&gt; is pretty limited, because it only allows us to jump to the start of the line. What if we want to jump to an arbitrary cell in the text grid?&lt;/p&gt;

&lt;p&gt;Well, we can do that with &lt;code&gt;cursor::Goto&lt;/code&gt;, say we want to print the text at (4,2):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::{color, cursor, clear};

fn main() {
    println!(&amp;quot;{clear}{goto}{red}more red than any comrade{reset}&amp;quot;,
             // Full screen clear.
             clear = clear::All,
             // Goto the cell.
             goto  = cursor::Goto(4, 2),
             red   = color::Fg(color::Red),
             reset = color::Fg(color::Reset));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;style&#34;&gt;Style&lt;/h2&gt;

&lt;p&gt;What if I want my gay space communism to have style?&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;style&lt;/code&gt; module provides escape codes for that. For example, let&amp;rsquo;s print it in bold (&lt;code&gt;style::Bold&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::{color, clear, style};

fn main() {
    println!(&amp;quot;{bold}{red}g{blue}a{green}y{red} space communism{reset}&amp;quot;,
            bold  = style::Bold,
            red   = color::Fg(color::Red),
            blue  = color::Fg(color::Blue),
            green = color::Fg(color::Green),
            reset = style::Reset);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Neat. Now we can control the cursor, clear stuff, set color, and set style. That should be good enough to get us started.&lt;/p&gt;

&lt;h1 id=&#34;entering-raw-mode&#34;&gt;Entering raw mode&lt;/h1&gt;

&lt;p&gt;Without raw mode, you cannot write a proper interactive TTY application. Raw mode gives you complete control over the TTY:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It disables the line buffering: As you might notice, your command-line application tends to behave like the command-line. The programs will first get the input when the user types &lt;code&gt;\n&lt;/code&gt;. Raw mode makes the program get the input after every key stroke.&lt;/li&gt;
&lt;li&gt;It disables displaying the input: Without raw mode, the things you type appear on the screen, making it insufficient for most interactive TTY applications, where keys can represent controls and not textual input.&lt;/li&gt;
&lt;li&gt;It disables canonicalization of the output: For example, &lt;code&gt;\n&lt;/code&gt; represents &amp;ldquo;go one cell down&amp;rdquo; not &amp;ldquo;break the line&amp;rdquo;, for line breaks &lt;code&gt;\n\r&lt;/code&gt; is needed.&lt;/li&gt;
&lt;li&gt;It disables scrolling.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, how do we enter raw mode?&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not that hard:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;use termion::raw::IntoRawMode;
use std::io::{Write, stdout};

fn main() {
    // Enter raw mode.
    let mut stdout = stdout().into_raw_mode().unwrap();

    // Write to stdout (note that we don&#39;t use `println!`)
    writeln!(stdout, &amp;quot;Hey there.&amp;quot;).unwrap();

    // Here the destructor is automatically called, and the terminal state is restored.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;inputs&#34;&gt;Inputs&lt;/h1&gt;

&lt;p&gt;Keys and modifiers are somewhat oddly encoded in the ANSI standards, and fortunately Termion parses those for you. If you take a look at the &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/input/trait.TermRead.html&#34;&gt;&lt;code&gt;TermRead&lt;/code&gt;&lt;/a&gt; trait, you&amp;rsquo;ll see the method called &lt;code&gt;keys&lt;/code&gt;. This returns an iterator over &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/event/enum.Key.html&#34;&gt;&lt;code&gt;Key&lt;/code&gt;&lt;/a&gt;, an enum which contains the parsed keys.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::event::Key;
use termion::input::TermRead;
use termion::raw::IntoRawMode;
use std::io::{Write, stdout, stdin};

fn main() {
    // Get the standard input stream.
    let stdin = stdin();
    // Get the standard output stream and go to raw mode.
    let mut stdout = stdout().into_raw_mode().unwrap();

    write!(stdout, &amp;quot;{}{}q to exit. Type stuff, use alt, and so on.{}&amp;quot;,
           // Clear the screen.
           termion::clear::All,
           // Goto (1,1).
           termion::cursor::Goto(1, 1),
           // Hide the cursor.
           termion::cursor::Hide).unwrap();
    // Flush stdout (i.e. make the output appear).
    stdout.flush().unwrap();

    for c in stdin.keys() {
        // Clear the current line.
        write!(stdout, &amp;quot;{}{}&amp;quot;, termion::cursor::Goto(1, 1), termion::clear::CurrentLine).unwrap();

        // Print the key we type...
        match c.unwrap() {
            // Exit.
            Key::Char(&#39;q&#39;) =&amp;gt; break,
            Key::Char(c)   =&amp;gt; println!(&amp;quot;{}&amp;quot;, c),
            Key::Alt(c)    =&amp;gt; println!(&amp;quot;Alt-{}&amp;quot;, c),
            Key::Ctrl(c)   =&amp;gt; println!(&amp;quot;Ctrl-{}&amp;quot;, c),
            Key::Left      =&amp;gt; println!(&amp;quot;&amp;lt;left&amp;gt;&amp;quot;),
            Key::Right     =&amp;gt; println!(&amp;quot;&amp;lt;right&amp;gt;&amp;quot;),
            Key::Up        =&amp;gt; println!(&amp;quot;&amp;lt;up&amp;gt;&amp;quot;),
            Key::Down      =&amp;gt; println!(&amp;quot;&amp;lt;down&amp;gt;&amp;quot;),
            _              =&amp;gt; println!(&amp;quot;Other&amp;quot;),
        }

        // Flush again.
        stdout.flush().unwrap();
    }

    // Show the cursor again before we exit.
    write!(stdout, &amp;quot;{}&amp;quot;, termion::cursor::Show).unwrap();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What the above snippet does is to open a blank screen, where it informs you what keys and modifiers you type as you press keys.&lt;/p&gt;

&lt;h1 id=&#34;asynchronized-stdin&#34;&gt;Asynchronized stdin&lt;/h1&gt;

&lt;p&gt;One interesting problem you will run into, while writing certain terminal application is that the stdin is blocking, and you need to wait to the user giving the input. This potentially could block your application from doing work while waiting for user input (e.g. you freeze the graphics).&lt;/p&gt;

&lt;p&gt;Fortunately, Termion has a solution to that &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/fn.async_stdin.html&#34;&gt;&lt;code&gt;termion::async_stdin()&lt;/code&gt;&lt;/a&gt;. In principle, it is really simple. It works around the limitation to TTYs by using another thread to read from the stdin, and when your main thread needs to read from the stream, it pops from a concurrent queue to read the bytes. It doesn&amp;rsquo;t scale to things like byte streams, but it works seamlessly with user input.&lt;/p&gt;

&lt;h1 id=&#34;mouse&#34;&gt;Mouse&lt;/h1&gt;

&lt;p&gt;You can read mouse clicks etc. by converting your stdin stream to &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/input/struct.MouseTerminal.html&#34;&gt;&lt;code&gt;termion::input::MouseTerminal&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;extern crate termion;

use termion::event::*;
use termion::cursor;
use termion::input::{TermRead, MouseTerminal};
use termion::raw::IntoRawMode;
use std::io::{self, Write};

fn main() {
    let stdin = io::stdin();
    let mut stdout = MouseTerminal::from(io::stdout().into_raw_mode().unwrap());
    // ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can clear the screen:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;    writeln!(stdout,
             &amp;quot;{}{}q to exit. Type stuff, use alt, click around...&amp;quot;,
             termion::clear::All,
             termion::cursor::Goto(1, 1))
        .unwrap();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you can read mouse inputs through the &lt;code&gt;events()&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;    for c in stdin.events() {
        let evt = c.unwrap();
        match evt {
            Event::Key(Key::Char(&#39;q&#39;)) =&amp;gt; break,
            Event::Mouse(me) =&amp;gt; {
                match me {
                    MouseEvent::Press(_, a, b) |
                    MouseEvent::Release(a, b) |
                    MouseEvent::Hold(a, b) =&amp;gt; {
                        write!(stdout, &amp;quot;{}&amp;quot;, cursor::Goto(a, b)).unwrap();
                    }
                }
            }
            _ =&amp;gt; {}
        }
        stdout.flush().unwrap();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, if you click around or hold your your mouse, the TTY cursor should follow.&lt;/p&gt;

&lt;h1 id=&#34;a-few-extra-tricks&#34;&gt;A few extra tricks&lt;/h1&gt;

&lt;h2 id=&#34;the-terminal-size&#34;&gt;The terminal size&lt;/h2&gt;

&lt;p&gt;Sometimes you might want to center or align things. This need the terminal size, which can be obtained by &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/fn.terminal_size.html&#34;&gt;&lt;code&gt;termion::terminal_size()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;bypassing-piped-input&#34;&gt;Bypassing piped input&lt;/h2&gt;

&lt;p&gt;Sometimes you might want to pipe some input to your program while controling the TTY. This is actually not that hard. With &lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/fn.get_tty.html&#34;&gt;&lt;code&gt;termion::get_tty()&lt;/code&gt;&lt;/a&gt;, you can read and write from the TTY, while still being able to read or write to stdin/stdout via &lt;code&gt;std::io&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;truecolor&#34;&gt;Truecolor&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.rs/termion/1.1.1/termion/color/struct.Rgb.html&#34;&gt;&lt;code&gt;termion::color::Rgb(r, g, b)&lt;/code&gt;&lt;/a&gt; allows you to use full 24-bit truecolor.&lt;/p&gt;

&lt;h1 id=&#34;trying-all-this-out-yourself&#34;&gt;Trying all this out yourself&lt;/h1&gt;

&lt;p&gt;There&amp;rsquo;s a lot of things you can do as well:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Writing a simple nano clone.&lt;/li&gt;
&lt;li&gt;Writing a TUI music player.&lt;/li&gt;
&lt;li&gt;Writing a TODO list manager.&lt;/li&gt;
&lt;li&gt;Writing an interactive TUI file manager.&lt;/li&gt;
&lt;li&gt;Writing a game.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;reference-programs-and-examples&#34;&gt;Reference programs and examples&lt;/h1&gt;

&lt;p&gt;If you need a hands-on reference or examples on using termion, you can check out one of the following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ticki/termion/tree/master/examples&#34;&gt;The termion examples&lt;/a&gt;* (easy/overview)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ticki/rem/blob/master/src/main.rs&#34;&gt;An utility to set countdowns/reminders in the terminal&lt;/a&gt;* (easy)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hoodie/battery-rs&#34;&gt;An utility to get the battery status from command line&lt;/a&gt;* (easy)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/redox-os/games-for-redox/blob/master/src/ice/main.rs&#34;&gt;Pokemon-style ice sliding puzzle for terminal&lt;/a&gt;* (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/redox-os/games-for-redox/blob/master/src/minesweeper/main.rs&#34;&gt;Minesweeper implementation&lt;/a&gt;* (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/redox-os/games-for-redox/blob/master/src/snake/main.rs&#34;&gt;Snake implementation&lt;/a&gt; (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ca1ek/ircim&#34;&gt;An IRC client&lt;/a&gt; (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MovingtoMars/liner&#34;&gt;A line-editing library&lt;/a&gt; (medium)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/IGI-111/Smith&#34;&gt;A standalone editor&lt;/a&gt; (hard)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Munksgaard/inquirer-rs&#34;&gt;A more high-level TTY library built on top of Termion&lt;/a&gt; (hard)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/little-dude/xi-tui&#34;&gt;A Termion Xi-editor frontend&lt;/a&gt; (hard)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
&lt;li&gt;= Recommended as reference or example.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want your program added, just contact me.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Hoare Logic for Rust</title>
      <link>http://ticki.github.io/blog/a-hoare-logic-for-rust/</link>
      <pubDate>Sat, 24 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/a-hoare-logic-for-rust/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;Lately, I&#39;ve been working on a Hoare-logic-based model of the Rust MIR, which I will introduce in the post. This is a minor step towards a memory model of Rust, and it allows formalization of programs and their behavior.&lt;/p&gt;

&lt;p&gt;This project was born out of the effort to formalize &lt;a href=&#34;https://github.com/redox-os/redox&#34;&gt;the Redox kernel&lt;/a&gt; and &lt;a href=&#34;https://github.com/redox-os/ralloc/tree/skiplist&#34;&gt;the ralloc memory allocator&lt;/a&gt; as well as coming up with a &lt;a href=&#34;https://github.com/rust-lang/rfcs/issues/1447&#34;&gt;Rust memory model&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here I will walk through the techniques, axioms, and transformations in detail. I&#39;ve divided this post into three parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;An introduction to Hoare logic: An gentle introduction for the beginners (can be skipped if you&#39;re already familiar with Hoare logic).&lt;/li&gt;
&lt;li&gt;Applying Hoare logic to the Rust MIR: Notably dropping structured programming in favour of a lower-level goto-based representation, and how it helps simplifying certain things.&lt;/li&gt;
&lt;li&gt;Reasoning about pointers: Pointers are notoriously hard to reason about. Here we try to formalize their behavior and give various insight on how they can be reasoned about. Priory to this part, we assume that pointers doesn&#39;t exist.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This blog post is not a formal specification or a paper, but rather a mere introduction to the subject and proposed axioms.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If the math doesn&#39;t show up properly, reload the page.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&#34;an-introduction-to-hoare-logic&#34;&gt;An introduction to Hoare logic&lt;/h1&gt;

&lt;p&gt;So, what is Hoare logic? Well, it&#39;s a set of axioms and inference rules allowing one to reason about &lt;em&gt;imperative programs&lt;/em&gt; in a rigorous manner.&lt;/p&gt;

&lt;p&gt;The program is divided into so called &lt;strong&gt;Hoare triples&lt;/strong&gt;, denoted &lt;span  class=&#34;math&#34;&gt;\(\{P\} \ S \ \{Q\}\)&lt;/span&gt;. &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; is called the &amp;quot;precondition&amp;quot;. Informally, if &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; is satisfied, then after &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt; (the statement or instruction) has been executed, &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; (the postcondition) should be true. In other words, &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; is true before &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt;, and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; should be true after.&lt;/p&gt;

&lt;p&gt;In fact, we can view &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt; as a function on the state space, going from &lt;span  class=&#34;math&#34;&gt;\(\sigma\)&lt;/span&gt; satisfying property &lt;span  class=&#34;math&#34;&gt;\(P(\sigma)\)&lt;/span&gt; to a state &lt;span  class=&#34;math&#34;&gt;\(S(\sigma) = \sigma&#39;\)&lt;/span&gt; satisfying the postcondition, &lt;span  class=&#34;math&#34;&gt;\(Q(\sigma&#39;)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Thus a Hoare triple can be seen as a 3-tuple&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[(P, f, Q)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;satisfying:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[P(\sigma) \to Q(f(\sigma))\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It turns out that this interpretation is a strong one, and we will use it throughout the post to derive the Hoare rules, some of which follows directly from this interpretation.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\forall \sigma.P(\sigma) \to Q(f(\sigma))}{\{P\}\ f\ \{Q\}} \qquad \frac{\{P\}\ f\ \{Q\}}{\forall \sigma.P(\sigma) \to Q(f(\sigma))}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(on a side note, this notation should be understood as: what is below the line is true if what is above is true)&lt;/p&gt;

&lt;h2 id=&#34;an-example&#34;&gt;An example&lt;/h2&gt;

&lt;p&gt;Suppose we have the program,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;// a = 4
a += 2;
// a = 6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is expressed by the Hoare triple&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\{a = 4\} \ a \gets a + 2 \ \{a = 6\}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So far, we have only introduced the notation, which in itself is worthless, what&#39;s really the core is the rules that allows us to reason about valid Hoare triples. We need a way to essentially construct new Hoare triples from old ones.&lt;/p&gt;

&lt;h2 id=&#34;rules-and-axioms&#34;&gt;Rules and axioms&lt;/h2&gt;

&lt;h3 id=&#34;empty-statement-rule&#34;&gt;Empty statement rule&lt;/h3&gt;

&lt;p&gt;The empty statement rule states that: Let &lt;span  class=&#34;math&#34;&gt;\(S\)&lt;/span&gt; be any statement which carries no side-effect, then &lt;span  class=&#34;math&#34;&gt;\(\{P\} \ S \ \{P\}\)&lt;/span&gt;, or in inference line notation:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{S \text{ is pure}}{\{P\} \ S \ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This rule is relatively simple: If the state is not changed, the invariants are neither. Note that this is only true for effect-less statements, since the statement could otherwise change variables or in other ways invalidate the postcondition.&lt;/p&gt;

&lt;p&gt;In fact, we can express it in terms of the identity function, &lt;span  class=&#34;math&#34;&gt;\(f(x)=x\)&lt;/span&gt;. Then,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[P(x) \to P(f(x)) = P(x)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Hence, the triple is valid.&lt;/p&gt;

&lt;h3 id=&#34;composition-rule&#34;&gt;Composition rule&lt;/h3&gt;

&lt;p&gt;The composition rule allows you to concatenate two statements (into a Hoare triple) if the first statement&#39;s postcondition is equal to the second statement&#39;s precondition:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P\}\ S\ \{Q\}, \quad \{Q\}\ T\ \{R\}}{\{P\}\ S;T\ \{R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It is left as an exercise for the reader to verify the correctness of the rule above.&lt;/p&gt;

&lt;h3 id=&#34;strengthening-and-weakening-conditions&#34;&gt;Strengthening and weakening conditions&lt;/h3&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{P_1 \to P_2,\quad \{P_2\}\ S\ \{Q_2\},\quad Q_2 \to Q_1}{\{P_1\}\ S\ \{Q_1\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So, what&#39;s going on here? Well, &lt;span  class=&#34;math&#34;&gt;\(P_1\)&lt;/span&gt; implies &lt;span  class=&#34;math&#34;&gt;\(P_2\)&lt;/span&gt;, so we can replace the precondition by a stronger version which implies the old one. The same cannot be applied to postcondition, because the strengthened precondition might not yield the strengthened postcondition after the statement. We can however replace it by a weaker postcondition (i.e. one which is implied by original postcondition).&lt;/p&gt;

&lt;p&gt;We can always weaken guarantees, but never assumptions, since the assumption is what the guarantee relies on. Assumptions can be made stronger, however.&lt;/p&gt;

&lt;p&gt;It is left as an exercise for the reader to verify the correctness of the rule above.&lt;/p&gt;

&lt;h3 id=&#34;the-assignment-axiom&#34;&gt;The assignment axiom&lt;/h3&gt;

&lt;p&gt;This axiom is the most important. It allows for reasoning about preconditions in the case of assignments. It is absolutely essential to Hoare logic.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P[x \gets E]\}\ x \gets E\ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(P[x \gets E]\)&lt;/span&gt; denotes replacing every free (unbound) &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Let&#39;s say &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; involves some assertion about &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;, then we can move it over the assignment (to the precondition) replacing &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; with the right-hand-side of the assignment, because every occurence of &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; represents said value anyway, so substituting the value &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; represents for &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; won&#39;t change the structure.&lt;/p&gt;

&lt;p&gt;Let&#39;s say we have the statement, &lt;span  class=&#34;math&#34;&gt;\(x \gets x + 2\)&lt;/span&gt;, with the postcondition &lt;span  class=&#34;math&#34;&gt;\(\{x = 6\}\)&lt;/span&gt;, we can then derive the Hoare triple:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\{x + 2 = 6\}\ x \gets x + 2\ \{x = 6\}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;One thing that is surprising, but also incredibly important, is that you substitute it into the precondition and not the postcondition. To see why such a rule (&lt;span  class=&#34;math&#34;&gt;\(\{P\}\ x \gets E\ \{P[x \gets E]\}\)&lt;/span&gt;) would be wrong, observe how you could derive &lt;span  class=&#34;math&#34;&gt;\(\{x = 1\}\ x \gets 2\ \{2 = 1\}\)&lt;/span&gt;, which is clearly false.&lt;/p&gt;

&lt;p&gt;It is also worth noting that, in this context, expressions cannot carry side-effects. We&#39;ll cover this in detail in part two.&lt;/p&gt;

&lt;h3 id=&#34;conditional-rule&#34;&gt;Conditional rule&lt;/h3&gt;

&lt;p&gt;So far, we have only covered a simple language without loops, conditionals, and other forms of branches.&lt;/p&gt;

&lt;p&gt;The first (and simplest) form of branches is a conditional non-cyclic branch (&lt;code&gt;if&lt;/code&gt;). These behaves in a very simple way:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{C \land P\}\ B\ \{Q\},\quad \{\neg C \land P\}\ E\ \{Q\}}{\{P\}\ \textbf{if } C \textbf{ then } B \textbf{ else } E \textbf{ end}\ \{Q\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;As complex this looks, it&#39;s actually relatively simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;In your &lt;code&gt;if&lt;/code&gt; statement&#39;s body, you can safely assume the &lt;code&gt;if&lt;/code&gt; condition to be true.&lt;/li&gt;
&lt;li&gt;If both branches shares their postcondition (&lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt;), then the &lt;code&gt;if&lt;/code&gt; statement does as well.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As an example, consider the code,&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;if x == 4 {
    // I can safely assume that x = 4 here.
    ...
    x = 2;
    // Now x = 2.
} else {
    // I can safely assume that x ≠ 4 here.
    ...
    x = 2;
    // Now x = 2.
}
// All branches share postcondition, so the whole if-statement does as well: x = 2
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-loop-rule&#34;&gt;The loop rule&lt;/h3&gt;

&lt;p&gt;The loop rule reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{I \land C\}\ B\ \{I\}}{\{I\}\ \textbf{while } C \textbf{ do } B \textbf{ done}\ \{I \land \neg C\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(I\)&lt;/span&gt; is called the &lt;em&gt;loop invariant&lt;/em&gt;, i.e. the condition which is true before and after the loop. The loop will terminate when &lt;span  class=&#34;math&#34;&gt;\(\neg C\)&lt;/span&gt;, hence the postcondition of the loop.&lt;/p&gt;

&lt;p&gt;As a simple example, take the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;let mut x = 3;
let mut y = 4;
// Precondition: x == 3 (loop invariant)
while y &amp;lt; 100 {
    // Precondition: y &amp;lt; 100 &amp;amp;&amp;amp; x == 3
    y += 1;
    // Posttcondition: x == 3 (loop invariant)
}
// Postcondition: !(y &amp;lt; 100) ⇒ y &amp;gt;= 100
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;applying-hoare-logic-to-the-mir&#34;&gt;Applying Hoare logic to the MIR&lt;/h1&gt;

&lt;p&gt;The Rust MIR is in many ways an interesting language. It can be seen as an extremely stripped-down version of Rust. What we&#39;ll work with is the MIR from the last compiler pass.&lt;/p&gt;

&lt;h2 id=&#34;the-rust-mir&#34;&gt;The Rust MIR&lt;/h2&gt;

&lt;p&gt;The Rust MIR has no structural control flow. It directly resembles the CFG of the program.&lt;/p&gt;

&lt;p&gt;There are three concepts we must be familiar with to understand the Rust MIR:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;: A graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Basic blocks&lt;/strong&gt;: The nodes in the graph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terminators&lt;/strong&gt;: The edges in the graph.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We&#39;ll not get into the representation of scopes and type information the MIR contains.&lt;/p&gt;

&lt;h3 id=&#34;functions&#34;&gt;Functions&lt;/h3&gt;

&lt;p&gt;Taking aside the type information, functions have two components: A set of variables and a Control Flow Graph.&lt;/p&gt;

&lt;p&gt;The function starts with a bunch of variable declarations (arguments, temporaries, and variables). There&#39;s one implicit variable, the &lt;code&gt;return&lt;/code&gt; variable, which contains the return values.&lt;/p&gt;

&lt;p&gt;Secondly, there&#39;s a set of basic blocks, as well as a starting block.&lt;/p&gt;

&lt;h3 id=&#34;basic-blocks&#34;&gt;Basic blocks&lt;/h3&gt;

&lt;p&gt;Basic blocks are the nodes of the CFG. They each represent a series of statements. In our model, we can wlog. assume that a statement is simply an assignment, &lt;span  class=&#34;math&#34;&gt;\(x \gets y\)&lt;/span&gt;, where &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt; is an operand. In other words, a basic block is of the form &lt;span  class=&#34;math&#34;&gt;\((x_1 \gets y_1; x_2 \gets y_2; \ldots; x_n \gets y_n, t)\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(t\)&lt;/span&gt; being the terminator.&lt;/p&gt;

&lt;p&gt;In fact, we can go even further: A statement is a single assignment. This can be shown by simply constructing a map between the two graphs (by using the goto terminator to chain).&lt;/p&gt;

&lt;p&gt;Note that there are two kinds of assignments. Up until now, we have only considered the &lt;em&gt;simple assignment&lt;/em&gt; &lt;span  class=&#34;math&#34;&gt;\(x \gets y\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt; being a simple, effectless expression. There&#39;s actually a second form of assignment, the function call assignment, &lt;span  class=&#34;math&#34;&gt;\(x \gets f(y)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;In such an assignment, the function can change the state of the program, and thus care must be taken, since you cannot always use the assignment axiom. We&#39;ll get back to that later on.&lt;/p&gt;

&lt;h3 id=&#34;terminators&#34;&gt;Terminators&lt;/h3&gt;

&lt;p&gt;Terminators are what binds basic blocks together. Every basic block has an associated terminator, which takes one of the following forms:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Return from the current function: &lt;span  class=&#34;math&#34;&gt;\(\textbf{return}\)&lt;/span&gt;. The return value is stored in the &lt;code&gt;return&lt;/code&gt; variable.&lt;/li&gt;
&lt;li&gt;Calling a diverging function (&amp;quot;transferring&amp;quot; to the function), &lt;span  class=&#34;math&#34;&gt;\(f(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Non-conditionally jumping to another block &lt;span  class=&#34;math&#34;&gt;\(\textbf{goto}(b)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Jumping to another block if a condition is true, &lt;span  class=&#34;math&#34;&gt;\(\textbf{if}_c(b_1, b_2)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(there is a few - in the implementation - we ignore in our model for simplification purposes)&lt;/p&gt;

&lt;p&gt;Notice how none of these are structural. All are based around gotos. Not only does this simplify our analysis, but it&#39;s also more near to the machine representation.&lt;/p&gt;

&lt;p&gt;As an example, let&#39;s write a program that finds the 10th Fibonacci number:&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://i.imgur.com/gk6b2ZQ.png&#34; alt=&#34;Tenth Fibonacci number&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;First of all, the program starts by assigning starting values. Then it enters a loop with a conditional branch in the end (is 10 reached yet?). In this loop we do the classic, add the two numbers and shift one down. When the loops ends, we assign the return value, and then return from the function.&lt;/p&gt;

&lt;h2 id=&#34;reasoning-about-the-mir&#34;&gt;Reasoning about the MIR&lt;/h2&gt;

&lt;h3 id=&#34;unconditional-gotos&#34;&gt;Unconditional gotos&lt;/h3&gt;

&lt;p&gt;The first rule is the non-structural equivalent of the composition rule. All it says is that for a goto-statement to be valid, the precondition of the target basic block must be true:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P\}\ b_1\ \{Q\}, \quad \{Q\}\ b_2\ \{R\}}{\{P\}\ b_1; \textbf{goto}(b_2)\ \{R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&#34;conditional-gotos&#34;&gt;Conditional gotos&lt;/h3&gt;

&lt;p&gt;Conditional gotos are interesting in that it allows us to reason about both while-loops and if-statements in only run rule.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P \land C\}\ b_1\ \{Q\},\quad \{P \land \neg C\}\ b_2\ \{Q\}}{\{P\}\ \textbf{if}_C(b_1, b_2)\ \{Q\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;It is the non-structural equivalent of the conditional rule, we described earlier.&lt;/p&gt;

&lt;h3 id=&#34;function-calls&#34;&gt;Function calls&lt;/h3&gt;

&lt;p&gt;Functions take the form &lt;span  class=&#34;math&#34;&gt;\(f(x) \stackrel{\text{def}}{=} \{P(x)\}\ b\ \{Q(x)\}\)&lt;/span&gt;, i.e. an initial starting block, &lt;span  class=&#34;math&#34;&gt;\(b\)&lt;/span&gt;, and a precondition and postcondition, respectively.&lt;/p&gt;

&lt;p&gt;The rule of correctness for function calls reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{f(x) = \{P(x)\}\ b\ \{Q(x, \textbf{return})\}}{\{P(y) \land R[x \gets f(y)]\}\ x \gets f(y)\ \{Q(y, x) \land R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This one is a big one. Let&#39;s break it up:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The assumption (above the inference line) states that &lt;span  class=&#34;math&#34;&gt;\(f(x)\)&lt;/span&gt; is a Hoare triple with the precondition and postcondition being terms depending on the argument.&lt;/li&gt;
&lt;li&gt;The postcondition depends on the return value of &lt;span  class=&#34;math&#34;&gt;\(f(x)\)&lt;/span&gt; as well.&lt;/li&gt;
&lt;li&gt;The conclusion (below the inference line) consists of a Hoare triple with an assignment to &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The postcondition of the assignment is &lt;span  class=&#34;math&#34;&gt;\(Q(y, x)\)&lt;/span&gt; which express that the return value of the function is assigned to &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;, and the argument is &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt;. This is logically joined with &lt;span  class=&#34;math&#34;&gt;\(R\)&lt;/span&gt;, which is carried over to the other side:&lt;/li&gt;
&lt;li&gt;The precondition consists of &lt;span  class=&#34;math&#34;&gt;\(R[x \gets f(y)]\)&lt;/span&gt;, in a similar manner to the assignment axiom, as well as &lt;span  class=&#34;math&#34;&gt;\(P(y)\)&lt;/span&gt;, the precondition of the function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that this rule will be modified later when we introduce pointers into our model.&lt;/p&gt;

&lt;p&gt;Take this imaginary program:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;fn subtract_without_overflow(a: u32, b: u32) -&amp;gt; u32 {
    // Precondition: b ≤ a
    a - b
    // Postcondition: return ≤ a
}

fn main() {
    let mut n = 0;
    let mut res;
    while n &amp;lt; 10 {
        res = subtract_without_overflow(10, n);
        // Postcondition: res &amp;lt; 10 (this is what we&#39;re going to prove!)
        n += 1;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We know here that the condition for the loop is &lt;span  class=&#34;math&#34;&gt;\(x &lt; 10\)&lt;/span&gt;, as such we set:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\begin{align*}
x &amp;= \mathtt{res}\\
y &amp;= (10, n)\\
R &amp;= [\mathtt{res} &lt; 10]\\
P((a, b)) &amp;= [b \leq a]\\
Q((a, b), r) &amp;= [r \leq a]
\end{align*}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Plug it all in, and get:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{f(a, b) = \{n \leq a\}\ S\ \{f(a, b) \leq a\}}{\{n \leq 10 \land f(10, n) &lt; 10\}\ x \gets f(10, n)\ \{f(10, n) \leq 10 \land \mathtt{res} &lt; 10\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The desired result is obtained: the precondition implies that &lt;span  class=&#34;math&#34;&gt;\(n &lt; 10\)&lt;/span&gt;, which is also the loop condition.&lt;/p&gt;

&lt;!--  --&gt;

&lt;p&gt;Thus, we can conclude that there is no overflow in the program. Cool, no?&lt;/p&gt;

&lt;h3 id=&#34;dont-repeat-yourself&#34;&gt;Don&#39;t repeat yourself!&lt;/h3&gt;

&lt;p&gt;The rest of the rules are exactly matching the &amp;quot;classical&amp;quot; Hoare logic axioms. In other words, the assignment axiom, skip axiom, and consequence axiom remains unchanged.&lt;/p&gt;

&lt;h1 id=&#34;reasoning-about-pointers&#34;&gt;Reasoning about pointers&lt;/h1&gt;

&lt;p&gt;This is a tricky subject. Pointers are notorious for being hard to reason about. In fact, they are probably the single hardest subject in program verification.&lt;/p&gt;

&lt;h2 id=&#34;approach-1-global-reasoning&#34;&gt;Approach 1: Global reasoning&lt;/h2&gt;

&lt;p&gt;We could simply consider memory as one big array, in which pointers are indexes, but it turns out such a model is not only non-local, but also very messy, as such we need to derive a more expressive and convenient model to be able to reason about pointers without too much hassle.&lt;/p&gt;

&lt;h2 id=&#34;approach-2-relational-alias-analysis&#34;&gt;Approach 2: Relational alias analysis&lt;/h2&gt;

&lt;p&gt;To start with, I&#39;ll introduce a model I call &amp;quot;relational alias analysis&amp;quot;. We define an equivalence relation, &lt;span  class=&#34;math&#34;&gt;\(\sim\)&lt;/span&gt;, on the set of variables. This equivalence relation tells if two variables are &lt;em&gt;aliased&lt;/em&gt; (i.e. pointers to the same location).&lt;/p&gt;

&lt;h3 id=&#34;aliasing-variables&#34;&gt;Aliasing variables&lt;/h3&gt;

&lt;p&gt;The first axiom reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{x \sim y\}}{\{x = y\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;i.e. if two variables are aliased, they&#39;re equal.&lt;/p&gt;

&lt;p&gt;This is perhaps more of a definition than an axiom. None the less, it describes the semantics of our alias relation.&lt;/p&gt;

&lt;p&gt;Then we describe the behavior of alias asignments:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{A = \textbf{alias}(a)\}\ a \stackrel{\text{alias}}{\gets} b\ \{\textbf{alias}(a) = A \cup \{b\}\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;(&lt;span  class=&#34;math&#34;&gt;\(\textbf{alias}(x)\)&lt;/span&gt; defines the equivalence class of &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; under &lt;span  class=&#34;math&#34;&gt;\(\sim\)&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;This allows for declaring a variable to be aliased with another variable.&lt;/p&gt;

&lt;h3 id=&#34;assignment-axiom-for-aliased-values&#34;&gt;Assignment axiom for aliased values&lt;/h3&gt;

&lt;p&gt;Preconditions and postconditions can contain statements on the value behind the pointer, which has the unfortunate consequence that the old assignment axiom schema is no longer valid.&lt;/p&gt;

&lt;p&gt;In fact, we simply need to observe that previously, we had &lt;span  class=&#34;math&#34;&gt;\(\textbf{alias}(x) = \{x\}\)&lt;/span&gt;. Now that we introduced aliased values, the situation changed, and the equivalence class can be arbitrarily large.&lt;/p&gt;

&lt;p&gt;We put,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P[\textbf{alias}(x) \gets E]\}\ x \gets E\ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Note that &lt;span  class=&#34;math&#34;&gt;\(P[A \gets E]\)&lt;/span&gt; means that we replace every element &lt;span  class=&#34;math&#34;&gt;\(a \in A\)&lt;/span&gt; with &lt;span  class=&#34;math&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;In other words, we do the same as before except that we assign the value to &lt;em&gt;all&lt;/em&gt; the aliased variables.&lt;/p&gt;

&lt;h3 id=&#34;insufficiency&#34;&gt;Insufficiency&lt;/h3&gt;

&lt;p&gt;This model allows reasoning about aliases, but &lt;em&gt;not&lt;/em&gt; pointers in general. In fact, it cannot reason about &lt;code&gt;noalias&lt;/code&gt; pointers, deallocation, and pointer arithmetics.&lt;/p&gt;

&lt;h2 id=&#34;approach-3-separation-logic&#34;&gt;Approach 3: Separation logic&lt;/h2&gt;

&lt;p&gt;Separation logic was originally introduced by JC Reynolds in one of the most cited program verification papers ever. It is more complex than the alternative model we just presented, but also more expressive in some cases.&lt;/p&gt;

&lt;h3 id=&#34;modeling-memory&#34;&gt;Modeling memory&lt;/h3&gt;

&lt;p&gt;Our model of memory consists of multiple new notations. First of all, the model becomes memory aware. We use &lt;span  class=&#34;math&#34;&gt;\(p \mapsto x\)&lt;/span&gt; to denote that some pointer, &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt;, maps to the value &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;We use the notation &lt;span  class=&#34;math&#34;&gt;\(\mathcal{H}(p)\)&lt;/span&gt; to denote pointer reads. The reason we keep the notation function-like is because it is, in fact, just a function! It simply maps pointers to values. We can define,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{p \mapsto x}{\mathcal{H}(p) = x}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We denote pointer writes by &lt;span  class=&#34;math&#34;&gt;\(p \stackrel{\text{ptr}}{\gets} x\)&lt;/span&gt;.&lt;/p&gt;

&lt;h4 id=&#34;disjointness&#34;&gt;Disjointness&lt;/h4&gt;

&lt;p&gt;The first feature of separation logic is the notion of &amp;quot;separate conjunction&amp;quot;, denotes &lt;span  class=&#34;math&#34;&gt;\(P * Q\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;This asserts that &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; are both true and independent, i.e. their &amp;quot;heaps&amp;quot; are disjointed and not affected by the statement of the Hoare triple. In particular, let &lt;span  class=&#34;math&#34;&gt;\(A\)&lt;/span&gt; be the domain of &lt;span  class=&#34;math&#34;&gt;\(\mathcal{H}\)&lt;/span&gt;, then let &lt;span  class=&#34;math&#34;&gt;\(\{A_1, A_2\}\)&lt;/span&gt; be some semipartition of &lt;span  class=&#34;math&#34;&gt;\(A\)&lt;/span&gt; (&lt;span  class=&#34;math&#34;&gt;\(A_1 \cap A_2 = \emptyset\)&lt;/span&gt;), then put &lt;span  class=&#34;math&#34;&gt;\(A_1 = \textbf{ref}(P)\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(A_b = \textbf{ref}(Q)\)&lt;/span&gt; (&lt;span  class=&#34;math&#34;&gt;\(\textbf{ref}(P)\)&lt;/span&gt; denotes all the locations that are referenced in &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt;, e.g. &lt;span  class=&#34;math&#34;&gt;\(\textbf{ref}([\mathcal{H}(x) = 3]) = \{x\}\)&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;We can then put &lt;span  class=&#34;math&#34;&gt;\(P * Q\)&lt;/span&gt;. This might seem useless at first (how much different from &lt;span  class=&#34;math&#34;&gt;\(\land\)&lt;/span&gt; is it?), but it is incredibly important: If &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; are dependent, not by sharing a free variable, but instead share a variable through aliasing (say &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt; has &lt;span  class=&#34;math&#34;&gt;\(x\)&lt;/span&gt; free and &lt;span  class=&#34;math&#34;&gt;\(Q\)&lt;/span&gt; has &lt;span  class=&#34;math&#34;&gt;\(y\)&lt;/span&gt; free, and &lt;span  class=&#34;math&#34;&gt;\(x \sim y\)&lt;/span&gt;).&lt;/p&gt;

&lt;p&gt;All this will be formally defined in the next subsection.&lt;/p&gt;

&lt;h4 id=&#34;the-frame-rule&#34;&gt;The frame rule&lt;/h4&gt;

&lt;p&gt;The frame rule is the most important component of separation logic. It reads,&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\textbf{mut}(C) \cap \textbf{free}(R) = \emptyset,\quad \{P\}\ C\ \{Q\}}{\{P * R\}\ C\ \{Q * R\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\(\textbf{mut}(C)\)&lt;/span&gt; means the set of variables &lt;span  class=&#34;math&#34;&gt;\(C\)&lt;/span&gt; &amp;quot;mutates&amp;quot; (changes) when executed. For example, &lt;span  class=&#34;math&#34;&gt;\(\textbf{mut}(a \gets b) = \{a\}\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;What the rule says is that if &lt;span  class=&#34;math&#34;&gt;\(C\)&lt;/span&gt; never changes the &amp;quot;environment&amp;quot; from &lt;span  class=&#34;math&#34;&gt;\(R\)&lt;/span&gt;, then you can safely join the precondition and postcondition with &lt;span  class=&#34;math&#34;&gt;\(R\)&lt;/span&gt; of some Hoare triple with &lt;span  class=&#34;math&#34;&gt;\(C\)&lt;/span&gt;.&lt;/p&gt;

&lt;h3 id=&#34;the-behavior-of-byreference-assignments&#34;&gt;The behavior of by-reference assignments&lt;/h3&gt;

&lt;p&gt;The next thing we need is a way to reason about assignments behind pointers, or &amp;quot;pointer writes&amp;quot;. We use the term &amp;quot;by-reference assignments&amp;quot; to signify the similarities between normal assignments.&lt;/p&gt;

&lt;p&gt;Starting by defining by-reference assignment, we add a rule allowing us to write to valid pointers:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P * p \mapsto \bullet\}\ p \stackrel{\text{ptr}}{\gets} x\ \{P * p \mapsto x\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Next, we need to specify the semantics of &lt;em&gt;reading&lt;/em&gt; from a pointer:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\{P \land p \mapsto x\}\ k \gets \mathcal{H}(p)\ \{Q\}}{\{P \land p \mapsto x\}\ k \gets x\ \{Q\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In other words, writing the data read from a pointer to a variable is equivalent to writing the value it&#39;s pointing to. This is more of a definition than an actual rule, because it is obvious, ignoring the notation.&lt;/p&gt;

&lt;h3 id=&#34;allocation&#34;&gt;Allocation&lt;/h3&gt;

&lt;p&gt;Allocation is what introduces a new heap store/pointer into the heap. And its behavior is relatively straight-forward:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{p \notin \textbf{free}(P)}{\{P\}\ p \gets \textbf{alloc}(s)\ \{P * p \to \bullet\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Namely, if &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; is not contained in &lt;span  class=&#34;math&#34;&gt;\(P\)&lt;/span&gt;, allocation creates a new, separate pointer. &lt;span  class=&#34;math&#34;&gt;\(\bullet\)&lt;/span&gt; denotes that the pointer is uninitialized or the value is unknown.&lt;/p&gt;

&lt;h3 id=&#34;deallocation&#34;&gt;Deallocation&lt;/h3&gt;

&lt;p&gt;As an example, take the dealloc function. This function obviously requires that there is no usage of the pointer later on (i.e. no use-after-free). We can express this in a relatively simple way:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{}{\{P * p \mapsto x\}\ \textbf{dealloc}(p)\ \{P\}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The &lt;span  class=&#34;math&#34;&gt;\(*\)&lt;/span&gt; here express the independence of the content and validity of the pointer &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt;, which is really where separation logic shines: We can express pointer relation, and in this case, make sure that there is no usage of &lt;span  class=&#34;math&#34;&gt;\(p\)&lt;/span&gt; after the free.&lt;/p&gt;

&lt;h3 id=&#34;pointers-on-the-stack&#34;&gt;Pointers on the stack&lt;/h3&gt;

&lt;p&gt;In a formal model, the stack and the heap are not semantically different. In fact, we can interpret function calls as allocating the arguments onto the heap and deallocating them again when returning.&lt;/p&gt;

&lt;h3 id=&#34;detecting-memory-leaks&#34;&gt;Detecting memory leaks&lt;/h3&gt;

&lt;p&gt;In this model, it is surprisingly easy to prove your program leak-free. You simply have to put that the heap is empty in the postcondition and propagate it forward.&lt;/p&gt;

&lt;h2 id=&#34;future-work-and-whats-next&#34;&gt;Future work and what&#39;s next&lt;/h2&gt;

&lt;p&gt;Currently, I am writing a theorem extractor, which will generate the statement of correctness for some arbitrary program. This can then be fed into SMT solver and shown to be true.&lt;/p&gt;

&lt;p&gt;Another aspect is the compilation itself, which must be a verified process, as such I am working on a compiler and formal proof of correctness of said compiler.&lt;/p&gt;

&lt;p&gt;Lastly, I can formally verify Ralloc and Redox.&lt;/p&gt;

&lt;h2 id=&#34;conclusion-and-final-words&#34;&gt;Conclusion and final words&lt;/h2&gt;

&lt;p&gt;We have seen how a modest set of rules can create an elegant way to reason about the complex behavior of programs. Rust already has a very interesting form of static analysis, but it is decidable and much simpler, as a result, there is a lot of things it can not reason about, like raw pointers. We need a more advanced model (like the one we proposed in this post) to reason about such things.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>