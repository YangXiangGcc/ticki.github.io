<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on Ticki&#39;s blog</title>
    <link>http://ticki.github.io/tags/programming/index.xml</link>
    <description>Recent content in Programming on Ticki&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://ticki.github.io/tags/programming/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>An Atomic Hash Table</title>
      <link>http://ticki.github.io/blog/an-atomic-hash-table/</link>
      <pubDate>Sat, 13 May 2017 00:00:00 +0000</pubDate>
      
      <guid>http://ticki.github.io/blog/an-atomic-hash-table/</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;In programs where there is some kind of global state, you will often find
the need for having a key-value map; you could for example imagine keeping some
kind of cache of a bunch of entries from database table. Obviously, you&#39;d just
use a hash table, easy right?&lt;/p&gt;

&lt;p&gt;Not really. Imagine that there is multiple threads. One approach is to wrap it
in a mutex to ensure thread safety, but that would kind of miss the point of
concurrency: It wouldn&#39;t be concurrent, it would just be blocking. So, imagine
we want a non-blocking, truly concurrent hash table. How&#39;d we go about that?&lt;/p&gt;

&lt;p&gt;Several ways to get there exists, unfortunately they all have their trade-offs.
In this blog post, I am going represent the implementation of such table, which
I came up with during the development of &lt;a href=&#34;https://github.com/ticki/tfs&#34;&gt;TFS&lt;/a&gt;.
It is hopefully a sane way to solve the issue with good trade-offs: It has
relatively good consistency guarantees, yet very fast.&lt;/p&gt;

&lt;p&gt;To be clear, if you&#39;re just looking for a quick way to implement this, this is
probably not the post for you. It is somewhat complicated, and you might be
fine with a hash table with locks and striping. However, if you are looking for
something scalable to an arbitrary number of CPU cores, this is for you.&lt;/p&gt;

&lt;h1 id=&#34;preface&#34;&gt;Preface&lt;/h1&gt;

&lt;p&gt;I assume that you are familiar with hash tables and hash table design, as well
as basic terms in concurrent programming.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;To be able to actually approach the problem, we have to define a limited API
for the hash table. Here&#39;s the obvious basics:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;insert(key, value)&lt;/code&gt; - for, well, inserting into the table.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;remove(key)&lt;/code&gt; - for removing a entry from the table.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get(key)&lt;/code&gt; - for querying a value from the table.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many ways the above set can be expanded, such as iteration, bulk
operations, etc., but we will stick to those for simplicity.&lt;/p&gt;

&lt;h1 id=&#34;the-structure&#34;&gt;The structure&lt;/h1&gt;

&lt;p&gt;As any data structure, you will have the structure (how it is organized, that
is) and the algorithms manipulating it. It is only natural to start by asking:
How should the keys and values be organized?&lt;/p&gt;

&lt;p&gt;There is not really a single answer to that question, but there&#39;s a few things
to keep in mind. First of all, we probably want to avoid reallocation. That
thing is messy, and often impossible, to do atomically; you will almost always
end up introducing some kind of lock, either explicitly or implicitly, so we
must necessarily use a kind of linearly growing structure.&lt;/p&gt;

&lt;p&gt;What if we used some kind of tree?&lt;/p&gt;

&lt;p&gt;If you&#39;ve seen my post about &lt;a href=&#34;https://ticki.github.io/blog/collision-resolution-with-nested-hash-tables/&#34;&gt;nested hash
tables&lt;/a&gt;,
you will know where I&#39;m going, namely that we must organize a tree of tables,
such that extending it, simply means placing a new table at a leaf of the tree.&lt;/p&gt;

&lt;p&gt;The idea is this: Every table has some fixed number of buckets, each of which
can be in one of following states:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Empty - no entry, key, or value.&lt;/li&gt;
&lt;li&gt;Leaf - there is a key and value.&lt;/li&gt;
&lt;li&gt;Branch - the bucket &amp;quot;branches&amp;quot; to another table.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The structure itself is nothing, but a root table. The operations are defined
on any table, and not just the root table, as this allows us to use recursion.&lt;/p&gt;

&lt;p&gt;We&#39;ll see why in a second.&lt;/p&gt;

&lt;h1 id=&#34;the-algorithm&#34;&gt;The algorithm&lt;/h1&gt;

&lt;p&gt;The biggest obstruction to designing concurrent hash table (or hash tables in
general) is of course the collision resolution. When two keys collide (which
will happen with almost certainty), there must be a way of resolving this
collision, meaning to make them able to coexist in the same map.&lt;/p&gt;

&lt;p&gt;In this structure, it is simply done by going from a leaf of the tree to a
branch, containing both key-value pairs (say a bucket is &lt;code&gt;Leaf(entry)&lt;/code&gt; and you
want to insert an entry there called &lt;code&gt;entry2&lt;/code&gt;, then the bucket is changed to
&lt;code&gt;Table(new_table)&lt;/code&gt; where &lt;code&gt;new_table&lt;/code&gt; has both entries; if the two keys still
collide in the new table, repeat).&lt;/p&gt;

&lt;p&gt;The natural question to ask here is how we hash. Surely, if we simply used the
same hash function throughout, a collision would keep happening, even after
branches, as the keys would be assigned the same bucket, over and over again.
Hence, we must choose a way, wherein we can produce distinct hashes for
different tables.&lt;/p&gt;

&lt;p&gt;The easiest way to do this is of course using one of these fancy &amp;quot;seeded hash
functions&amp;quot;, but even that has it&#39;s limitations: What if (like most of these
functions) their independence isn&#39;t mathematically proven? What if there
exists a key which would always generate a collision, despite of the seed? In
other words, it relies on the correctness and quality of the underlying hash
function, which we would like to avoid. We will come back to another way of
producing the hashes in next section.&lt;/p&gt;

&lt;p&gt;Searching is pretty obvious from here: We simply index by the hash until we
find a leaf or empty bucket, in which case we&#39;re done.&lt;/p&gt;

&lt;p&gt;Lastly, we have deleting, which is simply a matter of replacing a bucket with
&amp;quot;empty&amp;quot;.&lt;/p&gt;

&lt;h1 id=&#34;hashing&#34;&gt;Hashing&lt;/h1&gt;

&lt;p&gt;So, how exactly do we generate the sequence (I say sequence, as in the indexes
for each of the tables in traversing to the entry, namely the sequence of
buckets to follow/jump)?&lt;/p&gt;

&lt;p&gt;Well, we could start by formulating a property of correctness:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Given enough elements of the sequence, it shall be possible to reproduce the
hashed value?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since the sequence is never-ending (and hence has an infinite codomain), this
is possible to satisfy, contrary to classical hash functions, with a finite
codomain.&lt;/p&gt;

&lt;p&gt;Another way of thinking about this is that such function is a function of byte
strings to numbers in the interval, &lt;span  class=&#34;math&#34;&gt;\(\left[0;1\right)\)&lt;/span&gt;. This definition
actually allows us to go one step further and define bijectiveness to create the
ideal of such function.&lt;/p&gt;

&lt;p&gt;Anyway, now that we know what we&#39;re looking for, we must find a way to actually
find it.&lt;/p&gt;

&lt;p&gt;The way I&#39;ve done it is by having a state, which is altering based on the
previous element; and a stack, which contains permuted input.&lt;/p&gt;

&lt;p&gt;The idea is like this: First the stack is populated with the input,
one-byte-by-one. Each byte XOR&#39;d with the state and then permuted by some
table. The generated element is pushed to the stack and is made the state.&lt;/p&gt;

&lt;p&gt;When the stack is populated, elements of the sequence can be read by popping
from the stack, and then permuting the popped element according to the state
(i.e. XOR, then use lookup table). Again, this is set to the state.&lt;/p&gt;

&lt;p&gt;So why exactly this thing? Doesn&#39;t it seem a bit arbitrary?&lt;/p&gt;

&lt;p&gt;Well, there is a reason. First of all, the state is there for bringing in some
context, such that the element in the output can&#39;t simply be predicted
according to the same element in the input.&lt;/p&gt;

&lt;p&gt;Secondly, from the stack, we can reconstruct the input sequence. And from the
output sequence, we can reconstruct the stack, so the input sequence is
reconstructible from the input sequence.&lt;/p&gt;

&lt;p&gt;But why not simply have the output sequence be the stack? Well, without
permuting it after each pop, entropy would only move upwards, i.e. the output
would only depend on the elements &lt;em&gt;before&lt;/em&gt; it in the input. The permutation in
the pop brings in context of the elements &lt;em&gt;after&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Is this optimal? Probably not, but it&#39;s fine, as it isn&#39;t exactly critical in
performance, kind of like how you don&#39;t optimize command-line parsing in
&lt;code&gt;wget&lt;/code&gt;, the real hit comes from the atomics, not the hashing.&lt;/p&gt;

&lt;h1 id=&#34;making-it-concurrent&#34;&gt;Making it concurrent&lt;/h1&gt;

&lt;p&gt;If you&#39;ve noticed, so far we&#39;ve really only constructed our table in terms of
single-threaded algorithms.&lt;/p&gt;

&lt;p&gt;Maybe it&#39;s obvious to the reader how to make it concurrent, maybe not. It
depends on your experience with things, but I hope the text that follows will
make it sensical to you.&lt;/p&gt;

&lt;p&gt;We can represent each bucket in a node (table) as a pointer, which we only
modify atomically. The null state represents the empty bucket. This allows us
to do certain pretty convenient things.&lt;/p&gt;

&lt;p&gt;Namely, it allows us to use CAS (compare-and-swap) to ensure that the entry,
we&#39;re replacing, is indeed empty. If not, we get the non-matching value, which
allows us to handle things case-by-case.&lt;/p&gt;

&lt;h2 id=&#34;searching&#34;&gt;Searching&lt;/h2&gt;

&lt;p&gt;Searching is painfully obvious: Simply read and follow the appropriate buckets,
until you get to a leaf, which is your final destination. It is a match, if the
key of the leaf matches the key, you&#39;re searching for.&lt;/p&gt;

&lt;h2 id=&#34;insertion&#34;&gt;Insertion&lt;/h2&gt;

&lt;p&gt;As described above, the insert routine starts by effectively CAS-ing the bucket
of the key for null pointers (empty buckets), and replacing with the leaf
containing the key and value, we&#39;re interested in inserting.&lt;/p&gt;

&lt;p&gt;If it failed, we do the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If the bucket is a leaf, try to CAS this bucket pointer to a branching table
containing the leaf&#39;s K/V pair and the pair, we&#39;re inserting. If the CAS
fails, start over with the updated value (see below).&lt;/li&gt;
&lt;li&gt;If the bucket is a table, simply go to that table and insert it there.&lt;/li&gt;
&lt;li&gt;If the bucket is an empty (this means there&#39;ve been an ABA condition; another
thread has removed the value in the meantime), start over with the updated value.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;removal&#34;&gt;Removal&lt;/h2&gt;

&lt;p&gt;Removal is slightly more complicated.&lt;/p&gt;

&lt;p&gt;First, the bucket is read. If it is a subtable, recurse and remove on that.&lt;/p&gt;

&lt;p&gt;If not a table and not empty, then you must CAS for the pointer to the null
pointer. If this fails, one must restart the process (ABA).&lt;/p&gt;

&lt;h1 id=&#34;optional-the-aba-problem&#34;&gt;(Optional:) The ABA problem&lt;/h1&gt;

&lt;p&gt;I&#39;ve mentioned the ABA problem a few times so far, and it turns out to be a
very important issue of concurrent algorithms, but what does it mean?&lt;/p&gt;

&lt;p&gt;In simple terms, it means that some data, we&#39;ve read has changed in the
meantime.&lt;/p&gt;

&lt;p&gt;This is why I don&#39;t read and then write the modified version. Instead I read
&lt;span  class=&#34;math&#34;&gt;\(A\)&lt;/span&gt;, then find &lt;span  class=&#34;math&#34;&gt;\(f(A)\)&lt;/span&gt; and then try to CAS &lt;span  class=&#34;math&#34;&gt;\(A\)&lt;/span&gt; to &lt;span  class=&#34;math&#34;&gt;\(f(A)\)&lt;/span&gt;. It can be a
pain, but it&#39;s necessary. The reason for this is that the atomic, we read as
&lt;span  class=&#34;math&#34;&gt;\(A\)&lt;/span&gt;, could have been changed before or while we evaluated &lt;span  class=&#34;math&#34;&gt;\(f(A)\)&lt;/span&gt;, hence
meaning that &lt;span  class=&#34;math&#34;&gt;\(f(A)\)&lt;/span&gt; would reverse progress another thread has made.&lt;/p&gt;

&lt;p&gt;So the question (to which I have already revealed the answer) is, what happens
if this CAS fails?&lt;/p&gt;

&lt;p&gt;Well, we use a loop. If it fails, we use the new value and repeat our
operation.&lt;/p&gt;

&lt;p&gt;Doesn&#39;t this kind of defeat the point of concurrent algorithms? Well, in a
sense, it does make it non-atomic, but it isn&#39;t really a spin-lock or anything
like that, even though it can seem like that: It doesn&#39;t wait for the data to
be in a state (like a spin-lock will do), it just repeats if the data failed to
be substituted.&lt;/p&gt;

&lt;h1 id=&#34;the-code&#34;&gt;The code&lt;/h1&gt;

&lt;p&gt;It is available
&lt;a href=&#34;https://github.com/redox-os/tfs/tree/master/atomic-hashmap/src&#34;&gt;here&lt;/a&gt;. Note
that it won&#39;t compile right now, as it relies on some patches I made to the
&lt;code&gt;crossbeam&lt;/code&gt; library. Eventually, I will replace the dependency with my own
replacement for crossbeam.&lt;/p&gt;

&lt;h1 id=&#34;limitations&#34;&gt;Limitations&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;It cannot shrink. Hopefully, this isn&#39;t that big of an issue, given that the
space is reusable, despite not being able to shrink. It is similar to certain
balanced trees, which cannot shrink. In general, the space isn&#39;t lost, nor
leads to unbounded memory usage.&lt;/li&gt;
&lt;li&gt;It is cache suboptimal. This cannot really be avoided. There will, likely, be
more than one cache miss, during traversal. However, the added concurrency
often pays that off performance-wise.&lt;/li&gt;
&lt;li&gt;It relies on some type of concurrent memory reclamation (e.g. hazard
pointers) in order to free memory of removed entries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Are these worth it? Well, it depends on what you&#39;re doing. In my case, these
are all very much worth it, and I haven&#39;t had any issue with any of them, but
every program is unique and will have its own set of conditions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skip Lists: Done Right</title>
      <link>http://ticki.github.io/blog/skip-lists-done-right/</link>
      <pubDate>Sat, 17 Sep 2016 13:46:49 +0200</pubDate>
      
      <guid>http://ticki.github.io/blog/skip-lists-done-right/</guid>
      <description>

&lt;p&gt;&lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css&#34;&gt;&lt;/p&gt;

&lt;h1 id=&#34;what-is-a-skip-list&#34;&gt;What is a skip list?&lt;/h1&gt;

&lt;p&gt;In short, skip lists are a linked-list-like structure which allows for fast search. It consists of a base list holding the elements, together with a tower of lists maintaining a linked hierarchy of subsequences, each skipping over fewer elements.&lt;/p&gt;

&lt;p&gt;Skip list is a wonderful data structure, one of my personal favorites, but a trend in the past ten years has made them more and more uncommon as a single-threaded in-memory structure.&lt;/p&gt;

&lt;p&gt;My take is that this is because of how hard they are to get right. The simplicity can easily fool you into being too relaxed with respect to performance, and while they are simple, it is important to pay attention to the details.&lt;/p&gt;

&lt;p&gt;In the past five years, people have become increasingly sceptical of skip lists&amp;rsquo; performance, due to their poor cache behavior when compared to e.g. B-trees, but fear not, a good implementation of skip lists can easily outperform B-trees while being implementable in only a couple of hundred lines.&lt;/p&gt;

&lt;p&gt;How? We will walk through a variety of techniques that can be used to achieve this speed-up.&lt;/p&gt;

&lt;p&gt;These are my thoughts on how a bad and a good implementation of skip list looks like.&lt;/p&gt;

&lt;h2 id=&#34;advantages&#34;&gt;Advantages&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Skip lists perform very well on rapid insertions because there are no rotations or reallocations.&lt;/li&gt;
&lt;li&gt;They&amp;rsquo;re simpler to implement than both self-balancing binary search trees and hash tables.&lt;/li&gt;
&lt;li&gt;You can retrieve the next element in constant time (compare to logarithmic time for inorder traversal for BSTs and linear time in hash tables).&lt;/li&gt;
&lt;li&gt;The algorithms can easily be modified to a more specialized structure (like segment or range &amp;ldquo;trees&amp;rdquo;, indexable skip lists, or keyed priority queues).&lt;/li&gt;
&lt;li&gt;Making it lockless is simple.&lt;/li&gt;
&lt;li&gt;It does well in persistent (slow) storage (often even better than AVL and EH).&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;a-naïve-but-common-implementation&#34;&gt;A naïve (but common) implementation&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/nNjOtfa.png&#34; alt=&#34;Each shortcut has its own node.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our skip list consists of (in this case, three) lists, stacked such that the &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n&lt;/b&gt;&amp;lsquo;th list visits a subset of the node the &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n - 1&lt;/b&gt;&amp;lsquo;th list does. This subset is defined by a probability distribution, which we will get back to later.&lt;/p&gt;

&lt;p&gt;If you rotate the skip list and remove duplicate edges, you can see how it resembles a binary search tree:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/DO031ek.png&#34; alt=&#34;A binary search tree.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Say I wanted to look up the node &amp;ldquo;30&amp;rdquo;, then I&amp;rsquo;d perform normal binary search from the root and down. Due to duplicate nodes, we use the rule of going right if both children are equal:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/H5KjvqC.png&#34; alt=&#34;Searching the tree.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Self-balancing Binary Search Trees often have complex algorithms to keep the tree balanced, but skip lists are easier: They aren&amp;rsquo;t trees, they&amp;rsquo;re similar to trees in some ways, but they are not trees.&lt;/p&gt;

&lt;p&gt;Every node in the skip list is given a &amp;ldquo;height&amp;rdquo;, defined by the highest level containing the node (similarly, the number of decendants of a leaf containing the same value). As an example, in the above diagram, &amp;ldquo;42&amp;rdquo; has height 2, &amp;ldquo;25&amp;rdquo; has height 3, and &amp;ldquo;11&amp;rdquo; has height 1.&lt;/p&gt;

&lt;p&gt;When we insert, we assign the node a height, following the probability distribution:&lt;/p&gt;

&lt;p&gt;&lt;center style=&#34;font: 400 1.21em KaTeX_Math;font-style: italic;&#34;&gt; p(n) = 2&lt;sup&gt;1-n&lt;/sup&gt; &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;To obtain this distribution, we flip a coin until it hits tails, and count the flips:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;uint generate_level() {
    uint n = 0;
    while coin_flip() {
        n++;
    }

    return n;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By this distribution, statistically the parent layer would contain half as many nodes, so searching is amortized &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;O(log &lt;i&gt;n&lt;/i&gt;) &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Note that we only have pointers to the right and below node, so insertion must be done while searching, that is, instead of searching and then inserting, we insert whenever we go a level down (pseudocode):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;-- Recursive skip list insertion function.
define insert(elem, root, height, level):
    if right of root &amp;lt; elem:
        -- If right isn&#39;t &amp;quot;overshot&amp;quot; (i.e. we are going to long), we go right.
        return insert(elem, right of root, height, level)
    else:
        if level = 0:
            -- We&#39;re at bottom level and the right node is overshot, hence
            -- we&#39;ve reached our goal, so we insert the node inbetween root
            -- and the node next to root.
            old ← right of root
            right of root ← elem
            right of elem ← old
        else:
            if level ≤ height:
                -- Our level is below the height, hence we need to insert a
                -- link before we go on.
                old ← right of root
                right of root ← elem
                right of elem ← old

            -- Go a level down.
            return insert(elem, below root, height, level - 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above algorithm is recursive, but we can with relative ease turn it into an iterative form (or let tail-call optimization do the job for us).&lt;/p&gt;

&lt;p&gt;As an example, here&amp;rsquo;s a diagram, the curved lines marks overshoots/edges where a new node is inserted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/jr9V8Ot.png&#34; alt=&#34;An example&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;waste-waste-everywhere&#34;&gt;Waste, waste everywhere&lt;/h1&gt;

&lt;p&gt;That seems fine doesn&amp;rsquo;t it? No, not at all. It&amp;rsquo;s absolute garbage.&lt;/p&gt;

&lt;p&gt;There is a total and complete waste of space going on. Let&amp;rsquo;s assume there are &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;n&lt;/b&gt; elements, then the tallest node is approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;&lt;i&gt;h = &lt;/i&gt;log&lt;sub&gt;2&lt;/sub&gt; &lt;i&gt;n&lt;/i&gt;&lt;/b&gt;, that gives us approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;1 + Σ&lt;sub&gt;&lt;i&gt;k ←0..h&lt;/i&gt;&lt;/sub&gt; &lt;i&gt;&lt;/i&gt;2&lt;sup&gt;&lt;i&gt;-k&lt;/i&gt;&lt;/sup&gt; n ≈ 2&lt;i&gt;n&lt;/i&gt;&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;2&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; is certainly no small amount, especially if you consider what each node contains, a pointer to the inner data, the node right and down, giving 5 pointers in total, so a single structure of &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; nodes consists of approximately &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;6&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; pointers.&lt;/p&gt;

&lt;p&gt;But memory isn&amp;rsquo;t even the main concern! When you need to follow a pointer on every decrease (apprx. 50% of all the links), possibly leading to cache misses. It turns out that there is a really simple fix for solving this:&lt;/p&gt;

&lt;p&gt;Instead of linking vertically, a good implementation should consist of a singly linked list, in which each node contains  an array (representing the nodes above) with pointers to later nodes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Fd6gDLv.png&#34; alt=&#34;A better skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you represent the links (&amp;ldquo;shortcuts&amp;rdquo;) through dynamic arrays, you will still often get cache miss. Particularly, you might get a cache miss on both the node itself (which is not data local) and/or the dynamic array. As such, I recommend using a fixed-size array (beware of the two negative downsides: 1. more space usage, 2. a hard limit on the highest level, and the implication of linear upperbound when &lt;i style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;h &amp;gt; c&lt;/i&gt;. Furthermore, you should keep small enough to fit a cache line.).&lt;/p&gt;

&lt;p&gt;Searching is done by following the top shortcuts as long as you don&amp;rsquo;t overshoot your target, then you decrement the level and repeat, until you reach the lowest level and overshoot. Here&amp;rsquo;s an example of searching for &amp;ldquo;22&amp;rdquo;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/cQsPnGa.png&#34; alt=&#34;Searching for &amp;quot;22&amp;quot;.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In pseudocode:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;define search(skip_list, needle):
    -- Initialize to the first node at the highest level.
    level ← max_level
    current_node ← root of skip_list

    loop:
        -- Go right until we overshoot.
        while level&#39;th shortcut of current_node &amp;lt; needle:
            current_node ← level&#39;th shortcut of current_node

        if level = 0:
            -- We hit our target.
            return current_node
        else:
            -- Decrement the level.
            level ← level - 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;b-style-font-400-1-21em-katex-math-o-1-b-level-generation&#34;&gt;&lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;O(1)&lt;/b&gt; level generation&lt;/h1&gt;

&lt;p&gt;Even William Pugh did this mistake in &lt;a href=&#34;http://epaperpress.com/sortsearch/download/skiplist.pdf&#34;&gt;his original paper&lt;/a&gt;. The problem lies in the way the level is generated: Repeating coin flips (calling the random number generator, and checking parity), can mean a couple of RNG state updates (approximately 2 on every insertion). If your RNG is a slow one (e.g. you need high security against DOS attacks), this is noticable.&lt;/p&gt;

&lt;p&gt;The output of the RNG is uniformly distributed, so you need to apply some function which can transform this into the desired distribution. My favorite is this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;define generate_level():
    -- First we apply some mask which makes sure that we don&#39;t get a level
    -- above our desired level. Then we find the first set bit.
    ffz(random() &amp;amp; ((1 &amp;lt;&amp;lt; max_level) - 1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This of course implies that you &lt;code&gt;max_level&lt;/code&gt; is no higher than the bit width of the &lt;code&gt;random()&lt;/code&gt; output. In practice, most RNGs return 32-bit or 64-bit integers, which means this shouldn&amp;rsquo;t be a problem, unless you have more elements than there can be in your address space.&lt;/p&gt;

&lt;h1 id=&#34;improving-cache-efficiency&#34;&gt;Improving cache efficiency&lt;/h1&gt;

&lt;p&gt;A couple of techniques can be used to improve the cache efficiency:&lt;/p&gt;

&lt;h2 id=&#34;memory-pools&#34;&gt;Memory pools&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/Wa8IVBJ.png&#34; alt=&#34;A skip list in a memory pool.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our nodes are simply fixed-size blocks, so we can keep them data local, with high allocation/deallocation performance, through linked memory pools (SLOBs), which is basically just a list of free objects.&lt;/p&gt;

&lt;p&gt;The order doesn&amp;rsquo;t matter. Indeed, if we swap &amp;ldquo;9&amp;rdquo; and &amp;ldquo;7&amp;rdquo;, we can suddenly see that this is simply a skip list:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/O863RR1.png&#34; alt=&#34;It&#39;s true.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can keep these together in some arbitrary number of (not necessarily consecutive) pages, drastically reducing cache misses, when the nodes are of smaller size.&lt;/p&gt;

&lt;p&gt;Since these are pointers into memory, and not indexes in an array, we need not reallocate on growth. We can simply extend the free list.&lt;/p&gt;

&lt;h2 id=&#34;flat-arrays&#34;&gt;Flat arrays&lt;/h2&gt;

&lt;p&gt;If we are interested in compactness and have a insertion/removal ratio near to 1, a variant of linked memory pools can be used: We can store the skip list in a flat array, such that we have indexes into said array instead of pointers.&lt;/p&gt;

&lt;h2 id=&#34;unrolled-lists&#34;&gt;Unrolled lists&lt;/h2&gt;

&lt;p&gt;Unrolled lists means that instead of linking each element, you link some number of fixed-size chuncks contains two or more elements (often the chunk is around 64 bytes, i.e. the normal cache line size).&lt;/p&gt;

&lt;p&gt;Unrolling is essential for a good cache performance. Depending on the size of the objects you store, unrolling can reduce cache misses when following links while searching by 50-80%.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s an example of an unrolled skip list:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/FYpPQPh.png&#34; alt=&#34;A simple 4 layer unrolled skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The gray box marks excessive space in the chunk, i.e. where new elements can be placed. Searching is done over the skip list, and when a candidate is found, the chunk is searched through &lt;strong&gt;linear&lt;/strong&gt; search. To insert, you push to the chunk (i.e. replace the first free space). If no excessive space is available, the insertion happens in the skip list itself.&lt;/p&gt;

&lt;p&gt;Note that these algorithms requires information about how we found the chunk. Hence we store a &amp;ldquo;back look&amp;rdquo;, an array of the last node visited, for each level. We can then backtrack if we couldn&amp;rsquo;t fit the element into the chunk.&lt;/p&gt;

&lt;p&gt;We effectively reduce cache misses by some factor depending on the size of the object you store. This is due to fewer links need to be followed before the goal is reached.&lt;/p&gt;

&lt;h1 id=&#34;self-balancing-skip-lists&#34;&gt;Self-balancing skip lists&lt;/h1&gt;

&lt;p&gt;Various techniques can be used to improve the height generation, to give a better distribution. In other words, we make the level generator aware of our nodes, instead of purely random, independent RNGs.&lt;/p&gt;

&lt;h2 id=&#34;self-correcting-skip-list&#34;&gt;Self-correcting skip list&lt;/h2&gt;

&lt;p&gt;The simplest way to achieve a content-aware level generator is to keep track of the number of node of each level in the skip list. If we assume there are &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt; nodes, the expected number of nodes with level &lt;b style=&#34;font: 400 1.21em KaTeX_Math&#34;&gt;&lt;i&gt;l&lt;/i&gt;&lt;/b&gt; is &lt;b style=&#34;font: 400 1.21em KaTeX_Main&#34;&gt;2&lt;sup&gt;&lt;i&gt;-l&lt;/i&gt;&lt;/sup&gt;&lt;i&gt;n&lt;/i&gt;&lt;/b&gt;. Subtracting this from actual number gives us a measure of how well-balanced each height is:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/bBf7kcg.png&#34; alt=&#34;Balance&#34; /&gt;&lt;/p&gt;

&lt;p&gt;When we generate a new node&amp;rsquo;s level, you choose one of the heights with the biggest under-representation (see the black line in the diagram), either randomly or by some fixed rule (e.g. the highest or the lowest).&lt;/p&gt;

&lt;h2 id=&#34;perfectly-balanced-skip-lists&#34;&gt;Perfectly balanced skip lists&lt;/h2&gt;

&lt;p&gt;Perfect balancing often ends up hurting performance, due to backwards level changes, but it is possible. The basic idea is to reduce the most over-represented level when removing elements.&lt;/p&gt;

&lt;h1 id=&#34;an-extra-remark&#34;&gt;An extra remark&lt;/h1&gt;

&lt;p&gt;Skip lists are wonderful as an alternative to Distributed Hash Tables. Performance is mostly about the same, but skip lists are more DoS resistant if you make sure that all links are F2F.&lt;/p&gt;

&lt;p&gt;Each node represents a node in the network. Instead of having a head node and a nil node, we connect the ends, so any machine can search starting at it self:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/moD7oy9.png&#34; alt=&#34;A network organized as a skip list.&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you want a secure open system, the trick is that any node can invite a node, giving it a level equal to or lower than the level itself. If the node control the key space in the interval of A to B, we partition it into two and transfer all KV pairs in the second part to the new node. Obviously, this approach has no privilege escalation, so you can&amp;rsquo;t initialize a sybil attack easily.&lt;/p&gt;

&lt;h1 id=&#34;conclusion-and-final-words&#34;&gt;Conclusion and final words&lt;/h1&gt;

&lt;p&gt;By apply a lot of small, subtle tricks, we can drastically improve performance of skip lists, providing a simpler and faster alternative to Binary Search Trees. Many of these are really just minor tweaks, but give an absolutely enormous speed-up.&lt;/p&gt;

&lt;p&gt;The diagrams were made with &lt;a href=&#34;https://en.wikipedia.org/wiki/Dia_(software)&#34;&gt;Dia&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/PGF/TikZ&#34;&gt;TikZ&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>